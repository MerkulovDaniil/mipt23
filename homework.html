<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>üíÄ –î–æ–º–∞—à–∫–∞</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./favicon.svg" rel="icon" type="image/svg+xml">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js" integrity="sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A" crossorigin="anonymous"></script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="üíÄ –î–æ–º–∞—à–∫–∞">
<meta property="og:description" content="">
<meta name="twitter:title" content="üíÄ –î–æ–º–∞—à–∫–∞">
<meta name="twitter:description" content="">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./logo.svg" alt="mipt23.fmin.xyz" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./program.html"> 
<span class="menu-text">üöÄ –ü—Ä–æ–≥—Ä–∞–º–º–∞</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./homework.html" aria-current="page"> 
<span class="menu-text">üíÄ –î–æ–º–∞—à–∫–∞</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./projects.html"> 
<span class="menu-text">üèõ –ü—Ä–æ–µ–∫—Ç—ã</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/MerkulovDaniil/mipt23" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://www.youtube.com/watch?v=duPZb4AGz3c&amp;list=PLQSHEO58cjmMt8PY2H0ObJJ6FKbn0PoBx" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-youtube"></i></a>
    <a href="https://t.me/mipt23_fmin" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-telegram"></i></a>
    <a href="https://fmin.xyz" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-gem"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#matrix-calculus" id="toc-matrix-calculus" class="nav-link active" data-scroll-target="#matrix-calculus">Matrix calculus</a></li>
  <li><a href="#automatic-differentiation-and-jax" id="toc-automatic-differentiation-and-jax" class="nav-link" data-scroll-target="#automatic-differentiation-and-jax">Automatic differentiation and jax</a></li>
  <li><a href="#convex-sets" id="toc-convex-sets" class="nav-link" data-scroll-target="#convex-sets">Convex sets</a></li>
  <li><a href="#convex-functions" id="toc-convex-functions" class="nav-link" data-scroll-target="#convex-functions">Convex functions</a></li>
  <li><a href="#conjugate-sets" id="toc-conjugate-sets" class="nav-link" data-scroll-target="#conjugate-sets">Conjugate sets</a></li>
  <li><a href="#conjugate-functions" id="toc-conjugate-functions" class="nav-link" data-scroll-target="#conjugate-functions">Conjugate functions</a></li>
  <li><a href="#subgradient-and-subdifferential" id="toc-subgradient-and-subdifferential" class="nav-link" data-scroll-target="#subgradient-and-subdifferential">Subgradient and subdifferential</a></li>
  <li><a href="#kkt-and-duality" id="toc-kkt-and-duality" class="nav-link" data-scroll-target="#kkt-and-duality">KKT and duality</a></li>
  <li><a href="#linear-programming" id="toc-linear-programming" class="nav-link" data-scroll-target="#linear-programming">Linear programming</a></li>
  <li><a href="#sequence-convergence" id="toc-sequence-convergence" class="nav-link" data-scroll-target="#sequence-convergence">Sequence convergence</a></li>
  <li><a href="#line-search" id="toc-line-search" class="nav-link" data-scroll-target="#line-search">Line search</a></li>
  <li><a href="#zero-order-methods" id="toc-zero-order-methods" class="nav-link" data-scroll-target="#zero-order-methods">Zero-order methods</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient Descent</a></li>
  <li><a href="#subgradient-descent" id="toc-subgradient-descent" class="nav-link" data-scroll-target="#subgradient-descent">Subgradient Descent</a></li>
  <li><a href="#accelerated-methods" id="toc-accelerated-methods" class="nav-link" data-scroll-target="#accelerated-methods">Accelerated methods</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/MerkulovDaniil/mipt23/edit/main/homework.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header">
</header>


<section id="matrix-calculus" class="level3">
<h3 class="anchored" data-anchor-id="matrix-calculus">Matrix calculus</h3>
<ol type="1">
<li><p>Given a matrix <span class="math inline">A</span> of size <span class="math inline">m \times n</span> and a vector <span class="math inline">x</span> of size <span class="math inline">n \times 1</span>, compute the gradient of the function <span class="math inline">f(x) = \text{tr}(A^T A x x^T)</span> with respect to <span class="math inline">x</span>.</p></li>
<li><p>Find the gradient <span class="math inline">\nabla f(x)</span> and hessian <span class="math inline">f''(x)</span>, if <span class="math inline">f(x) = \dfrac{1}{2} \Vert Ax - b\Vert^2_2</span>.</p></li>
<li><p>Find the gradient <span class="math inline">\nabla f(x)</span> and hessian <span class="math inline">f''(x)</span>, if <span class="math display">
f(x) = \frac1m \sum\limits_{i=1}^m \log \left( 1 + \exp(a_i^{T}x) \right) + \frac{\mu}{2}\Vert x\Vert _2^2, \; a_i, x \in \mathbb R^n, \; \mu&gt;0
</span></p></li>
<li><p>Compute the gradient <span class="math inline">\nabla_A f(A)</span> of the trace of the matrix exponential function <span class="math inline">f(A) = \text{tr}(e^A)</span> with respect to <span class="math inline">A</span>. Hint: hint: Use the definition of the matrix exponential. Use the definition of the differential <span class="math inline">df = f(A + dA) - f(A) + o(\Vert dA \Vert)</span> with the limit <span class="math inline">\Vert dA \Vert \to 0</span>.</p></li>
<li><p>Find the gradient <span class="math inline">\nabla f(x)</span> and hessian <span class="math inline">f''(x)</span>, if <span class="math inline">f(x) = \frac{1}{2}\Vert A - xx^T\Vert^2_F, A \in \mathbb{S}^n</span></p></li>
<li><p>Calculate the first and the second derivative of the following function <span class="math inline">f : S \to \mathbb{R}</span></p>
<p><span class="math display">
f(t) = \text{det}(A ‚àí tI_n),
</span></p>
<p>where <span class="math inline">A \in \mathbb{R}^{n \times n}, S := \{t \in \mathbb{R} : \text{det}(A ‚àí tI_n) \neq 0\}</span>.</p></li>
<li><p>Find the gradient <span class="math inline">\nabla f(X)</span>, if <span class="math inline">f(X) = \text{tr}\left( AX^2BX^{-\top} \right)</span>.</p></li>
</ol>
</section>
<section id="automatic-differentiation-and-jax" class="level3">
<h3 class="anchored" data-anchor-id="automatic-differentiation-and-jax">Automatic differentiation and jax</h3>
<p>You can use any automatic differentiation framework in this section (Jax, PyTorch, Autograd etc.)</p>
<ol type="1">
<li><p>You will work with the following function for this exercise, <span class="math display">
f(x,y)=e^{‚àí\left(sin(x)‚àícos(y)\right)^2}
</span><br>
Draw the computational graph for the function. Note, that it should contain only primitive operations - you need to do it automatically - <a href="https://bnikolic.co.uk/blog/python/jax/2022/02/22/jax-outputgraph-rev.html">jax example</a>, <a href="https://github.com/waleedka/hiddenlayer">PyTorch example</a> - you can google/find your way to visualize it.</p></li>
<li><p>Compare analytic and autograd (with any framework) approach for the calculation of the gradient of:<br>
<span class="math display">
f(A) = \text{tr}(e^A)
</span></p></li>
<li><p>We can use automatic differentiation not only to calculate necessary gradients but also for tuning hyperparameters of the algorithm like learning rate in gradient descent (with gradient descent ü§Ø). Suppose, we have the following function <span class="math inline">f(x) = \frac{1}{2}\Vert x\Vert^2</span>, select a random point <span class="math inline">x_0 \in \mathbb{B}^{1000} = \{0 \leq x_i \leq 1 \mid \forall i\}</span>. Consider <span class="math inline">10</span> steps of the gradient descent starting from the point <span class="math inline">x_0</span>: <span class="math display">
x_{k+1} = x_k - \alpha_k \nabla f(x_k)
</span> Your goal in this problem is to write the function, that takes <span class="math inline">10</span> scalar values <span class="math inline">\alpha_i</span> and return the result of the gradient descent on function <span class="math inline">L = f(x_{10})</span>. And optimize this function using gradient descent on <span class="math inline">\alpha \in \mathbb{R}^{10}</span>. Suppose that each of <span class="math inline">10</span> components of <span class="math inline">\alpha</span> is uniformly distributed on <span class="math inline">[0; 0.1]</span>. <span class="math display">
\alpha_{k+1} = \alpha_k - \beta \frac{\partial L}{\partial \alpha}
</span> Choose any constant <span class="math inline">\beta</span> and the number of steps you need. Describe the obtained results. How would you understand, that the obtained schedule (<span class="math inline">\alpha \in \mathbb{R}^{10}</span>) becomes better than it was at the start? How do you check numerically local optimality in this problem?</p></li>
<li><p>Compare analytic and autograd (with any framework) approach for the gradient of:<br>
<span class="math display">
f(X) = - \log \det X
</span></p></li>
<li><p>Compare analytic and autograd (with any framework) approach for the gradient and hessian of:<br>
<span class="math display">
f(x) = x^\top x x^\top x
</span></p></li>
</ol>
<hr>
</section>
<section id="convex-sets" class="level3">
<h3 class="anchored" data-anchor-id="convex-sets">Convex sets</h3>
<ol type="1">
<li>Show, that if <span class="math inline">S \subseteq \mathbb{R}^n</span> is convex set, then its interior <span class="math inline">\mathbf{int } S</span> and closure <span class="math inline">\bar{S}</span> are also convex sets.</li>
<li>Show, that <span class="math inline">\mathbf{conv}\{xx^\top: x \in \mathbb{R}^n, \Vert x\Vert  = 1\} = \{A \in \mathbb{S}^n_+: \text{tr}(A) = 1\}</span>.</li>
<li>Let <span class="math inline">K \subseteq \mathbb{R}^n_+</span> is a cone. Prove that it is convex if and only if a set of <span class="math inline">\{x \in K \mid \sum\limits_{i=1}^n x_i = 1 \}</span> is convex.</li>
<li>Prove that the set of <span class="math inline">\{x \in \mathbb{R}^2 \mid e^{x_1}\le x_2\}</span> is convex.</li>
<li>Show that the set of directions of the non-strict local descending of the differentiable function in a point is a convex cone. (Previously, the question contained a typo ‚Äústrict local descending‚Äù)</li>
<li>Is the following set convex <span class="math display">
S = \left\{ a \in \mathbb{R}^k \mid p(0) = 1, \vert p(t) \vert\leq 1 \text{ for } \alpha\leq t \leq \beta\right\},
</span> where <span class="math display">
p(t) = a_1 + a_2 t + \ldots + a_k t^{k-1} \;?
</span></li>
</ol>
<hr>
</section>
<section id="convex-functions" class="level3">
<h3 class="anchored" data-anchor-id="convex-functions">Convex functions</h3>
<ol type="1">
<li><p>Consider the function <span class="math inline">f(x) = x^d</span>, where <span class="math inline">x \in \mathbb{R}_{+}</span>. Fill the following table with ‚úÖ or ‚ùé. Explain your answers</p>
<div class="table-responsive">
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">d</span></th>
<th style="text-align: center;">Convex</th>
<th style="text-align: center;">Concave</th>
<th style="text-align: center;">Strictly Convex</th>
<th style="text-align: center;"><span class="math inline">\mu</span>-strongly convex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">-2, x \in \mathbb{R}_{++}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">-1, x \in \mathbb{R}_{++}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">0</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">0.5</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">1</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\in (1; 2)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">2</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">&gt; 2</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div></li>
<li><p>Prove that the entropy function, defined as</p>
<p><span class="math display">
f(x) = -\sum_{i=1}^n x_i \log(x_i),
</span></p>
<p>with <span class="math inline">\text{dom}(f) = \{x \in \R^n_{++} : \sum_{i=1}^n x_i = 1\}</span>, is strictly concave.</p></li>
<li><p>Show, that the function <span class="math inline">f: \mathbb{R}^n_{++} \to \mathbb{R}</span> is convex if <span class="math inline">f(x) = - \prod\limits_{i=1}^n x_i^{\alpha_i}</span> if <span class="math inline">\mathbf{1}^T \alpha = 1, \alpha \succeq 0</span>.</p></li>
<li><p>Show that the maximum of a convex function <span class="math inline">f</span> over the polyhedron <span class="math inline">P = \text{conv}\{v_1, \ldots, v_k\}</span> is achieved at one of its vertices, i.e.,</p>
<p><span class="math display">
\sup_{x \in P} f(x) = \max_{i=1, \ldots, k} f(v_i).
</span></p>
<p>A stronger statement is: the maximum of a convex function over a closed bounded convex set is achieved at an extreme point, i.e., a point in the set that is not a convex combination of any other points in the set. (you do not have to prove it). <em>Hint:</em> Assume the statement is false, and use Jensen‚Äôs inequality.</p></li>
<li><p>Show, that the two definitions of <span class="math inline">\mu</span>-strongly convex functions are equivalent:</p>
<ol type="1">
<li><p><span class="math inline">f(x)</span> is <span class="math inline">\mu</span>-strongly convex <span class="math inline">\iff</span> for any <span class="math inline">x_1, x_2 \in S</span> and <span class="math inline">0 \le \lambda \le 1</span> for some <span class="math inline">\mu &gt; 0</span>:</p>
<p><span class="math display">
f(\lambda x_1 + (1 - \lambda)x_2) \le \lambda f(x_1) + (1 - \lambda)f(x_2) - \frac{\mu}{2} \lambda (1 - \lambda)\|x_1 - x_2\|^2
</span></p></li>
<li><p><span class="math inline">f(x)</span> is <span class="math inline">\mu</span>-strongly convex <span class="math inline">\iff</span> if there exists <span class="math inline">\mu&gt;0</span> such that the function <span class="math inline">f(x) - \dfrac{\mu}{2}\Vert x\Vert^2</span> is convex.</p></li>
</ol></li>
</ol>
</section>
<section id="conjugate-sets" class="level3">
<h3 class="anchored" data-anchor-id="conjugate-sets">Conjugate sets</h3>
<ol type="1">
<li><p>Let <span class="math inline">\mathbb{A}_n</span> be the set of all <span class="math inline">n</span> dimensional antisymmetric matrices (s.t. <span class="math inline">X^T = - X</span>). Show that <span class="math inline">\left( \mathbb{A}_n\right)^* = \mathbb{S}_n</span>.</p></li>
<li><p>Find the sets <span class="math inline">S^{*}, S^{**}, S^{***}</span>, if</p>
<p><span class="math display">
S = \{ x \in \mathbb{R}^2 \mid x_1 + x_2 \ge 0, \;\; -\dfrac12x_1 + x_2 \ge 0, \;\; 2x_1 + x_2 \ge -1 \;\; -2x_1 + x_2 \ge -3\}
</span></p></li>
<li><p>Prove, that <span class="math inline">B_p</span> and <span class="math inline">B_{p_*}</span> are inter-conjugate, i.e.&nbsp;<span class="math inline">(B_p)^* = B_{p_*}, (B_{p_*})^* = B_p</span>, where <span class="math inline">B_p</span> is the unit ball (w.r.t. <span class="math inline">p</span> - norm) and <span class="math inline">p, p_*</span> are conjugated, i.e.&nbsp;<span class="math inline">p^{-1} + p^{-1}_* = 1</span>. You can assume, that <span class="math inline">p_* = \infty</span> if <span class="math inline">p = 1</span> and vice versa.</p></li>
</ol>
<hr>
</section>
<section id="conjugate-functions" class="level3">
<h3 class="anchored" data-anchor-id="conjugate-functions">Conjugate functions</h3>
<ol type="1">
<li><p>Find <span class="math inline">f^*(y)</span>, if <span class="math inline">f(x) = \vert 2x \vert</span></p></li>
<li><p>Prove, that if <span class="math inline">f(x) = \inf\limits_{u+v = x} (g(u) + h(v))</span>, then <span class="math inline">f^*(y) = g^*(y) + h^*(y)</span>.</p></li>
<li><p>Find <span class="math inline">f^*(y)</span>, if <span class="math inline">f(x) = \log \left( \sum\limits_{i=1}^n e^{x_i} \right)</span></p></li>
<li><p>Prove, that if <span class="math inline">f(x) = g(Ax)</span>, then <span class="math inline">f^*(y) = g^*(A^{-\top}y)</span></p></li>
<li><p>Find <span class="math inline">f^*(Y)</span>, if <span class="math inline">f(X) = - \ln \det X, X \in \mathbb{S}^n_{++}</span></p></li>
<li><p>The scalar Huber function is defined as</p>
<p><span class="math display">
f_{\text{hub}}(x) =
\begin{cases}
\frac{1}{2} x^2 &amp; \text{if } |x| \leq 1 \\
|x| - \frac{1}{2} &amp; \text{if } |x| &gt; 1
\end{cases}
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./huber_function.svg" class="img-fluid figure-img"></p>
<figcaption>Scalar case</figcaption>
</figure>
</div>
<p>This convex function arises in various applications, notably in robust estimation. This problem explores the generalizations of the Huber function to <span class="math inline">\mathbb{R}^n</span>. A straightforward extension to <span class="math inline">\mathbb{R}^n</span> is expressed as <span class="math inline">f_{\text{hub}}(x_1) + \ldots + f_{\text{hub}}(x_n)</span>, yet this formulation is not circularly symmetric, that is, it‚Äôs not invariant under the transformation of <span class="math inline">x</span> by an orthogonal matrix. A circularly symmetric extension to <span class="math inline">\mathbb{R}^n</span> is given by</p>
<p><span class="math display">
f_{\text{cshub}}(x) = f_{\text{hub}}(\Vert x\Vert )=
\begin{cases}
\frac{1}{2} \Vert x\Vert_2 ^2 &amp; \text{if } \Vert x\Vert_2 \leq 1 \\
\Vert x\Vert_2 - \frac{1}{2} &amp; \text{if } \Vert x\Vert_2 &gt; 1
\end{cases}
</span></p>
<p>where the subscript denotes ‚Äúcircularly symmetric Huber function‚Äù. Show, that <span class="math inline">f_{\text{cshub}}</span> is convex. Find the conjugate function <span class="math inline">f^*(y)</span>.</p></li>
</ol>
<hr>
</section>
<section id="subgradient-and-subdifferential" class="level3">
<h3 class="anchored" data-anchor-id="subgradient-and-subdifferential">Subgradient and subdifferential</h3>
<ol type="1">
<li>Find <span class="math inline">\partial f(x)</span>, if <span class="math display">
f(x) = \text{Parametric ReLU}(x) = \begin{cases}
     x &amp; \text{if } x &gt; 0, \\
     ax &amp; \text{otherwise}.
\end{cases}
</span></li>
<li>Prove, that <span class="math inline">x_0</span> - is the minimum point of a function <span class="math inline">f(x)</span> if and only if <span class="math inline">0 \in \partial f(x_0)</span>.</li>
<li>Find <span class="math inline">\partial f(x)</span>, if <span class="math inline">f(x) = \Vert Ax - b\Vert _1</span>.</li>
<li>Find <span class="math inline">\partial f(x)</span>, if <span class="math inline">f(x) = e^{\Vert x\Vert}</span>.</li>
<li>Find <span class="math inline">\partial f(x)</span>, if <span class="math inline">f(x) = \frac12 \Vert Ax - b\Vert _2^2 + \lambda \Vert x\Vert_1, \quad \lambda &gt; 0</span>.</li>
<li>Let <span class="math inline">S \subseteq \mathbb{R}^n</span> be a convex set. We will call a <em>normal cone</em> of the set <span class="math inline">S</span> at a point <span class="math inline">x</span> the following set: <span class="math display">
N_S(x) = \left\{c \in \mathbb{R}^n : \langle c, y-x\rangle \leq 0 \quad \forall y \in S\right\}
</span>
<ol type="i">
<li><p>Draw a normal cone for a set at the points <span class="math inline">A, B, C, D, E, F</span> on the figure below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./normal_cone.svg" class="img-fluid figure-img"></p>
<figcaption>Draw a normal cone for the set <span class="math inline">S</span> in these points</figcaption>
</figure>
</div></li>
<li><p>Show, that <span class="math inline">N_S(x) = \{0\} \quad \forall x \in \mathbf{ri }(S)</span>.</p></li>
<li><p>Show, that the subdifferential <span class="math inline">\partial I_S(x) = N_S(x)</span> if <span class="math inline">I_S(x)</span> is the indicator function, i.e.&nbsp; <span class="math display">
I_S(x) = \begin{cases}0,\text{if } x \in S\\ \infty, \text{otherwise}\end{cases}
</span></p></li>
</ol></li>
</ol>
<hr>
</section>
<section id="kkt-and-duality" class="level3">
<h3 class="anchored" data-anchor-id="kkt-and-duality">KKT and duality</h3>
<p>In this section, you can consider either the arbitrary norm or the Euclidian norm if nothing else is specified.</p>
<ol type="1">
<li><p><strong>Toy example</strong> <span class="math display">
\begin{split}
&amp; x^2 + 1 \to \min\limits_{x \in \mathbb{R} }\\
\text{s.t. } &amp; (x-2)(x-4) \leq 0
\end{split}
</span></p>
<ol type="1">
<li>Give the feasible set, the optimal value, and the optimal solution.</li>
<li>Plot the objective <span class="math inline">x^2 +1</span> versus <span class="math inline">x</span>. On the same plot, show the feasible set, optimal point, and value, and plot the Lagrangian <span class="math inline">L(x,\mu)</span> versus <span class="math inline">x</span> for a few positive values of <span class="math inline">\mu</span>. Verify the lower bound property (<span class="math inline">p^* \geq \inf_x L(x, \mu)</span>for <span class="math inline">\mu \geq 0</span>). Derive and sketch the Lagrange dual function <span class="math inline">g</span>.</li>
<li>State the dual problem, and verify that it is a concave maximization problem. Find the dual optimal value and dual optimal solution <span class="math inline">\mu^*</span>. Does strong duality hold?</li>
<li>Let <span class="math inline">p^*(u)</span> denote the optimal value of the problem</li>
</ol>
<p><span class="math display">
\begin{split}
&amp; x^2 + 1 \to \min\limits_{x \in \mathbb{R} }\\
\text{s.t. } &amp; (x-2)(x-4) \leq u
\end{split}
</span></p>
<p>as a function of the parameter <span class="math inline">u</span>. Plot <span class="math inline">p^*(u)</span>. Verify that <span class="math inline">\dfrac{dp^*(0)}{du} = -\mu^*</span></p></li>
<li><p>Derive the dual problem for the Ridge regression problem with <span class="math inline">A \in \mathbb{R}^{m \times n}, b \in \mathbb{R}^m, \lambda &gt; 0</span>:</p>
<p><span class="math display">
\begin{split}
\dfrac{1}{2}\|y-b\|^2 + \dfrac{\lambda}{2}\|x\|^2 &amp;\to \min\limits_{x \in \mathbb{R}^n, y \in \mathbb{R}^m }\\
\text{s.t. } &amp; y = Ax
\end{split}
</span></p></li>
<li><p>Derive the dual problem for the support vector machine problem with <span class="math inline">A \in \mathbb{R}^{m \times n}, \mathbf{1} \in \mathbb{R}^m \in \mathbb{R}^m, \lambda &gt; 0</span>:</p>
<p><span class="math display">
\begin{split}
\langle \mathbf{1}, t\rangle + \dfrac{\lambda}{2}\|x\|^2 &amp;\to \min\limits_{x \in \mathbb{R}^n, t \in \mathbb{R}^m }\\
\text{s.t. } &amp; Ax \succeq \mathbf{1} - t \\
&amp; t \succeq 0
\end{split}
</span></p></li>
<li><p>Give an explicit solution to the following LP.</p>
<p><span class="math display">
\begin{split}
&amp; c^\top x \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; 1^\top x = 1, \\
&amp; x \succeq 0
\end{split}
</span></p>
<p>This problem can be considered the simplest portfolio optimization problem.</p></li>
<li><p>Show, that the following problem has a unique solution and find it:</p>
<p><span class="math display">
\begin{split}
&amp; \langle C^{-1}, X\rangle - \log \det X \to \min\limits_{x \in \mathbb{R}^{n \times n} }\\
\text{s.t. } &amp; \langle Xa, a\rangle \leq 1,
\end{split}
</span></p>
<p>where <span class="math inline">C \in \mathbb{S}^n_{++}, a \in \mathbb{R}^n \neq 0</span>. The answer should not involve inversion of the matrix <span class="math inline">C</span>.</p></li>
<li><p>Give an explicit solution to the following QP.</p>
<p><span class="math display">
\begin{split}
&amp; c^\top x \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; (x - x_c)^\top A (x - x_c) \leq 1,
\end{split}
</span></p>
<p>where <span class="math inline">A \in \mathbb{S}^n_{++}, c \neq 0, x_c \in \mathbb{R}^n</span>.</p></li>
<li><p>Consider the equality-constrained least-squares problem</p>
<p><span class="math display">
\begin{split}
&amp; \|Ax - b\|_2^2 \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; Cx = d,
\end{split}
</span></p>
<p>where <span class="math inline">A \in \mathbb{R}^{m \times n}</span> with <span class="math inline">\mathbf{rank }A = n</span>, and <span class="math inline">C \in \mathbb{R}^{k \times n}</span> with <span class="math inline">\mathbf{rank }C = k</span>. Give the KKT conditions, and derive expressions for the primal solution <span class="math inline">x^*</span> and the dual solution <span class="math inline">\lambda^*</span>.</p></li>
<li><p>Derive the KKT conditions for the problem</p>
<p><span class="math display">
\begin{split}
&amp; \mathbf{tr \;}X - \log\text{det }X \to \min\limits_{X \in \mathbb{S}^n_{++} }\\
\text{s.t. } &amp; Xs = y,
\end{split}
</span></p>
<p>where <span class="math inline">y \in \mathbb{R}^n</span> and <span class="math inline">s \in \mathbb{R}^n</span> are given with <span class="math inline">y^\top s = 1</span>. Verify that the optimal solution is given by</p>
<p><span class="math display">
X^* = I + yy^\top - \dfrac{1}{s^\top s}ss^\top
</span></p></li>
<li><p><strong>Supporting hyperplane interpretation of KKT conditions</strong>. Consider a <strong>convex</strong> problem with no equality constraints</p>
<p><span class="math display">
\begin{split}
&amp; f_0(x) \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; f_i(x) \leq 0, \quad i = [1,m]
\end{split}
</span></p>
<p>Assume, that <span class="math inline">\exists x^* \in \mathbb{R}^n, \mu^* \in \mathbb{R}^m</span> satisfy the KKT conditions</p>
<p><span class="math display">
\begin{split}
&amp; \nabla_x L (x^*, \mu^*) = \nabla f_0(x^*) + \sum\limits_{i=1}^m\mu_i^*\nabla f_i(x^*) = 0 \\
&amp; \mu^*_i \geq 0, \quad i = [1,m] \\
&amp; \mu^*_i f_i(x^*) = 0, \quad i = [1,m]\\
&amp; f_i(x^*) \leq 0, \quad i = [1,m]
\end{split}
</span></p>
<p>Show that</p>
<p><span class="math display">
\nabla f_0(x^*)^\top (x - x^*) \geq 0
</span></p>
<p>for all feasible <span class="math inline">x</span>. In other words, the KKT conditions imply the simple optimality criterion or <span class="math inline">\nabla f_0(x^*)</span> defines a supporting hyperplane to the feasible set at <span class="math inline">x^*</span>.</p></li>
<li><p><strong>Fenchel + Lagrange = ‚ô•.</strong> Express the dual problem of</p>
<p><span class="math display">
\begin{split}
&amp; c^\top x\to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; f(x) \leq 0
\end{split}
</span></p>
<p>with <span class="math inline">c \neq 0</span>, in terms of the conjugate function <span class="math inline">f^*</span>. Explain why the problem you give is convex. We do not assume <span class="math inline">f</span> is convex.</p></li>
<li><p><strong>A penalty method for equality constraints.</strong> We consider the problem of minimization</p>
<p><span class="math display">
\begin{split}
&amp; f_0(x) \to \min\limits_{x \in \mathbb{R}^{n} }\\
\text{s.t. } &amp; Ax = b,
\end{split}
</span></p>
<p>where $f_0(x): ^n $ is convex and differentiable, and <span class="math inline">A \in \mathbb{R}^{m \times n}</span> with <span class="math inline">\mathbf{rank }A = m</span>. In a quadratic penalty method, we form an auxiliary function</p>
<p><span class="math display">
\phi(x) = f_0(x) + \alpha \|Ax - b\|_2^2,
</span></p>
<p>where <span class="math inline">\alpha &gt; 0</span> is a parameter. This auxiliary function consists of the objective plus the penalty term <span class="math inline">\alpha \Vert Ax - b\Vert_2^2</span>. The idea is that a minimizer of the auxiliary function, <span class="math inline">\tilde{x}</span>, should be an approximate solution to the original problem. Intuition suggests that the larger the penalty weight <span class="math inline">\alpha</span>, the better the approximation <span class="math inline">\tilde{x}</span> to a solution of the original problem. Suppose <span class="math inline">\tilde{x}</span> is a minimizer of <span class="math inline">\phi(x)</span>. Show how to find, from <span class="math inline">\tilde{x}</span>, a dual feasible point for the original problem. Find the corresponding lower bound on the optimal value of the original problem.</p></li>
<li><p><strong>Analytic centering.</strong> Derive a dual problem for</p>
<p><span class="math display">
-\sum_{i=1}^m \log (b_i - a_i^\top x) \to \min\limits_{x \in \mathbb{R}^{n} }
</span></p>
<p>with domain <span class="math inline">\{x \mid a^\top_i x &lt; b_i , i = [1,m]\}</span>.</p>
<p>First introduce new variables <span class="math inline">y_i</span> and equality constraints <span class="math inline">y_i = b_i ‚àí a^\top_i x</span>. (The solution to this problem is called the analytic center of the linear inequalities <span class="math inline">a^\top_i x \leq b_i ,i = [1,m]</span>. Analytic centers have geometric applications, and play an important role in barrier methods.)</p></li>
</ol>
<hr>
</section>
<section id="linear-programming" class="level3">
<h3 class="anchored" data-anchor-id="linear-programming">Linear programming</h3>
<ol type="1">
<li><p><strong>üì±üéßüíª Covers manufacturing.</strong> Lyzard Corp is producing covers for the following products:</p>
<ul>
<li>üì± phones</li>
<li>üéß headphones</li>
<li>üíª laptops</li>
</ul>
<p>The company‚Äôs production facilities are such that if we devote the entire production to headphone covers, we can produce 5000 of them in one day. If we devote the entire production to phone covers or laptop covers, we can produce 4000 or 2000 of them in one day.</p>
<p>The production schedule is one week (6 working days), and the week‚Äôs production must be stored before distribution. Storing 1000 headphone covers (packaging included) takes up 30 cubic feet of space. Storing 1000 phone covers (packaging included) takes up 50 cubic feet of space, and storing 1000 laptop covers (packaging included) takes up 220 cubic feet of space. The total storage space available is 1500 cubic feet.</p>
<p>Due to commercial agreements with Lyzard Corp has to deliver at least 4500 headphone covers and 4000 laptop covers per week to strengthen the product‚Äôs diffusion.</p>
<p>The marketing department estimates that the weekly demand for headphones covers, phone, and laptop covers does not exceed 10000 14000, and 7000 units, therefore the company does not want to produce more than these amounts for headphones, phone, and laptop covers.</p>
<p>Finally, the net profit per headphone cover, phone cover, and laptop cover are $5, $7, and $12, respectively.</p>
<p>The aim is to determine a weekly production schedule that maximizes the total net profit.</p>
<ol type="1">
<li><p>Write a Linear Programming formulation for the problem. Use the following variables:</p>
<ul>
<li><span class="math inline">y_1</span> = number of headphones covers produced over the week,<br>
</li>
<li><span class="math inline">y_2</span> = number of phone covers produced over the week,<br>
</li>
<li><span class="math inline">y_3</span> = number of laptop covers produced over the week.</li>
</ul></li>
<li><p>Find the solution to the problem using <a href="http://www.pyomo.org">PyOMO</a></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install pyomo</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> sudo apt<span class="op">-</span>get install glpk<span class="op">-</span>utils <span class="op">--</span>quiet  <span class="co"># GLPK</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> sudo apt<span class="op">-</span>get install coinor<span class="op">-</span>cbc <span class="op">--</span>quiet  <span class="co"># CoinOR</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Perform the sensitivity analysis. Which constraint could be relaxed to increase the profit the most? Prove it numerically.</p></li>
</ol></li>
<li><p>Prove the optimality of the solution</p>
<p><span class="math display">
x = \left(\frac{7}{3} , 0, \frac{1}{3}\right)^T
</span></p>
<p>to the following linear programming problem:</p>
<p><span class="math display">
\begin{split}
&amp; 9x_1 + 3x_2 + 7x_3 \to \max\limits_{x \in \mathbb{R}^3 }\\
\text{s.t. } &amp; 2x_1 + x_2 + 3x_3 \leq 6 \\
&amp; 5x_1 + 4x_2 + x_3 \leq 12 \\
&amp; 3x_3 \leq 1,\\
&amp; x_1, x_2, x_3 \geq 0
\end{split}
</span></p>
<p>but you cannot use any numerical algorithm here.</p></li>
<li><p>Transform the following linear program into an equivalent linear program in the standard form <span class="math inline">\left(c^\top x \to \min\limits_{x\in \mathbb{R}^n} : Ax = b,x ‚â• 0\right)</span>:</p>
<p><span class="math display">
\begin{split}
&amp; x_1‚àíx_2 \to \min\limits_{x \in \mathbb{R}^2 }\\
\text{s.t. } &amp; 2x_1 + x_2 \geq 3 \\
&amp; 3x_1 ‚àí x_2 \leq 7 \\
&amp; x_1 \geq 0
\end{split}
</span></p></li>
<li><p>Consider:</p>
<p><span class="math display">
\begin{split}
&amp; 4x_1 + 5x_2 + 2x_3 \to \max\limits_{x \in \mathbb{R}^3 }\\
\text{s.t. } &amp; 2x_1 - x_2 + 2x_3 \leq 9 \\
&amp; 3x_1 + 5x_2 + 4x_3 \leq 8 \\
&amp; x_1 + x_2 + 2x_3 \leq 2 \\
&amp; x_1, x_2, x_3 \geq 0,
\end{split}
</span></p>
<ol type="1">
<li>Find an optimal solution to the Linear Programming problem using the simplex method.</li>
<li>Write the dual linear program. Find an optimal dual solution. Do we have strong duality here?</li>
</ol></li>
</ol>
<hr>
</section>
<section id="sequence-convergence" class="level3">
<h3 class="anchored" data-anchor-id="sequence-convergence">Sequence convergence</h3>
<ol type="1">
<li><p>Determine the convergence or divergence of a given sequences</p>
<ul>
<li><span class="math inline">r_{k} = \frac{1}{\sqrt{k}}</span>.</li>
<li><span class="math inline">r_{k} = 0.606^k</span>.</li>
<li><span class="math inline">r_{k} = 0.606^{2^k}</span>.</li>
</ul></li>
<li><p>Determine the convergence or divergence of a given sequence <span class="math inline">r_k =\begin{cases} \frac{1}{k}, &amp; \text{if } k\text{ is even} \\ e^{-k}, &amp; \text{if } k\text{ is odd} \end{cases}</span>.</p></li>
<li><p>Determine the following sequence <span class="math inline">\{r_k\}</span> by convergence rate (linear, sublinear, superlinear). In the case of superlinear convergence, additionally, find out whether there is quadratic convergence.</p>
<p><span class="math display">
r_k = \dfrac{1}{k!}
</span></p></li>
<li><p>Determine the following sequence <span class="math inline">\{r_k\}</span> by convergence rate (linear, sublinear, superlinear). In the case of superlinear convergence, additionally find out whether there is quadratic convergence.</p>
<p><span class="math display">
r_k = \dfrac{1}{k^k}
</span></p></li>
<li><p>Let <span class="math inline">\{r_k\}</span> be a sequence of non-negative numbers given as <span class="math inline">r_{k+1} = Mr_k^2</span>, where <span class="math inline">M &gt; 0</span>, <span class="math inline">r_0 \geq 0</span>. Establish a necessary and sufficient condition on <span class="math inline">M</span> and <span class="math inline">r_0</span> under which the sequence <span class="math inline">r_k</span> will converge to zero. What is the rate of convergence?</p></li>
</ol>
</section>
<section id="line-search" class="level3">
<h3 class="anchored" data-anchor-id="line-search">Line search</h3>
<ol type="1">
<li><p>Show that for a one-dimensional quadratic function decreasing at zero, its minimum satisfies Armijo‚Äôs condition for any <span class="math inline">c_1: 0 \leq c_1 \leq \dfrac12</span>:</p>
<p><span class="math display">
f(x_k - \alpha \nabla f (x_k)) \leq f(x_k) - c_1 \cdot \alpha\|\nabla f(x_k)\|_2^2
</span></p></li>
<li><p><strong>Implementing and Testing Line Search Conditions in Gradient Descent</strong></p>
<p><span class="math display">
x_{k+1} = x_k - \alpha \nabla f(x_k)
</span></p>
<p>In this assignment, you will modify an existing Python code for gradient descent to include various line search conditions. You will test these modifications on two functions: a quadratic function and the Rosenbrock function. The main objectives are to understand how different line search strategies influence the convergence of the gradient descent algorithm and to compare their efficiencies based on the number of function evaluations.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize_scalar</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">214</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the quadratic function and its gradient</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quadratic_function(x, A, b):</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> np.dot(x.T, np.dot(A, x)) <span class="op">-</span> np.dot(b.T, x)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad_quadratic(x, A, b):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(A, x) <span class="op">-</span> b</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a 2D quadratic problem with a specified condition number</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_quadratic_problem(cond_number):</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Random symmetric matrix</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> np.random.randn(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> np.dot(M, M.T)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ensure the matrix has the desired condition number</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    U, s, V <span class="op">=</span> np.linalg.svd(M)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> np.linspace(cond_number, <span class="dv">1</span>, <span class="bu">len</span>(s))  <span class="co"># Spread the singular values</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.dot(U, np.dot(np.diag(s), V))</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Random b</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> np.random.randn(<span class="dv">2</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A, b</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient descent function</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(start_point, A, b, stepsize_func, max_iter<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> start_point.copy()</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    trajectory <span class="op">=</span> [x.copy()]</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> grad_quadratic(x, A, b)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        step_size <span class="op">=</span> stepsize_func(x, grad)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">-=</span> step_size <span class="op">*</span> grad</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        trajectory.append(x.copy())</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(trajectory)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Backtracking line search strategy using scipy</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backtracking_line_search(x, grad, A, b, alpha<span class="op">=</span><span class="fl">0.3</span>, beta<span class="op">=</span><span class="fl">0.8</span>):</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> objective(t):</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> quadratic_function(x <span class="op">-</span> t <span class="op">*</span> grad, A, b)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> minimize_scalar(objective, method<span class="op">=</span><span class="st">'golden'</span>)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res.x</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ill-posed problem</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>cond_number <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>A, b <span class="op">=</span> generate_quadratic_problem(cond_number)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Starting point</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>start_point <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="fl">1.8</span>])</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform gradient descent with both strategies</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>trajectory_fixed <span class="op">=</span> gradient_descent(start_point, A, b, <span class="kw">lambda</span> x, g: <span class="fl">5e-2</span>)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>trajectory_backtracking <span class="op">=</span> gradient_descent(start_point, A, b, <span class="kw">lambda</span> x, g: backtracking_line_search(x, g, A, b))</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the trajectories on a contour plot</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>x1, x2 <span class="op">=</span> np.meshgrid(np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">400</span>), np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">400</span>))</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.array([quadratic_function(np.array([x, y]), A, b) <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(x1.flatten(), x2.flatten())]).reshape(x1.shape)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>plt.contour(x1, x2, Z, levels<span class="op">=</span><span class="dv">50</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>plt.plot(trajectory_fixed[:, <span class="dv">0</span>], trajectory_fixed[:, <span class="dv">1</span>], <span class="st">'o-'</span>, label<span class="op">=</span><span class="st">'Fixed Step Size'</span>)</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>plt.plot(trajectory_backtracking[:, <span class="dv">0</span>], trajectory_backtracking[:, <span class="dv">1</span>], <span class="st">'o-'</span>, label<span class="op">=</span><span class="st">'Backtracking Line Search'</span>)</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a><span class="co"># Add markers for start and optimal points</span></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>plt.plot(start_point[<span class="dv">0</span>], start_point[<span class="dv">1</span>], <span class="st">'ro'</span>, label<span class="op">=</span><span class="st">'Start Point'</span>)</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>optimal_point <span class="op">=</span> np.linalg.solve(A, b)</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>plt.plot(optimal_point[<span class="dv">0</span>], optimal_point[<span class="dv">1</span>], <span class="st">'y*'</span>, markersize<span class="op">=</span><span class="dv">15</span>, label<span class="op">=</span><span class="st">'Optimal Point'</span>)</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Gradient Descent Trajectories on Quadratic Function'</span>)</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x1'</span>)</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'x2'</span>)</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">"linesearch.svg"</span>)</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="linesearch.svg" class="img-fluid figure-img"></p>
<figcaption>The code above plots this</figcaption>
</figure>
</div>
<p>Start by reviewing the provided Python code. This code implements gradient descent with a fixed step size and a backtracking line search on a quadratic function. Familiarize yourself with how the gradient descent function and the step size strategies are implemented.</p>
<ol type="1">
<li><p>Modify the gradient descent function to include the following line search conditions:</p>
<ol type="a">
<li>Sufficient Decrease Condition</li>
<li>Curvature Condition</li>
<li>Goldstein Condition</li>
<li>Wolfe Condition</li>
<li>Dichotomy</li>
</ol>
<p>Test your modified gradient descent algorithm with the implemented line search conditions on the provided quadratic function. Plot the trajectories over iterations for each condition. Choose and specify hyperparameters for inexact line search condition. Choose and specify the termination criterion. Start from the point <span class="math inline">x_0 = (-1, 2)^T</span>.</p></li>
<li><p>Compare these 7 methods from the budget perspective. Plot the graph of function value from the number of function evaluations for each method on the same graph.</p></li>
<li><p>Plot trajectory for another function with the same set of methods</p>
<p><span class="math display">
f(x_1, x_2) =  10(x_2 ‚àí x_1^2)^2 + (x_1 ‚àí 1)^2
</span></p>
<p>with <span class="math inline">x_0 = (-1, 2)^T</span>. You might need to adjust hyperparameters.</p></li>
<li><p>Plot the same function value from the number of function calls for this experiment.</p></li>
</ol></li>
</ol>
</section>
<section id="zero-order-methods" class="level3">
<h3 class="anchored" data-anchor-id="zero-order-methods">Zero-order methods</h3>
<ol type="1">
<li><p>Solve approximately the Travelling Salesman Problem with any zero-order optimization method.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="salesman_problem.svg" class="img-fluid figure-img"></p>
<figcaption>Illustration of TSP</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial <span class="im">import</span> distance_matrix</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_random_symmetric_tsp(num_cities, seed<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    np.random.seed(seed)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> np.random.rand(num_cities, <span class="dv">2</span>)  <span class="co"># Generate random coordinates</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    dist_matrix <span class="op">=</span> distance_matrix(points, points)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    dist_matrix <span class="op">=</span> (dist_matrix <span class="op">+</span> dist_matrix.T) <span class="op">/</span> <span class="dv">2</span>  <span class="co"># Ensure symmetry</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.array(dist_matrix)  <span class="co"># Convert to JAX array for further processing</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>num_cities <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>dist_matrix <span class="op">=</span> generate_random_symmetric_tsp(num_cities)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dist_matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>You can use the <a href="https://fmin.xyz/docs/applications/salesman_problem.html">genetic</a> algorithm with any significant modification of mutation(5/10)</li>
<li>You can use another algorithm (10/10)</li>
</ul></li>
<li><p>In this assignment, we aim to explore and compare the efficacy of traditional and zero-order optimization methods in training a simple neural network. The conventional approach, Stochastic Gradient Descent (SGD), has been widely used due to its efficiency in handling large datasets and its capability to work in a huge-scale setting. This method relies on gradients of the loss function with respect to the network‚Äôs parameters. In contrast, zero-order optimization methods, also known as derivative-free methods, optimize without explicit gradient information. These methods are particularly useful for non-differentiable, noisy, or highly complex loss landscapes. The assignment‚Äôs objective is to explore such algorithms as Genetic Algorithms, Simulated Annealing, Gradient-free methods, or the Nelder-Mead method for real-world problems.</p>
<p>Note, that a variety of feed-forward neural networks could be represented as a series of linear transformations, followed by some nonlinear function (say, <span class="math inline">\text{ReLU }(x)</span>):</p>
<p><span class="math display">
\mathcal{NN}(x) = f_L \circ w_L \circ \ldots \circ f_1 \circ w_1 \circ x,
</span></p>
<p>where <span class="math inline">L</span> is the number of layers, <span class="math inline">f_i</span> - non-linear activation function, <span class="math inline">w_i = W_i x + b_i</span> - linear layer. We can denote the training data by <span class="math inline">X</span> and the labels by <span class="math inline">y</span>. The overall optimization problem here is to train the neural network to approximate the mapping <span class="math inline">X \to y</span>, i.e.&nbsp;<span class="math inline">\mathcal{NN}(X)</span> should be as close to <span class="math inline">y</span> as possible for all data points. We can ensure this by minimizing the loss function, which depends on the neural network parameters:</p>
<p><span class="math display">
\mathcal{L}(\mathcal{NN}(X, W, b), y) \to \min_{W, b}
</span></p>
<p>Typically, we use a cross-entropy loss function for the classification task. Do not worry if you are not familiar with neural networks or machine learning. Try to focus on the optimization algorithm here. You are provided with an example of this approach in the <a href="https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/NN_ES.ipynb">Colab notebook</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ES_vs_SGD-8k.svg" class="img-fluid figure-img"></p>
<figcaption>Comparison of SGD vs Evolutionary strategy for neural network without hidden layer.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ES_vs_SGD-42k.svg" class="img-fluid figure-img"></p>
<figcaption>Comparison of SGD vs Evolutionary strategy for neural network with several hidden layers.</figcaption>
</figure>
</div>
<p>The assignment requires you to implement a chosen zero-order optimization algorithm and compare its performance against SGD in training a predefined simple neural network (you can vary the structure of the network as you want for this problem). The comparison should focus on aspects such as convergence speed, final accuracy, and computational efficiency. Students should provide a name of the chosen zero-order algorithm and implement it in Python. You can use any method you want except the Evolutionary strategy, which is already in the example above.</p></li>
</ol>
</section>
<section id="gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent">Gradient Descent</h3>
<ol type="1">
<li><p><strong>Convergence of Gradient Descent in non-convex smooth case</strong></p>
<p>We will assume nothing about the convexity of <span class="math inline">f</span>. We will show that gradient descent reaches an <span class="math inline">\varepsilon</span>-substationary point <span class="math inline">x</span>, such that <span class="math inline">\|\nabla f(x)\|_2 \leq \varepsilon</span>, in <span class="math inline">O(1/\varepsilon^2)</span> iterations. Important note: you may use here Lipschitz parabolic upper bound:</p>
<p><span id="eq-quad_ub"><span class="math display">
f(y) \leq f(x) + \nabla f(x)^T (y-x) + \frac{L}{2} \|y-x\|_2^2, \;\;\;
\text{for all $x,y$}.  
  \tag{1}</span></span></p>
<ul>
<li><p>Plug in <span class="math inline">y = x^{k+1} = x^{k} - \alpha \nabla f(x^k), x = x^k</span> to (<a href="#eq-quad_ub" class="quarto-xref">Equation&nbsp;1</a>) to show that</p>
<p><span class="math display">
  f(x^{k+1}) \leq f(x^k) - \Big (1-\frac{L\alpha}{2} \Big) \alpha \|\nabla f(x^k)\|_2^2.
  </span></p></li>
<li><p>Use <span class="math inline">\alpha \leq 1/L</span>, and rearrange the previous result, to get</p>
<p><span class="math display">
  \|\nabla f(x^k)\|_2^2 \leq \frac{2}{\alpha} \left( f(x^k) - f(x^{k+1}) \right).
  </span></p></li>
<li><p>Sum the previous result over all iterations from <span class="math inline">1,\ldots,k+1</span> to establish</p>
<p><span class="math display">
  \sum_{i=0}^k \|\nabla f(x^{i})\|_2^2 \leq
  \frac{2}{\alpha} ( f(x^{0}) - f^*).
  </span></p></li>
<li><p>Lower bound the sum in the previous result to get</p>
<p><span class="math display">
  \min_{i=0,\ldots,k} \|\nabla f(x^{i}) \|_2
  \leq \sqrt{\frac{2}{\alpha(k+1)} (f(x^{0}) - f^*)},
  </span> which establishes the desired <span class="math inline">O(1/\varepsilon^2)</span> rate for achieving <span class="math inline">\varepsilon</span>-substationarity.</p></li>
</ul></li>
<li><p><strong>How gradient descent convergence depends on the condition number and dimensionality.</strong> Investigate how the number of iterations required for gradient descent to converge depends on the following two parameters: the condition number <span class="math inline">\kappa \geq 1</span> of the function being optimized, and the dimensionality <span class="math inline">n</span> of the space of variables being optimized.</p>
<p>To do this, for given parameters <span class="math inline">n</span> and <span class="math inline">\kappa</span>, randomly generate a quadratic problem of size <span class="math inline">n</span> with condition number <span class="math inline">\kappa</span> and run gradient descent on it with some fixed required precision. Measure the number of iterations <span class="math inline">T(n, \kappa)</span> that the method required for convergence (successful termination based on the stopping criterion).</p>
<p>Recommendation: The simplest way to generate a random quadratic problem of size <span class="math inline">n</span> with a given condition number <span class="math inline">\kappa</span> is as follows. It is convenient to take a diagonal matrix <span class="math inline">A \in S_{n}^{++}</span> as simply the diagonal matrix <span class="math inline">A = \text{Diag}(a)</span>, whose diagonal elements are randomly generated within <span class="math inline">[1, \kappa]</span>, and where <span class="math inline">\min(a) = 1</span>, <span class="math inline">\max(a) = \kappa</span>. As the vector <span class="math inline">b \in \mathbb{R}^n</span>, you can take a vector with random elements. Diagonal matrices are convenient to consider since they can be efficiently processed with even for large values of <span class="math inline">n</span>.</p>
<p>Fix a certain value of the dimensionality <span class="math inline">n</span>. Iterate over different condition numbers <span class="math inline">\kappa</span> on a grid and plot the dependence of <span class="math inline">T(n,\kappa)</span> against <span class="math inline">\kappa</span>. Since the quadratic problem is generated randomly each time, repeat this experiment several times. As a result, for a fixed value of <span class="math inline">n</span>, you should obtain a whole family of curves showing the dependence of <span class="math inline">T(n, \kappa)</span> on <span class="math inline">\kappa</span>. Draw all these curves in the same color for clarity (for example, red).</p>
<p>Now increase the value of <span class="math inline">n</span> and repeat the experiment. You should obtain a new family of curves <span class="math inline">T(n',\kappa)</span> against <span class="math inline">\kappa</span>. Draw all these curves in the same color but different from the previous one (for example, blue).</p>
<p>Repeat this procedure several times for other values of <span class="math inline">n</span>. Eventually, you should have several different families of curves - some red (corresponding to one value of <span class="math inline">n</span>), some blue (corresponding to another value of <span class="math inline">n</span>), some green, etc.</p>
<p>Note that it makes sense to iterate over the values of the dimensionality <span class="math inline">n</span> on a logarithmic grid (for example, <span class="math inline">n = 10, n = 100, n = 1000</span>, etc.). Use the following stopping criterion: <span class="math inline">\|\nabla f(x_k)\|_2^2 \leq \varepsilon \|\nabla f(x_0)\|_2^2</span> with <span class="math inline">\varepsilon = 10^{-5}</span>. Select the starting point <span class="math inline">x_0 = (1, \ldots, 1)^T</span></p>
<p>What conclusions can be drawn from the resulting picture?</p></li>
</ol>
</section>
<section id="subgradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="subgradient-descent">Subgradient Descent</h3>
<ol type="1">
<li><p><strong>Subgradient descent convergence with several stepsize strategies.</strong> In this problem you will have to prove the convergence of subgradient descent (<span class="math inline">x^{k+1} = x^k - \alpha_k g_k</span>) for several stepsize strategies. First prove, that</p>
<p><span class="math display">
\|x^{k+1} - x^*\|_2^2 \leq \|x^{k} - x^*\|_2^2 - 2\alpha_k \left(f(x^k) - f^* \right) + \alpha^2_k \|g_k\|_2^2
</span></p>
<p>Then, using <span class="math inline">\|g\|_2 \leq G, \|x^0 - x^*\| \leq R</span> prove, that</p>
<p><span class="math display">
\|x^{k+1} - x^*\|_2^2 \leq R^2 - 2\sum\limits_{i=1}^k\alpha_i \left(f(x^i) - f^* \right) + G^2\sum\limits_{i=1}^k\alpha^2_i
</span></p>
<p>Then, using <span class="math inline">f_k^{\text{best}} = \min\limits_{i=1,\ldots,k} f(x^i)</span> prove, that</p>
<p><span class="math display">
f_k^{\text{best}} - f^* \leq \frac{R^2 + G^2\sum\limits_{i=1}^k\alpha^2_i}{2\sum\limits_{i=1}^k\alpha_i}
</span></p>
<p>After that, finalize the bound for the following stepsize choosing strategies</p>
<ul>
<li><p>constant step size <span class="math inline">\alpha_k = \alpha</span></p></li>
<li><p>constant step length <span class="math inline">\alpha_k = \frac{\gamma}{\|g_k\|_2}</span> (so <span class="math inline">\|x^{k+1} - x^k\|_2 = \gamma</span>)</p></li>
<li><p>Inverse square root <span class="math inline">\frac{R}{G\sqrt{k}}</span></p></li>
<li><p>Inverse <span class="math inline">\frac1k</span></p></li>
<li><p>Polyak‚Äôs step size:</p>
<p><span class="math display">
  \alpha_k = \frac{f(x^k) - f^*}{\|g_k\|_2^2}
  </span></p></li>
</ul></li>
<li><p><strong>Subgradient methods for Lasso.</strong> Consider the optimization problem</p>
<p><span class="math display">
\min_{x \in \mathbb{R}^n} f(x) := \frac12 \|Ax - b\|^2 + \lambda \|x\|_1,
</span></p>
<p>with variables <span class="math inline">x \in \mathbb{R}^n</span> and problem data <span class="math inline">A \in \mathbb{R}^{m \times n}</span>, <span class="math inline">b \in \mathbb{R}^m</span> and <span class="math inline">\lambda &gt; 0</span>. This model is known as Lasso, or Least Squares with <span class="math inline">l_1</span> regularization, which encourages sparsity in the solution via the non-smooth penalty <span class="math inline">\|x\|_1 := \sum_{j=1}^n |x_j|</span>. In this problem, we will explore various subgradient methods for fitting this model.</p>
<ul>
<li><p>Derive the subdifferential <span class="math inline">\partial f(x)</span> of the objective.</p></li>
<li><p>Find the update rule of the subgradient method and state the computational complexity of applying one update using big O notation in terms of the dimensions.</p></li>
<li><p>Let <span class="math inline">n = 1000</span>, <span class="math inline">m = 200</span> and <span class="math inline">\lambda = 0.01</span>. Generate a random matrix <span class="math inline">A \in \mathbb{R}^{m \times n}</span> with independent Gaussian entries with mean 0 and variance <span class="math inline">1/m</span>, and a fixed vector <span class="math inline">x^* = {\underbrace{[1, \ldots, 1}_{\text{k times}}, \underbrace{0, \ldots, 0]}_{\text{n-k times}}}^T \in \mathbb{R}^n</span>. Let <span class="math inline">k = 5</span> and then set <span class="math inline">b = Ax^*</span>. Implement the subgradient method to minimize <span class="math inline">f(x)</span>, initialized at the all-zeros vector. Try different step size rules, including:</p>
<ul>
<li><p>constant step size <span class="math inline">\alpha_k = \alpha</span></p></li>
<li><p>constant step length <span class="math inline">\alpha_k = \frac{\gamma}{\|g_k\|_2}</span> (so <span class="math inline">\|x^{k+1} - x^k\|_2 = \gamma</span>)</p></li>
<li><p>Inverse square root <span class="math inline">\frac{1}{\sqrt{k}}</span></p></li>
<li><p>Inverse <span class="math inline">\frac1k</span></p></li>
<li><p>Polyak‚Äôs step length with estimated objective value:</p>
<p><span class="math display">
  \alpha_k = \frac{f(x_k) - f_k^{\text{best}} + \gamma_k}{\|g_k\|_2^2}, \quad \text{ with} \sum_{k=1}^\infty \gamma_k = \infty, \quad \sum_{k=1}^\infty \gamma_k^2 &lt; \infty
  </span></p>
<p>For example, one can use <span class="math inline">\gamma_k = \frac{10}{10 + k}</span>. Here <span class="math inline">f_k^{\text{best}} - \gamma_k</span> serves as estimate of <span class="math inline">f^*</span>. It is better to take <span class="math inline">\gamma_k</span> in the same scale as the objective value. One can show, that <span class="math inline">f_k^{\text{best}} \to f^*</span>.</p></li>
</ul>
<p>Plot objective value versus iteration curves of different step size rules on the same figure.</p></li>
<li><p>Repeat previous part using a heavy ball term, <span class="math inline">\beta_k(x^k - x^{k-1})</span>, added to the subgradient. Try different step size rules as in the previous part and tune the heavy ball parameter <span class="math inline">\beta_k = \beta</span> for faster convergence.</p></li>
</ul></li>
<li><p><strong>Finding a point in the intersection of convex sets.</strong> Let <span class="math inline">A \in \mathbb{R}^{n \times n}</span> be a positive definite matrix and let <span class="math inline">\Sigma</span> be an <span class="math inline">n \times n</span> diagonal matrix with diagonal entries <span class="math inline">\sigma_1,...,\sigma_n &gt; 0</span>, and <span class="math inline">y</span> a given vector in <span class="math inline">\mathbb{R}^n</span>. Consider the compact convex sets <span class="math inline">U = \{x \in \mathbb{R}^n \mid \|A^{1/2}(x-y)\|_2 \leq 1\}</span> and <span class="math inline">V = \{x \in \mathbb{R}^n \mid \|\Sigma x\|_\infty \leq 1\}</span>.</p>
<ul>
<li><p>Minimize maximum distance from the current point to the convex sets.</p>
<p><span class="math display">
  \min_{x\in\mathbb{R}^n} f(x) =  \min_{x\in\mathbb{R}^n} \max\{\mathbf{dist}(x, U), \mathbf{dist}(x, V)\}
  </span></p>
<p>propose an algorithm to find a point <span class="math inline">x \in U \cap V</span>. You can assume that <span class="math inline">U \cap V</span> is not empty. Your algorithm must be provably converging (although you do not need to prove it and you can simply refer to the lecture slides).</p></li>
<li><p>Implement your algorithm with the following data: <span class="math inline">n = 2</span>, <span class="math inline">y = (3, 2)</span>, <span class="math inline">\sigma_1 = 0.5</span>, <span class="math inline">\sigma_2 = 1</span>,</p>
<p><span class="math display">
  A = \begin{bmatrix}
  1 &amp; 0 \\
  -1 &amp; 1
  \end{bmatrix},
  </span></p>
<p>and <span class="math inline">x = (2, 1)</span>. Plot the objective value of your optimization problem versus the number of iterations.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="convex_intersection.png" class="img-fluid figure-img"></p>
<figcaption>Illustration of the problem</figcaption>
</figure>
</div></li>
</ol>
</section>
<section id="accelerated-methods" class="level3">
<h3 class="anchored" data-anchor-id="accelerated-methods">Accelerated methods</h3>
<ol type="1">
<li><p><strong>Local Convergence of Heavy Ball Method.</strong> We will work with the heavy ball method in this problem</p>
<p><span class="math display">
\tag{HB}
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1})
</span></p>
<p>It is known, that for the quadratics the best choice of hyperparameters is <span class="math inline">\alpha^* = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}, \beta^* = \dfrac{(\sqrt{L} - \sqrt{\mu})^2}{(\sqrt{L} + \sqrt{\mu})^2}</span>, which ensures accelerated linear convergence for a strongly convex quadratic function.</p>
<p>Consider the following continuously differentiable, strongly convex with parameter <span class="math inline">\mu</span>, and smooth function with parameter <span class="math inline">L</span>:</p>
<p><span class="math display">
f(x) =
\begin{cases}
\frac{25}{2}x^2, &amp; \text{if } x &lt; 1 \\
\frac12x^2 + 24x - 12, &amp; \text{if } 1 \leq x &lt; 2 \\
\frac{25}{2}x^2 - 24x + 36, &amp; \text{if } x \geq 2
\end{cases}
\quad
\nabla f(x) =
\begin{cases}
25x, &amp; \text{if } x &lt; 1 \\
x + 24, &amp; \text{if } 1 \leq x &lt; 2 \\
25x - 24, &amp; \text{if } x \geq 2
\end{cases}
</span></p>
<ol type="1">
<li><p>How to prove, that the given function is convex? Strongly convex? Smooth?</p></li>
<li><p>Find the constants <span class="math inline">\mu</span> and <span class="math inline">L</span> for a given function.</p></li>
<li><p>Plot the function value for <span class="math inline">x \in [-4, 4]</span>.</p></li>
<li><p>Run the Heavy Ball method for the function with optimal hyperparameters <span class="math inline">\alpha^* = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}, \beta^* = \dfrac{(\sqrt{L} - \sqrt{\mu})^2}{(\sqrt{L} + \sqrt{\mu})^2}</span> for quadratic function, starting from <span class="math inline">x_0 = 3.5</span>. If you have done everything above correctly, you should receive something like</p>
<div class="quarto-video"><video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="heavy_ball_conv.mp4"></video></div>
<p>You can use the following code for plotting:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.animation <span class="im">as</span> animation</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient of the function</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad_f(x):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Heavy Ball method implementation</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> heavy_ball_method(alpha, beta, x0, num_iterations):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.zeros(num_iterations <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    x_prev <span class="op">=</span> x0</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    x_curr <span class="op">=</span> x0  <span class="co"># Initialize x[1] same as x[0] to start the algorithm</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        x[i] <span class="op">=</span> x_curr</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        x_new <span class="op">=</span> x_curr <span class="op">-</span> alpha <span class="op">*</span> grad_f(x_curr) <span class="op">+</span> beta <span class="op">*</span> (x_curr <span class="op">-</span> x_prev)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        x_prev <span class="op">=</span> x_curr</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        x_curr <span class="op">=</span> x_new</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    x[num_iterations] <span class="op">=</span> x_curr</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> ...</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> ...</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>alpha_star <span class="op">=</span> ...</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>beta_star <span class="op">=</span> ...</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> ...</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>num_iterations <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the trajectory of the method</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>trajectory <span class="op">=</span> heavy_ball_method(alpha_star, beta_star, x0, num_iterations)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup the figure and axes for the animation</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="fl">3.5</span>))</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">"Heavy ball method with optimal hyperparameters Œ±* Œ≤*"</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Function for updating the animation</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update(i):</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    ax1.clear()</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    ax2.clear()</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot f(x) and trajectory</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>    x_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">100</span>)</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    f_vals <span class="op">=</span> np.piecewise(x_vals, [x_vals <span class="op">&lt;</span> <span class="dv">1</span>, (x_vals <span class="op">&gt;=</span> <span class="dv">1</span>) <span class="op">&amp;</span> (x_vals <span class="op">&lt;</span> <span class="dv">2</span>), x_vals <span class="op">&gt;=</span> <span class="dv">2</span>],</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>                        [<span class="kw">lambda</span> x: <span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span>, <span class="kw">lambda</span> x: <span class="fl">.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">-</span> <span class="dv">12</span>, <span class="kw">lambda</span> x: <span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">+</span> <span class="dv">36</span>])</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>    ax1.plot(x_vals, f_vals, <span class="st">'b-'</span>)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>    ax1.plot(trajectory[:i], [<span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="cf">if</span> x <span class="op">&lt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="fl">.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">-</span> <span class="dv">12</span> <span class="cf">if</span> x <span class="op">&lt;</span> <span class="dv">2</span> <span class="cf">else</span> <span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">+</span> <span class="dv">36</span> <span class="cf">for</span> x <span class="kw">in</span> trajectory[:i]], <span class="st">'ro-'</span>)</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add vertical dashed lines at x=1 and x=2 on the left subplot</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>    ax1.axvline(x<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'navy'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    ax1.axvline(x<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'navy'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot function value from iteration</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>    f_trajectory <span class="op">=</span> [<span class="va">None</span> <span class="cf">for</span> x <span class="kw">in</span> trajectory]</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    f_trajectory[:i] <span class="op">=</span> [<span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="cf">if</span> x <span class="op">&lt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="fl">.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">-</span> <span class="dv">12</span> <span class="cf">if</span> x <span class="op">&lt;</span> <span class="dv">2</span> <span class="cf">else</span> <span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">+</span> <span class="dv">36</span> <span class="cf">for</span> x <span class="kw">in</span> trajectory[:i]]</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>    ax2.plot(<span class="bu">range</span>(<span class="bu">len</span>(trajectory)), f_trajectory, <span class="st">'ro-'</span>)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlim(<span class="dv">0</span>, <span class="bu">len</span>(trajectory))</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylim(<span class="bu">min</span>(f_vals), <span class="bu">max</span>(f_vals))</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add horizontal dashed lines at f(1) and f(2) on the right subplot</span></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>    f_1 <span class="op">=</span> <span class="fl">12.5</span> <span class="op">*</span> <span class="fl">1.0</span><span class="op">**</span><span class="dv">2</span></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>    f_2 <span class="op">=</span> <span class="fl">.5</span> <span class="op">*</span> <span class="fl">2.</span><span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">24</span> <span class="op">*</span> <span class="fl">2.</span> <span class="op">-</span> <span class="dv">12</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    ax2.axhline(y<span class="op">=</span>f_1, color<span class="op">=</span><span class="st">'navy'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>    ax2.axhline(y<span class="op">=</span>f_2, color<span class="op">=</span><span class="st">'navy'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ax1.set_title("Function f(x) and Trajectory")</span></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>    ax1.set_xlabel(<span class="st">"x"</span>)</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylabel(<span class="st">"f(x)"</span>)</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>    ax1.grid(linestyle<span class="op">=</span><span class="st">":"</span>)</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ax2.set_title("Function Value from Iteration")</span></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlabel(<span class="st">"Iteration"</span>)</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylabel(<span class="st">"f(x)"</span>)</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>    ax2.grid(linestyle<span class="op">=</span><span class="st">":"</span>)</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the animation</span></span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>ani <span class="op">=</span> animation.FuncAnimation(fig, update, frames<span class="op">=</span>num_iterations, repeat<span class="op">=</span><span class="va">False</span>, interval<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>HTML(ani.to_jshtml())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Change the starting point to <span class="math inline">x_0 = 3.4</span>. What do you see? How could you name such a behavior of the method?</p></li>
<li><p>Change the hyperparameter <span class="math inline">\alpha^{\text{Global}} = \frac2L, \beta^{\text{Global}} = \frac{\mu}{L}</span> and run the method again from <span class="math inline">x_0 = 3.4</span>. Check whether you have accelerated convergence here.</p></li>
</ol>
<p>Context: this counterexample was provided in the <a href="https://arxiv.org/pdf/1408.3595.pdf">paper</a>, while the global convergence of the heavy ball method for general smooth strongly convex function was introduced in another <a href="https://arxiv.org/pdf/1412.7457.pdf">paper</a>. Recently, it was <a href="https://arxiv.org/pdf/2307.11291.pdf">suggested</a>, that the heavy-ball (HB) method provably does not reach an accelerated convergence rate on smooth strongly convex problems.</p></li>
<li><p>In this problem we will work with accelerated methods applied to the logistic regression problem. A good visual introduction to the topic is available <a href="https://mlu-explain.github.io/logistic-regression/">here</a>.</p>
<p>Logistic regression is a standard model in classification tasks. For simplicity, consider only the case of binary classification. Informally, the problem is formulated as follows: There is a training sample <span class="math inline">\{(a_i, b_i)\}_{i=1}^m</span>, consisting of <span class="math inline">m</span> vectors <span class="math inline">a_i \in \mathbb{R}^n</span> (referred to as features) and corresponding numbers <span class="math inline">b_i \in \{-1, 1\}</span> (referred to as classes or labels). The goal is to construct an algorithm <span class="math inline">b(\cdot)</span>, which for any new feature vector <span class="math inline">a</span> automatically determines its class <span class="math inline">b(a) \in \{-1, 1\}</span>.</p>
<p>In the logistic regression model, the class determination is performed based on the sign of the linear combination of the components of the vector <span class="math inline">a</span> with some fixed coefficients <span class="math inline">x \in \mathbb{R}^n</span>:</p>
<p><span class="math display">
b(a) := \text{sign}(\langle a, x \rangle).
</span></p>
<p>The coefficients <span class="math inline">x</span> are the parameters of the model and are adjusted by solving the following optimization problem:</p>
<p><span class="math display">
\tag{LogReg}
\min_{x \in \mathbb{R}^n} \left( \frac{1}{m} \sum_{i=1}^m \ln(1 + \exp(-b_i \langle a_i, x \rangle)) + \frac{\lambda}{2} \|x\|^2 \right),
</span></p>
<p>where <span class="math inline">\lambda \geq 0</span> is the regularization coefficient (a model parameter).</p>
<ol type="1">
<li><p>Will the LogReg problem be convex for <span class="math inline">\lambda = 0</span>? What is the gradient of the objective function? Will it be strongly convex? What if you will add regularization with <span class="math inline">\lambda &gt; 0</span>?</p></li>
<li><p>We will work with the real-world data for <span class="math inline">A</span> and <span class="math inline">b</span>: take the mushroom dataset. Be careful, you will need to predict if the mushroom is poisonous or edible. A poor model can cause death in this exercise.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_svmlight_file</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># URL of the file to download</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">'https://mipt23.fmin.xyz/files/mushrooms.txt'</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the file and save it locally</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> requests.get(url)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> <span class="st">'mushrooms.txt'</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure the request was successful</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> response.status_code <span class="op">==</span> <span class="dv">200</span>:</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(dataset, <span class="st">'wb'</span>) <span class="im">as</span> f:</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        f.write(response.content)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load the dataset from the downloaded file</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> load_svmlight_file(dataset)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    A, b <span class="op">=</span> data[<span class="dv">0</span>].toarray(), data[<span class="dv">1</span>]</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    n, d <span class="op">=</span> A.shape</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Data loaded successfully."</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Number of samples: </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, Number of features: </span><span class="sc">{</span>d<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Failed to download the file. Status code: </span><span class="sc">{</span>response<span class="sc">.</span>status_code<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Divide the data into two parts: training and test. We will train the model on the <span class="math inline">A_{train}</span>, <span class="math inline">b_{train}</span> and measure the accuracy of the model on the <span class="math inline">A_{test}</span>, <span class="math inline">b_{test}</span>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and test sets</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>A_train, A_test, b_train, b_test <span class="op">=</span> train_test_split(A, b, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">214</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>For the training part <span class="math inline">A_{train}</span>, <span class="math inline">b_{train}</span>, estimate the constants <span class="math inline">\mu, L</span> of the training/optimization problem. Use the same small value <span class="math inline">\lambda</span> for all experiments</p></li>
<li><p>Using gradient descent with the step <span class="math inline">\frac{1}{L}</span>, train a model. Plot: accuracy versus iteration number.</p>
<p><span class="math display">
\tag{HB}
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1})
</span></p>
<p>Fix a step <span class="math inline">\alpha = \frac{1}{L}</span> and search for different values of the momentum <span class="math inline">\beta</span> from <span class="math inline">-1</span> to <span class="math inline">1</span>. Choose your own convergence criterion and plot convergence for several values of momentum on the same graph. Is the convergence always monotonic?</p></li>
<li><p>For the best value of momentum <span class="math inline">\beta</span>, plot the dependence of the model accuracy on the test sample on the running time of the method. Add to the same graph the convergence of gradient descent with step <span class="math inline">\frac{1}{L}</span>. Draw a conclusion. Ensure, that you use the same value of <span class="math inline">\lambda</span> for both methods.</p></li>
<li><p>Solve the logistic regression problem using the Nesterov method.</p>
<p><span class="math display">
\tag{NAG}
x_{k+1} = x_k - \alpha \nabla f(x_k + \beta (x_k - x_{k-1})) + \beta (x_k - x_{k-1})  
</span></p>
<p>Fix a step <span class="math inline">\frac{1}{L}</span> and search for different values of momentum <span class="math inline">\beta</span> from <span class="math inline">-1</span> to <span class="math inline">1</span>. Check also the momentum values equal to <span class="math inline">\frac{k}{k+3}</span>, <span class="math inline">\frac{k}{k+2}</span>, <span class="math inline">\frac{k}{k+1}</span> (<span class="math inline">k</span> is the number of iterations), and if you are solving a strongly convex problem, also <span class="math inline">\frac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}</span>. Plot the convergence of the method as a function of the number of iterations (choose the convergence criterion yourself) for different values of the momentum. Is the convergence always monotonic?</p></li>
<li><p>For the best value of momentum <span class="math inline">\beta</span>, plot the dependence of the model accuracy on the test sample on the running time of the method. Add this graph to the graphs for the heavy ball and gradient descent from the previous steps. Make a conclusion.</p></li>
<li><p>Now we drop the estimated value of <span class="math inline">L</span> and will try to do it adaptively. Let us make the selection of the constant <span class="math inline">L</span> adaptive.</p>
<p><span class="math display">
f(y) \leq f(x^k) + \langle \nabla f(x^k), y - x^k \rangle + \frac{L}{2}\|x^k - y\|_2^2
</span></p>
<p>In particular, the procedure might work:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backtracking_L(f, grad, x, h, L0, rho, maxiter<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> L0</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    fx <span class="op">=</span> f(x)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    gradx <span class="op">=</span> grad(x)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">iter</span> <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">iter</span> <span class="op">&lt;</span> maxiter :</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> x <span class="op">-</span> <span class="dv">1</span> <span class="op">/</span> L <span class="op">*</span> h</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> f(y) <span class="op">&lt;=</span> fx <span class="op">-</span> <span class="dv">1</span> <span class="op">/</span> L gradx.dot(h) <span class="op">+</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> L) h.dot(h):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>            L <span class="op">=</span> L <span class="op">*</span> rho</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">iter</span> <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> L</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>What should <span class="math inline">h</span> be taken as? Should <span class="math inline">\rho</span> be greater or less than <span class="math inline">1</span>? Should <span class="math inline">L_0</span> be taken as large or small? Draw a similar figure as it was in the previous step for L computed adaptively (6 lines - GD, HB, NAG, GD adaptive L, HB adaptive L, NAG adaptive L)</p></li>
</ol></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/mipt23\.fmin\.xyz");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/MerkulovDaniil/mipt23/edit/main/homework.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer><script>videojs(video_shortcode_videojs_video1);</script>




</body></html>