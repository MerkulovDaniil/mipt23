<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.521">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>üíÄ –î–æ–º–∞—à–∫–∞</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./favicon.svg" rel="icon" type="image/svg+xml">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js" integrity="sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A" crossorigin="anonymous"></script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<meta property="og:title" content="üíÄ –î–æ–º–∞—à–∫–∞">
<meta property="og:description" content="">
<meta name="twitter:title" content="üíÄ –î–æ–º–∞—à–∫–∞">
<meta name="twitter:description" content="">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./logo.svg" alt="mipt23.fmin.xyz" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./program.html"> 
<span class="menu-text">üöÄ –ü—Ä–æ–≥—Ä–∞–º–º–∞</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./homework.html" aria-current="page"> 
<span class="menu-text">üíÄ –î–æ–º–∞—à–∫–∞</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./projects.html"> 
<span class="menu-text">üèõ –ü—Ä–æ–µ–∫—Ç—ã</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/MerkulovDaniil/mipt23" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://www.youtube.com/watch?v=duPZb4AGz3c&amp;list=PLQSHEO58cjmMt8PY2H0ObJJ6FKbn0PoBx" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-youtube"></i></a>
    <a href="https://t.me/mipt23_fmin" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-telegram"></i></a>
    <a href="https://new.fmin.xyz" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-gem"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#matrix-calculus" id="toc-matrix-calculus" class="nav-link active" data-scroll-target="#matrix-calculus">Matrix calculus</a></li>
  <li><a href="#automatic-differentiation-and-jax" id="toc-automatic-differentiation-and-jax" class="nav-link" data-scroll-target="#automatic-differentiation-and-jax">Automatic differentiation and jax</a></li>
  <li><a href="#convex-sets" id="toc-convex-sets" class="nav-link" data-scroll-target="#convex-sets">Convex sets</a></li>
  <li><a href="#convex-functions" id="toc-convex-functions" class="nav-link" data-scroll-target="#convex-functions">Convex functions</a></li>
  <li><a href="#conjugate-sets" id="toc-conjugate-sets" class="nav-link" data-scroll-target="#conjugate-sets">Conjugate sets</a></li>
  <li><a href="#conjugate-functions" id="toc-conjugate-functions" class="nav-link" data-scroll-target="#conjugate-functions">Conjugate functions</a></li>
  <li><a href="#subgradient-and-subdifferential" id="toc-subgradient-and-subdifferential" class="nav-link" data-scroll-target="#subgradient-and-subdifferential">Subgradient and subdifferential</a></li>
  <li><a href="#kkt-and-duality" id="toc-kkt-and-duality" class="nav-link" data-scroll-target="#kkt-and-duality">KKT and duality</a></li>
  <li><a href="#linear-programming" id="toc-linear-programming" class="nav-link" data-scroll-target="#linear-programming">Linear programming</a></li>
  <li><a href="#sequence-convergence" id="toc-sequence-convergence" class="nav-link" data-scroll-target="#sequence-convergence">Sequence convergence</a></li>
  <li><a href="#line-search" id="toc-line-search" class="nav-link" data-scroll-target="#line-search">Line search</a></li>
  <li><a href="#zero-order-methods" id="toc-zero-order-methods" class="nav-link" data-scroll-target="#zero-order-methods">Zero-order methods</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/MerkulovDaniil/mipt23/edit/main/homework.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header">
</header>


<section id="matrix-calculus" class="level3">
<h3 class="anchored" data-anchor-id="matrix-calculus">Matrix calculus</h3>
<ol type="1">
<li><p>Given a matrix <span class="math inline">A</span> of size <span class="math inline">m \times n</span> and a vector <span class="math inline">x</span> of size <span class="math inline">n \times 1</span>, compute the gradient of the function <span class="math inline">f(x) = \text{tr}(A^T A x x^T)</span> with respect to <span class="math inline">x</span>.</p></li>
<li><p>Find the gradient <span class="math inline">\nabla f(x)</span> and hessian <span class="math inline">f''(x)</span>, if <span class="math inline">f(x) = \dfrac{1}{2} \Vert Ax - b\Vert^2_2</span>.</p></li>
<li><p>Find the gradient <span class="math inline">\nabla f(x)</span> and hessian <span class="math inline">f''(x)</span>, if <span class="math display">
f(x) = \frac1m \sum\limits_{i=1}^m \log \left( 1 + \exp(a_i^{T}x) \right) + \frac{\mu}{2}\Vert x\Vert _2^2, \; a_i, x \in \mathbb R^n, \; \mu&gt;0
</span></p></li>
<li><p>Compute the gradient <span class="math inline">\nabla_A f(A)</span> of the trace of the matrix exponential function <span class="math inline">f(A) = \text{tr}(e^A)</span> with respect to <span class="math inline">A</span>. Hint: hint: Use the definition of the matrix exponential. Use the definition of the differential <span class="math inline">df = f(A + dA) - f(A) + o(\Vert dA \Vert)</span> with the limit <span class="math inline">\Vert dA \Vert \to 0</span>.</p></li>
<li><p>Find the gradient <span class="math inline">\nabla f(x)</span> and hessian <span class="math inline">f''(x)</span>, if <span class="math inline">f(x) = \frac{1}{2}\Vert A - xx^T\Vert^2_F, A \in \mathbb{S}^n</span></p></li>
<li><p>Calculate the first and the second derivative of the following function <span class="math inline">f : S \to \mathbb{R}</span></p>
<p><span class="math display">
f(t) = \text{det}(A ‚àí tI_n),
</span></p>
<p>where <span class="math inline">A \in \mathbb{R}^{n \times n}, S := \{t \in \mathbb{R} : \text{det}(A ‚àí tI_n) \neq 0\}</span>.</p></li>
<li><p>Find the gradient <span class="math inline">\nabla f(X)</span>, if <span class="math inline">f(X) = \text{tr}\left( AX^2BX^{-\top} \right)</span>.</p></li>
</ol>
</section>
<section id="automatic-differentiation-and-jax" class="level3">
<h3 class="anchored" data-anchor-id="automatic-differentiation-and-jax">Automatic differentiation and jax</h3>
<p>You can use any automatic differentiation framework in this section (Jax, PyTorch, Autograd etc.)</p>
<ol type="1">
<li><p>You will work with the following function for this exercise, <span class="math display">
f(x,y)=e^{‚àí\left(sin(x)‚àícos(y)\right)^2}
</span><br>
Draw the computational graph for the function. Note, that it should contain only primitive operations - you need to do it automatically - <a href="https://bnikolic.co.uk/blog/python/jax/2022/02/22/jax-outputgraph-rev.html">jax example</a>, <a href="https://github.com/waleedka/hiddenlayer">PyTorch example</a> - you can google/find your way to visualize it.</p></li>
<li><p>Compare analytic and autograd (with any framework) approach for the calculation of the gradient of:<br>
<span class="math display">
f(A) = \text{tr}(e^A)
</span></p></li>
<li><p>We can use automatic differentiation not only to calculate necessary gradients but also for tuning hyperparameters of the algorithm like learning rate in gradient descent (with gradient descent ü§Ø). Suppose, we have the following function <span class="math inline">f(x) = \frac{1}{2}\Vert x\Vert^2</span>, select a random point <span class="math inline">x_0 \in \mathbb{B}^{1000} = \{0 \leq x_i \leq 1 \mid \forall i\}</span>. Consider <span class="math inline">10</span> steps of the gradient descent starting from the point <span class="math inline">x_0</span>: <span class="math display">
x_{k+1} = x_k - \alpha_k \nabla f(x_k)
</span> Your goal in this problem is to write the function, that takes <span class="math inline">10</span> scalar values <span class="math inline">\alpha_i</span> and return the result of the gradient descent on function <span class="math inline">L = f(x_{10})</span>. And optimize this function using gradient descent on <span class="math inline">\alpha \in \mathbb{R}^{10}</span>. Suppose that each of <span class="math inline">10</span> components of <span class="math inline">\alpha</span> is uniformly distributed on <span class="math inline">[0; 0.1]</span>. <span class="math display">
\alpha_{k+1} = \alpha_k - \beta \frac{\partial L}{\partial \alpha}
</span> Choose any constant <span class="math inline">\beta</span> and the number of steps you need. Describe the obtained results. How would you understand, that the obtained schedule (<span class="math inline">\alpha \in \mathbb{R}^{10}</span>) becomes better than it was at the start? How do you check numerically local optimality in this problem?</p></li>
<li><p>Compare analytic and autograd (with any framework) approach for the gradient of:<br>
<span class="math display">
f(X) = - \log \det X
</span></p></li>
<li><p>Compare analytic and autograd (with any framework) approach for the gradient and hessian of:<br>
<span class="math display">
f(x) = x^\top x x^\top x
</span></p></li>
</ol>
<hr>
</section>
<section id="convex-sets" class="level3">
<h3 class="anchored" data-anchor-id="convex-sets">Convex sets</h3>
<ol type="1">
<li>Show, that if <span class="math inline">S \subseteq \mathbb{R}^n</span> is convex set, then its interior <span class="math inline">\mathbf{int } S</span> and closure <span class="math inline">\bar{S}</span> are also convex sets.</li>
<li>Show, that <span class="math inline">\mathbf{conv}\{xx^\top: x \in \mathbb{R}^n, \Vert x\Vert = 1\} = \{A \in \mathbb{S}^n_+: \text{tr}(A) = 1\}</span>.</li>
<li>Let <span class="math inline">K \subseteq \mathbb{R}^n_+</span> is a cone. Prove that it is convex if and only if a set of <span class="math inline">\{x \in K \mid \sum\limits_{i=1}^n x_i = 1 \}</span> is convex.</li>
<li>Prove that the set of <span class="math inline">\{x \in \mathbb{R}^2 \mid e^{x_1}\le x_2\}</span> is convex.</li>
<li>Show that the set of directions of the non-strict local descending of the differentiable function in a point is a convex cone. (Previously, the question contained a typo ‚Äústrict local descending‚Äù)</li>
<li>Is the following set convex <span class="math display">
S = \left\{ a \in \mathbb{R}^k \mid p(0) = 1, \vert p(t) \vert\leq 1 \text{ for } \alpha\leq t \leq \beta\right\},
</span> where <span class="math display">
p(t) = a_1 + a_2 t + \ldots + a_k t^{k-1} \;?
</span></li>
</ol>
<hr>
</section>
<section id="convex-functions" class="level3">
<h3 class="anchored" data-anchor-id="convex-functions">Convex functions</h3>
<ol type="1">
<li><p>Consider the function <span class="math inline">f(x) = x^d</span>, where <span class="math inline">x \in \mathbb{R}_{+}</span>. Fill the following table with ‚úÖ or ‚ùé. Explain your answers</p>
<div class="table-responsive">
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">d</span></th>
<th style="text-align: center;">Convex</th>
<th style="text-align: center;">Concave</th>
<th style="text-align: center;">Strictly Convex</th>
<th style="text-align: center;"><span class="math inline">\mu</span>-strongly convex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">-2, x \in \mathbb{R}_{++}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">-1, x \in \mathbb{R}_{++}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">0</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">0.5</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">1</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\in (1; 2)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">2</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">&gt; 2</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div></li>
<li><p>Prove that the entropy function, defined as</p>
<p><span class="math display">
f(x) = -\sum_{i=1}^n x_i \log(x_i),
</span></p>
<p>with <span class="math inline">\text{dom}(f) = \{x \in \R^n_{++} : \sum_{i=1}^n x_i = 1\}</span>, is strictly concave.</p></li>
<li><p>Show, that the function <span class="math inline">f: \mathbb{R}^n_{++} \to \mathbb{R}</span> is convex if <span class="math inline">f(x) = - \prod\limits_{i=1}^n x_i^{\alpha_i}</span> if <span class="math inline">\mathbf{1}^T \alpha = 1, \alpha \succeq 0</span>.</p></li>
<li><p>Show that the maximum of a convex function <span class="math inline">f</span> over the polyhedron <span class="math inline">P = \text{conv}\{v_1, \ldots, v_k\}</span> is achieved at one of its vertices, i.e.,</p>
<p><span class="math display">
\sup_{x \in P} f(x) = \max_{i=1, \ldots, k} f(v_i).
</span></p>
<p>A stronger statement is: the maximum of a convex function over a closed bounded convex set is achieved at an extreme point, i.e., a point in the set that is not a convex combination of any other points in the set. (you do not have to prove it). <em>Hint:</em> Assume the statement is false, and use Jensen‚Äôs inequality.</p></li>
<li><p>Show, that the two definitions of <span class="math inline">\mu</span>-strongly convex functions are equivalent:</p>
<ol type="1">
<li><p><span class="math inline">f(x)</span> is <span class="math inline">\mu</span>-strongly convex <span class="math inline">\iff</span> for any <span class="math inline">x_1, x_2 \in S</span> and <span class="math inline">0 \le \lambda \le 1</span> for some <span class="math inline">\mu &gt; 0</span>:</p>
<p><span class="math display">
f(\lambda x_1 + (1 - \lambda)x_2) \le \lambda f(x_1) + (1 - \lambda)f(x_2) - \frac{\mu}{2} \lambda (1 - \lambda)\|x_1 - x_2\|^2
</span></p></li>
<li><p><span class="math inline">f(x)</span> is <span class="math inline">\mu</span>-strongly convex <span class="math inline">\iff</span> if there exists <span class="math inline">\mu&gt;0</span> such that the function <span class="math inline">f(x) - \dfrac{\mu}{2}\Vert x\Vert^2</span> is convex.</p></li>
</ol></li>
</ol>
</section>
<section id="conjugate-sets" class="level3">
<h3 class="anchored" data-anchor-id="conjugate-sets">Conjugate sets</h3>
<ol type="1">
<li><p>Let <span class="math inline">\mathbb{A}_n</span> be the set of all <span class="math inline">n</span> dimensional antisymmetric matrices (s.t. <span class="math inline">X^T = - X</span>). Show that <span class="math inline">\left( \mathbb{A}_n\right)^* = \mathbb{S}_n</span>.</p></li>
<li><p>Find the sets <span class="math inline">S^{*}, S^{**}, S^{***}</span>, if</p>
<p><span class="math display">
S = \{ x \in \mathbb{R}^2 \mid x_1 + x_2 \ge 0, \;\; -\dfrac12x_1 + x_2 \ge 0, \;\; 2x_1 + x_2 \ge -1 \;\; -2x_1 + x_2 \ge -3\}
</span></p></li>
<li><p>Prove, that <span class="math inline">B_p</span> and <span class="math inline">B_{p_*}</span> are inter-conjugate, i.e.&nbsp;<span class="math inline">(B_p)^* = B_{p_*}, (B_{p_*})^* = B_p</span>, where <span class="math inline">B_p</span> is the unit ball (w.r.t. <span class="math inline">p</span> - norm) and <span class="math inline">p, p_*</span> are conjugated, i.e.&nbsp;<span class="math inline">p^{-1} + p^{-1}_* = 1</span>. You can assume, that <span class="math inline">p_* = \infty</span> if <span class="math inline">p = 1</span> and vice versa.</p></li>
</ol>
<hr>
</section>
<section id="conjugate-functions" class="level3">
<h3 class="anchored" data-anchor-id="conjugate-functions">Conjugate functions</h3>
<ol type="1">
<li><p>Find <span class="math inline">f^*(y)</span>, if <span class="math inline">f(x) = \vert 2x \vert</span></p></li>
<li><p>Prove, that if <span class="math inline">f(x) = \inf\limits_{u+v = x} (g(u) + h(v))</span>, then <span class="math inline">f^*(y) = g^*(y) + h^*(y)</span>.</p></li>
<li><p>Find <span class="math inline">f^*(y)</span>, if <span class="math inline">f(x) = \log \left( \sum\limits_{i=1}^n e^{x_i} \right)</span></p></li>
<li><p>Prove, that if <span class="math inline">f(x) = g(Ax)</span>, then <span class="math inline">f^*(y) = g^*(A^{-\top}y)</span></p></li>
<li><p>Find <span class="math inline">f^*(Y)</span>, if <span class="math inline">f(X) = - \ln \det X, X \in \mathbb{S}^n_{++}</span></p></li>
<li><p>The scalar Huber function is defined as</p>
<p><span class="math display">
f_{\text{hub}}(x) =
\begin{cases}
\frac{1}{2} x^2 &amp; \text{if } |x| \leq 1 \\
|x| - \frac{1}{2} &amp; \text{if } |x| &gt; 1
\end{cases}
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./huber_function.svg" class="img-fluid figure-img"></p>
<figcaption>Scalar case</figcaption>
</figure>
</div>
<p>This convex function arises in various applications, notably in robust estimation. This problem explores the generalizations of the Huber function to <span class="math inline">\mathbb{R}^n</span>. A straightforward extension to <span class="math inline">\mathbb{R}^n</span> is expressed as <span class="math inline">f_{\text{hub}}(x_1) + \ldots + f_{\text{hub}}(x_n)</span>, yet this formulation is not circularly symmetric, that is, it‚Äôs not invariant under the transformation of <span class="math inline">x</span> by an orthogonal matrix. A circularly symmetric extension to <span class="math inline">\mathbb{R}^n</span> is given by</p>
<p><span class="math display">
f_{\text{cshub}}(x) = f_{\text{hub}}(\Vert x\Vert )=
\begin{cases}
\frac{1}{2} \Vert x\Vert_2 ^2 &amp; \text{if } \Vert x\Vert_2 \leq 1 \\
\Vert x\Vert_2 - \frac{1}{2} &amp; \text{if } \Vert x\Vert_2 &gt; 1
\end{cases}
</span></p>
<p>where the subscript denotes ‚Äúcircularly symmetric Huber function‚Äù. Show, that <span class="math inline">f_{\text{cshub}}</span> is convex. Find the conjugate function <span class="math inline">f^*(y)</span>.</p></li>
</ol>
<hr>
</section>
<section id="subgradient-and-subdifferential" class="level3">
<h3 class="anchored" data-anchor-id="subgradient-and-subdifferential">Subgradient and subdifferential</h3>
<ol type="1">
<li>Find <span class="math inline">\partial f(x)</span>, if <span class="math display">
f(x) = \text{Parametric ReLU}(x) = \begin{cases}
     x &amp; \text{if } x &gt; 0, \\
     ax &amp; \text{otherwise}.
\end{cases}
</span></li>
<li>Prove, that <span class="math inline">x_0</span> - is the minimum point of a function <span class="math inline">f(x)</span> if and only if <span class="math inline">0 \in \partial f(x_0)</span>.</li>
<li>Find <span class="math inline">\partial f(x)</span>, if <span class="math inline">f(x) = \Vert Ax - b\Vert _1</span>.</li>
<li>Find <span class="math inline">\partial f(x)</span>, if <span class="math inline">f(x) = e^{\Vert x\Vert}</span>.</li>
<li>Find <span class="math inline">\partial f(x)</span>, if <span class="math inline">f(x) = \frac12 \Vert Ax - b\Vert _2^2 + \lambda \Vert x\Vert_1, \quad \lambda &gt; 0</span>.</li>
<li>Let <span class="math inline">S \subseteq \mathbb{R}^n</span> be a convex set. We will call a <em>normal cone</em> of the set <span class="math inline">S</span> at a point <span class="math inline">x</span> the following set: <span class="math display">
N_S(x) = \left\{c \in \mathbb{R}^n : \langle c, y-x\rangle \leq 0 \quad \forall y \in S\right\}
</span>
<ol type="i">
<li><p>Draw a normal cone for a set at the points <span class="math inline">A, B, C, D, E, F</span> on the figure below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./normal_cone.svg" class="img-fluid figure-img"></p>
<figcaption>Draw a normal cone for the set <span class="math inline">S</span> in these points</figcaption>
</figure>
</div></li>
<li><p>Show, that <span class="math inline">N_S(x) = \{0\} \quad \forall x \in \mathbf{ri }(S)</span>.</p></li>
<li><p>Show, that the subdifferential <span class="math inline">\partial I_S(x) = N_S(x)</span> if <span class="math inline">I_S(x)</span> is the indicator function, i.e.&nbsp; <span class="math display">
I_S(x) = \begin{cases}0,\text{if } x \in S\\ \infty, \text{otherwise}\end{cases}
</span></p></li>
</ol></li>
</ol>
<hr>
</section>
<section id="kkt-and-duality" class="level3">
<h3 class="anchored" data-anchor-id="kkt-and-duality">KKT and duality</h3>
<p>In this section, you can consider either the arbitrary norm or the Euclidian norm if nothing else is specified.</p>
<ol type="1">
<li><p><strong>Toy example</strong> <span class="math display">
\begin{split}
&amp; x^2 + 1 \to \min\limits_{x \in \mathbb{R} }\\
\text{s.t. } &amp; (x-2)(x-4) \leq 0
\end{split}
</span></p>
<ol type="1">
<li>Give the feasible set, the optimal value, and the optimal solution.</li>
<li>Plot the objective <span class="math inline">x^2 +1</span> versus <span class="math inline">x</span>. On the same plot, show the feasible set, optimal point, and value, and plot the Lagrangian <span class="math inline">L(x,\mu)</span> versus <span class="math inline">x</span> for a few positive values of <span class="math inline">\mu</span>. Verify the lower bound property (<span class="math inline">p^* \geq \inf_x L(x, \mu)</span>for <span class="math inline">\mu \geq 0</span>). Derive and sketch the Lagrange dual function <span class="math inline">g</span>.</li>
<li>State the dual problem, and verify that it is a concave maximization problem. Find the dual optimal value and dual optimal solution <span class="math inline">\mu^*</span>. Does strong duality hold?</li>
<li>Let <span class="math inline">p^*(u)</span> denote the optimal value of the problem</li>
</ol>
<p><span class="math display">
\begin{split}
&amp; x^2 + 1 \to \min\limits_{x \in \mathbb{R} }\\
\text{s.t. } &amp; (x-2)(x-4) \leq u
\end{split}
</span></p>
<p>as a function of the parameter <span class="math inline">u</span>. Plot <span class="math inline">p^*(u)</span>. Verify that <span class="math inline">\dfrac{dp^*(0)}{du} = -\mu^*</span></p></li>
<li><p>Derive the dual problem for the Ridge regression problem with <span class="math inline">A \in \mathbb{R}^{m \times n}, b \in \mathbb{R}^m, \lambda &gt; 0</span>:</p>
<p><span class="math display">
\begin{split}
\dfrac{1}{2}\|y-b\|^2 + \dfrac{\lambda}{2}\|x\|^2 &amp;\to \min\limits_{x \in \mathbb{R}^n, y \in \mathbb{R}^m }\\
\text{s.t. } &amp; y = Ax
\end{split}
</span></p></li>
<li><p>Derive the dual problem for the support vector machine problem with <span class="math inline">A \in \mathbb{R}^{m \times n}, \mathbf{1} \in \mathbb{R}^m \in \mathbb{R}^m, \lambda &gt; 0</span>:</p>
<p><span class="math display">
\begin{split}
\langle \mathbf{1}, t\rangle + \dfrac{\lambda}{2}\|x\|^2 &amp;\to \min\limits_{x \in \mathbb{R}^n, t \in \mathbb{R}^m }\\
\text{s.t. } &amp; Ax \succeq \mathbf{1} - t \\
&amp; t \succeq 0
\end{split}
</span></p></li>
<li><p>Give an explicit solution to the following LP.</p>
<p><span class="math display">
\begin{split}
&amp; c^\top x \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; 1^\top x = 1, \\
&amp; x \succeq 0
\end{split}
</span></p>
<p>This problem can be considered the simplest portfolio optimization problem.</p></li>
<li><p>Show, that the following problem has a unique solution and find it:</p>
<p><span class="math display">
\begin{split}
&amp; \langle C^{-1}, X\rangle - \log \det X \to \min\limits_{x \in \mathbb{R}^{n \times n} }\\
\text{s.t. } &amp; \langle Xa, a\rangle \leq 1,
\end{split}
</span></p>
<p>where <span class="math inline">C \in \mathbb{S}^n_{++}, a \in \mathbb{R}^n \neq 0</span>. The answer should not involve inversion of the matrix <span class="math inline">C</span>.</p></li>
<li><p>Give an explicit solution to the following QP.</p>
<p><span class="math display">
\begin{split}
&amp; c^\top x \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; (x - x_c)^\top A (x - x_c) \leq 1,
\end{split}
</span></p>
<p>where <span class="math inline">A \in \mathbb{S}^n_{++}, c \neq 0, x_c \in \mathbb{R}^n</span>.</p></li>
<li><p>Consider the equality-constrained least-squares problem</p>
<p><span class="math display">
\begin{split}
&amp; \|Ax - b\|_2^2 \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; Cx = d,
\end{split}
</span></p>
<p>where <span class="math inline">A \in \mathbb{R}^{m \times n}</span> with <span class="math inline">\mathbf{rank }A = n</span>, and <span class="math inline">C \in \mathbb{R}^{k \times n}</span> with <span class="math inline">\mathbf{rank }C = k</span>. Give the KKT conditions, and derive expressions for the primal solution <span class="math inline">x^*</span> and the dual solution <span class="math inline">\lambda^*</span>.</p></li>
<li><p>Derive the KKT conditions for the problem</p>
<p><span class="math display">
\begin{split}
&amp; \mathbf{tr \;}X - \log\text{det }X \to \min\limits_{X \in \mathbb{S}^n_{++} }\\
\text{s.t. } &amp; Xs = y,
\end{split}
</span></p>
<p>where <span class="math inline">y \in \mathbb{R}^n</span> and <span class="math inline">s \in \mathbb{R}^n</span> are given with <span class="math inline">y^\top s = 1</span>. Verify that the optimal solution is given by</p>
<p><span class="math display">
X^* = I + yy^\top - \dfrac{1}{s^\top s}ss^\top
</span></p></li>
<li><p><strong>Supporting hyperplane interpretation of KKT conditions</strong>. Consider a <strong>convex</strong> problem with no equality constraints</p>
<p><span class="math display">
\begin{split}
&amp; f_0(x) \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; f_i(x) \leq 0, \quad i = [1,m]
\end{split}
</span></p>
<p>Assume, that <span class="math inline">\exists x^* \in \mathbb{R}^n, \mu^* \in \mathbb{R}^m</span> satisfy the KKT conditions</p>
<p><span class="math display">
\begin{split}
&amp; \nabla_x L (x^*, \mu^*) = \nabla f_0(x^*) + \sum\limits_{i=1}^m\mu_i^*\nabla f_i(x^*) = 0 \\
&amp; \mu^*_i \geq 0, \quad i = [1,m] \\
&amp; \mu^*_i f_i(x^*) = 0, \quad i = [1,m]\\
&amp; f_i(x^*) \leq 0, \quad i = [1,m]
\end{split}
</span></p>
<p>Show that</p>
<p><span class="math display">
\nabla f_0(x^*)^\top (x - x^*) \geq 0
</span></p>
<p>for all feasible <span class="math inline">x</span>. In other words, the KKT conditions imply the simple optimality criterion or <span class="math inline">\nabla f_0(x^*)</span> defines a supporting hyperplane to the feasible set at <span class="math inline">x^*</span>.</p></li>
<li><p><strong>Fenchel + Lagrange = ‚ô•.</strong> Express the dual problem of</p>
<p><span class="math display">
\begin{split}
&amp; c^\top x\to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; f(x) \leq 0
\end{split}
</span></p>
<p>with <span class="math inline">c \neq 0</span>, in terms of the conjugate function <span class="math inline">f^*</span>. Explain why the problem you give is convex. We do not assume <span class="math inline">f</span> is convex.</p></li>
<li><p><strong>A penalty method for equality constraints.</strong> We consider the problem of minimization</p>
<p><span class="math display">
\begin{split}
&amp; f_0(x) \to \min\limits_{x \in \mathbb{R}^{n} }\\
\text{s.t. } &amp; Ax = b,
\end{split}
</span></p>
<p>where $f_0(x): ^n $ is convex and differentiable, and <span class="math inline">A \in \mathbb{R}^{m \times n}</span> with <span class="math inline">\mathbf{rank }A = m</span>. In a quadratic penalty method, we form an auxiliary function</p>
<p><span class="math display">
\phi(x) = f_0(x) + \alpha \|Ax - b\|_2^2,
</span></p>
<p>where <span class="math inline">\alpha &gt; 0</span> is a parameter. This auxiliary function consists of the objective plus the penalty term <span class="math inline">\alpha \Vert Ax - b\Vert_2^2</span>. The idea is that a minimizer of the auxiliary function, <span class="math inline">\tilde{x}</span>, should be an approximate solution to the original problem. Intuition suggests that the larger the penalty weight <span class="math inline">\alpha</span>, the better the approximation <span class="math inline">\tilde{x}</span> to a solution of the original problem. Suppose <span class="math inline">\tilde{x}</span> is a minimizer of <span class="math inline">\phi(x)</span>. Show how to find, from <span class="math inline">\tilde{x}</span>, a dual feasible point for the original problem. Find the corresponding lower bound on the optimal value of the original problem.</p></li>
<li><p><strong>Analytic centering.</strong> Derive a dual problem for</p>
<p><span class="math display">
-\sum_{i=1}^m \log (b_i - a_i^\top x) \to \min\limits_{x \in \mathbb{R}^{n} }
</span></p>
<p>with domain <span class="math inline">\{x \mid a^\top_i x &lt; b_i , i = [1,m]\}</span>.</p>
<p>First introduce new variables <span class="math inline">y_i</span> and equality constraints <span class="math inline">y_i = b_i ‚àí a^\top_i x</span>. (The solution to this problem is called the analytic center of the linear inequalities <span class="math inline">a^\top_i x \leq b_i ,i = [1,m]</span>. Analytic centers have geometric applications, and play an important role in barrier methods.)</p></li>
</ol>
<hr>
</section>
<section id="linear-programming" class="level3">
<h3 class="anchored" data-anchor-id="linear-programming">Linear programming</h3>
<ol type="1">
<li><p><strong>üì±üéßüíª Covers manufacturing.</strong> Lyzard Corp is producing covers for the following products:</p>
<ul>
<li>üì± phones</li>
<li>üéß headphones</li>
<li>üíª laptops</li>
</ul>
<p>The company‚Äôs production facilities are such that if we devote the entire production to headphone covers, we can produce 5000 of them in one day. If we devote the entire production to phone covers or laptop covers, we can produce 4000 or 2000 of them in one day.</p>
<p>The production schedule is one week (6 working days), and the week‚Äôs production must be stored before distribution. Storing 1000 headphone covers (packaging included) takes up 30 cubic feet of space. Storing 1000 phone covers (packaging included) takes up 50 cubic feet of space, and storing 1000 laptop covers (packaging included) takes up 220 cubic feet of space. The total storage space available is 1500 cubic feet.</p>
<p>Due to commercial agreements with Lyzard Corp has to deliver at least 4500 headphone covers and 4000 laptop covers per week to strengthen the product‚Äôs diffusion.</p>
<p>The marketing department estimates that the weekly demand for headphones covers, phone, and laptop covers does not exceed 10000 14000, and 7000 units, therefore the company does not want to produce more than these amounts for headphones, phone, and laptop covers.</p>
<p>Finally, the net profit per headphone cover, phone cover, and laptop cover are $5, $7, and $12, respectively.</p>
<p>The aim is to determine a weekly production schedule that maximizes the total net profit.</p>
<ol type="1">
<li><p>Write a Linear Programming formulation for the problem. Use the following variables:</p>
<ul>
<li><span class="math inline">y_1</span> = number of headphones covers produced over the week,<br>
</li>
<li><span class="math inline">y_2</span> = number of phone covers produced over the week,<br>
</li>
<li><span class="math inline">y_3</span> = number of laptop covers produced over the week.</li>
</ul></li>
<li><p>Find the solution to the problem using <a href="http://www.pyomo.org">PyOMO</a></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install pyomo</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> sudo apt<span class="op">-</span>get install glpk<span class="op">-</span>utils <span class="op">--</span>quiet  <span class="co"># GLPK</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> sudo apt<span class="op">-</span>get install coinor<span class="op">-</span>cbc <span class="op">--</span>quiet  <span class="co"># CoinOR</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Perform the sensitivity analysis. Which constraint could be relaxed to increase the profit the most? Prove it numerically.</p></li>
</ol></li>
<li><p>Prove the optimality of the solution</p>
<p><span class="math display">
x = \left(\frac{7}{3} , 0, \frac{1}{3}\right)^T
</span></p>
<p>to the following linear programming problem:</p>
<p><span class="math display">
\begin{split}
&amp; 9x_1 + 3x_2 + 7x_3 \to \max\limits_{x \in \mathbb{R}^3 }\\
\text{s.t. } &amp; 2x_1 + x_2 + 3x_3 \leq 6 \\
&amp; 5x_1 + 4x_2 + x_3 \leq 12 \\
&amp; 3x_3 \leq 1,\\
&amp; x_1, x_2, x_3 \geq 0
\end{split}
</span></p>
<p>but you cannot use any numerical algorithm here.</p></li>
<li><p>Transform the following linear program into an equivalent linear program in the standard form <span class="math inline">\left(c^\top x \to \min\limits_{x\in \mathbb{R}^n} : Ax = b,x ‚â• 0\right)</span>:</p>
<p><span class="math display">
\begin{split}
&amp; x_1‚àíx_2 \to \min\limits_{x \in \mathbb{R}^2 }\\
\text{s.t. } &amp; 2x_1 + x_2 \geq 3 \\
&amp; 3x_1 ‚àí x_2 \leq 7 \\
&amp; x_1 \geq 0
\end{split}
</span></p></li>
<li><p>Consider:</p>
<p><span class="math display">
\begin{split}
&amp; 4x_1 + 5x_2 + 2x_3 \to \max\limits_{x \in \mathbb{R}^3 }\\
\text{s.t. } &amp; 2x_1 - x_2 + 2x_3 \leq 9 \\
&amp; 3x_1 + 5x_2 + 4x_3 \leq 8 \\
&amp; x_1 + x_2 + 2x_3 \leq 2 \\
&amp; x_1, x_2, x_3 \geq 0,
\end{split}
</span></p>
<ol type="1">
<li>Find an optimal solution to the Linear Programming problem using the simplex method.</li>
<li>Write the dual linear program. Find an optimal dual solution. Do we have strong duality here?</li>
</ol></li>
</ol>
<hr>
</section>
<section id="sequence-convergence" class="level3">
<h3 class="anchored" data-anchor-id="sequence-convergence">Sequence convergence</h3>
<ol type="1">
<li><p>Determine the convergence or divergence of a given sequences</p>
<ul>
<li><span class="math inline">r_{k} = \frac{1}{\sqrt{k}}</span>.</li>
<li><span class="math inline">r_{k} = 0.606^k</span>.</li>
<li><span class="math inline">r_{k} = 0.606^{2^k}</span>.</li>
</ul></li>
<li><p>Determine the convergence or divergence of a given sequence <span class="math inline">r_k =\begin{cases} \frac{1}{k}, &amp; \text{if } k\text{ is even} \\ e^{-k}, &amp; \text{if } k\text{ is odd} \end{cases}</span>.</p></li>
<li><p>Determine the following sequence <span class="math inline">\{r_k\}</span> by convergence rate (linear, sublinear, superlinear). In the case of superlinear convergence, additionally, find out whether there is quadratic convergence.</p>
<p><span class="math display">
r_k = \dfrac{1}{k!}
</span></p></li>
<li><p>Determine the following sequence <span class="math inline">\{r_k\}</span> by convergence rate (linear, sublinear, superlinear). In the case of superlinear convergence, additionally find out whether there is quadratic convergence.</p>
<p><span class="math display">
r_k = \dfrac{1}{k^k}
</span></p></li>
<li><p>Let <span class="math inline">\{r_k\}</span> be a sequence of non-negative numbers given as <span class="math inline">r_{k+1} = Mr_k^2</span>, where <span class="math inline">M &gt; 0</span>, <span class="math inline">r_0 \geq 0</span>. Establish a necessary and sufficient condition on <span class="math inline">M</span> and <span class="math inline">r_0</span> under which the sequence <span class="math inline">r_k</span> will converge to zero. What is the rate of convergence?</p></li>
</ol>
</section>
<section id="line-search" class="level3">
<h3 class="anchored" data-anchor-id="line-search">Line search</h3>
<ol type="1">
<li><p>Show that for a one-dimensional quadratic function decreasing at zero, its minimum satisfies Armijo‚Äôs condition for any <span class="math inline">c_1: 0 \leq c_1 \leq \dfrac12</span>:</p>
<p><span class="math display">
f(x_k - \alpha \nabla f (x_k)) \leq f(x_k) - c_1 \cdot \alpha\|\nabla f(x_k)\|_2^2
</span></p></li>
<li><p><strong>Implementing and Testing Line Search Conditions in Gradient Descent</strong></p>
<p><span class="math display">
x_{k+1} = x_k - \alpha \nabla f(x_k)
</span></p>
<p>In this assignment, you will modify an existing Python code for gradient descent to include various line search conditions. You will test these modifications on two functions: a quadratic function and the Rosenbrock function. The main objectives are to understand how different line search strategies influence the convergence of the gradient descent algorithm and to compare their efficiencies based on the number of function evaluations.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize_scalar</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">214</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the quadratic function and its gradient</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quadratic_function(x, A, b):</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> np.dot(x.T, np.dot(A, x)) <span class="op">-</span> np.dot(b.T, x)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad_quadratic(x, A, b):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(A, x) <span class="op">-</span> b</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a 2D quadratic problem with a specified condition number</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_quadratic_problem(cond_number):</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Random symmetric matrix</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> np.random.randn(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> np.dot(M, M.T)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ensure the matrix has the desired condition number</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    U, s, V <span class="op">=</span> np.linalg.svd(M)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> np.linspace(cond_number, <span class="dv">1</span>, <span class="bu">len</span>(s))  <span class="co"># Spread the singular values</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.dot(U, np.dot(np.diag(s), V))</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Random b</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> np.random.randn(<span class="dv">2</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A, b</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient descent function</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(start_point, A, b, stepsize_func, max_iter<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> start_point.copy()</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    trajectory <span class="op">=</span> [x.copy()]</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> grad_quadratic(x, A, b)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        step_size <span class="op">=</span> stepsize_func(x, grad)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">-=</span> step_size <span class="op">*</span> grad</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        trajectory.append(x.copy())</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(trajectory)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Backtracking line search strategy using scipy</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backtracking_line_search(x, grad, A, b, alpha<span class="op">=</span><span class="fl">0.3</span>, beta<span class="op">=</span><span class="fl">0.8</span>):</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> objective(t):</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> quadratic_function(x <span class="op">-</span> t <span class="op">*</span> grad, A, b)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> minimize_scalar(objective, method<span class="op">=</span><span class="st">'golden'</span>)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res.x</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ill-posed problem</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>cond_number <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>A, b <span class="op">=</span> generate_quadratic_problem(cond_number)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Starting point</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>start_point <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="fl">1.8</span>])</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform gradient descent with both strategies</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>trajectory_fixed <span class="op">=</span> gradient_descent(start_point, A, b, <span class="kw">lambda</span> x, g: <span class="fl">5e-2</span>)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>trajectory_backtracking <span class="op">=</span> gradient_descent(start_point, A, b, <span class="kw">lambda</span> x, g: backtracking_line_search(x, g, A, b))</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the trajectories on a contour plot</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>x1, x2 <span class="op">=</span> np.meshgrid(np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">400</span>), np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">400</span>))</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.array([quadratic_function(np.array([x, y]), A, b) <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(x1.flatten(), x2.flatten())]).reshape(x1.shape)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>plt.contour(x1, x2, Z, levels<span class="op">=</span><span class="dv">50</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>plt.plot(trajectory_fixed[:, <span class="dv">0</span>], trajectory_fixed[:, <span class="dv">1</span>], <span class="st">'o-'</span>, label<span class="op">=</span><span class="st">'Fixed Step Size'</span>)</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>plt.plot(trajectory_backtracking[:, <span class="dv">0</span>], trajectory_backtracking[:, <span class="dv">1</span>], <span class="st">'o-'</span>, label<span class="op">=</span><span class="st">'Backtracking Line Search'</span>)</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a><span class="co"># Add markers for start and optimal points</span></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>plt.plot(start_point[<span class="dv">0</span>], start_point[<span class="dv">1</span>], <span class="st">'ro'</span>, label<span class="op">=</span><span class="st">'Start Point'</span>)</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>optimal_point <span class="op">=</span> np.linalg.solve(A, b)</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>plt.plot(optimal_point[<span class="dv">0</span>], optimal_point[<span class="dv">1</span>], <span class="st">'y*'</span>, markersize<span class="op">=</span><span class="dv">15</span>, label<span class="op">=</span><span class="st">'Optimal Point'</span>)</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Gradient Descent Trajectories on Quadratic Function'</span>)</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x1'</span>)</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'x2'</span>)</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">"linesearch.svg"</span>)</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="linesearch.svg" class="img-fluid figure-img"></p>
<figcaption>The code above plots this</figcaption>
</figure>
</div>
<p>Start by reviewing the provided Python code. This code implements gradient descent with a fixed step size and a backtracking line search on a quadratic function. Familiarize yourself with how the gradient descent function and the step size strategies are implemented.</p>
<ol type="1">
<li><p>Modify the gradient descent function to include the following line search conditions:</p>
<ol type="a">
<li>Sufficient Decrease Condition</li>
<li>Curvature Condition</li>
<li>Goldstein Condition</li>
<li>Wolfe Condition</li>
<li>Dichotomy</li>
</ol>
<p>Test your modified gradient descent algorithm with the implemented line search conditions on the provided quadratic function. Plot the trajectories over iterations for each condition. Choose and specify hyperparameters for inexact line search condition. Choose and specify the termination criterion. Start from the point <span class="math inline">x_0 = (-1, 2)^T</span>.</p></li>
<li><p>Compare these 7 methods from the budget perspective. Plot the graph of function value from the number of function evaluations for each method on the same graph.</p></li>
<li><p>Plot trajectory for another function with the same set of methods</p>
<p><span class="math display">
f(x_1, x_2) =  10(x_2 ‚àí x_1^2)^2 + (x_1 ‚àí 1)^2
</span></p>
<p>with <span class="math inline">x_0 = (-1, 2)^T</span>. You might need to adjust hyperparameters.</p></li>
<li><p>Plot the same function value from the number of function calls for this experiment.</p></li>
</ol></li>
</ol>
</section>
<section id="zero-order-methods" class="level3">
<h3 class="anchored" data-anchor-id="zero-order-methods">Zero-order methods</h3>
<ol type="1">
<li><p>Solve approximately the Travelling Salesman Problem with any zero-order optimization method.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="salesman_problem.svg" class="img-fluid figure-img"></p>
<figcaption>Illustration of TSP</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial <span class="im">import</span> distance_matrix</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_random_symmetric_tsp(num_cities, seed<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    np.random.seed(seed)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> np.random.rand(num_cities, <span class="dv">2</span>)  <span class="co"># Generate random coordinates</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    dist_matrix <span class="op">=</span> distance_matrix(points, points)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    dist_matrix <span class="op">=</span> (dist_matrix <span class="op">+</span> dist_matrix.T) <span class="op">/</span> <span class="dv">2</span>  <span class="co"># Ensure symmetry</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.array(dist_matrix)  <span class="co"># Convert to JAX array for further processing</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>num_cities <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>dist_matrix <span class="op">=</span> generate_random_symmetric_tsp(num_cities)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dist_matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>You can use the <a href="https://fmin.xyz/docs/applications/salesman_problem.html">genetic</a> algorithm with any significant modification of mutation(5/10)</li>
<li>You can use another algorithm (10/10)</li>
</ul></li>
<li><p>In this assignment, we aim to explore and compare the efficacy of traditional and zero-order optimization methods in training a simple neural network. The conventional approach, Stochastic Gradient Descent (SGD), has been widely used due to its efficiency in handling large datasets and its capability to work in a huge-scale setting. This method relies on gradients of the loss function with respect to the network‚Äôs parameters. In contrast, zero-order optimization methods, also known as derivative-free methods, optimize without explicit gradient information. These methods are particularly useful for non-differentiable, noisy, or highly complex loss landscapes. The assignment‚Äôs objective is to explore such algorithms as Genetic Algorithms, Simulated Annealing, Gradient-free methods, or the Nelder-Mead method for real-world problems.</p>
<p>Note, that a variety of feed-forward neural networks could be represented as a series of linear transformations, followed by some nonlinear function (say, <span class="math inline">\text{ReLU }(x)</span>):</p>
<p><span class="math display">
\mathcal{NN}(x) = f_L \circ w_L \circ \ldots \circ f_1 \circ w_1 \circ x,
</span></p>
<p>where <span class="math inline">L</span> is the number of layers, <span class="math inline">f_i</span> - non-linear activation function, <span class="math inline">w_i = W_i x + b_i</span> - linear layer. We can denote the training data by <span class="math inline">X</span> and the labels by <span class="math inline">y</span>. The overall optimization problem here is to train the neural network to approximate the mapping <span class="math inline">X \to y</span>, i.e.&nbsp;<span class="math inline">\mathcal{NN}(X)</span> should be as close to <span class="math inline">y</span> as possible for all data points. We can ensure this by minimizing the loss function, which depends on the neural network parameters:</p>
<p><span class="math display">
\mathcal{L}(\mathcal{NN}(X, W, b), y) \to \min_{W, b}
</span></p>
<p>Typically, we use a cross-entropy loss function for the classification task. Do not worry if you are not familiar with neural networks or machine learning. Try to focus on the optimization algorithm here. You are provided with an example of this approach in the <a href="https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/NN_ES.ipynb">Colab notebook</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ES_vs_SGD-8k.svg" class="img-fluid figure-img"></p>
<figcaption>Comparison of SGD vs Evolutionary strategy for neural network without hidden layer.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ES_vs_SGD-8k.svg" class="img-fluid figure-img"></p>
<figcaption>Comparison of SGD vs Evolutionary strategy for neural network with several hidden layers.</figcaption>
</figure>
</div>
<p>The assignment requires you to implement a chosen zero-order optimization algorithm and compare its performance against SGD in training a predefined simple neural network (you can vary the structure of the network as you want for this problem). The comparison should focus on aspects such as convergence speed, final accuracy, and computational efficiency. Students should provide a name of the chosen zero-order algorithm and implement it in Python. You can use any method you want except the Evolutionary strategy, which is already in the example above.</p></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/mipt23\.fmin\.xyz");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>