<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>üíÄ –î–æ–º–∞—à–∫–∞</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./favicon.svg" rel="icon" type="image/svg+xml">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js" integrity="sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A" crossorigin="anonymous"></script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="üíÄ –î–æ–º–∞—à–∫–∞">
<meta property="og:description" content="">
<meta name="twitter:title" content="üíÄ –î–æ–º–∞—à–∫–∞">
<meta name="twitter:description" content="">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./logo.svg" alt="mipt23.fmin.xyz" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./program.html"> 
<span class="menu-text">üöÄ –ü—Ä–æ–≥—Ä–∞–º–º–∞</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./homework.html" aria-current="page"> 
<span class="menu-text">üíÄ –î–æ–º–∞—à–∫–∞</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./projects.html"> 
<span class="menu-text">üèõ –ü—Ä–æ–µ–∫—Ç—ã</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/MerkulovDaniil/mipt23" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://www.youtube.com/watch?v=duPZb4AGz3c&amp;list=PLQSHEO58cjmMt8PY2H0ObJJ6FKbn0PoBx" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-youtube"></i></a>
    <a href="https://t.me/mipt23_fmin" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-telegram"></i></a>
    <a href="https://fmin.xyz" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-gem"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#matrix-calculus" id="toc-matrix-calculus" class="nav-link active" data-scroll-target="#matrix-calculus">Matrix calculus</a></li>
  <li><a href="#automatic-differentiation-and-jax" id="toc-automatic-differentiation-and-jax" class="nav-link" data-scroll-target="#automatic-differentiation-and-jax">Automatic differentiation and jax</a></li>
  <li><a href="#convex-sets" id="toc-convex-sets" class="nav-link" data-scroll-target="#convex-sets">Convex sets</a></li>
  <li><a href="#convex-functions" id="toc-convex-functions" class="nav-link" data-scroll-target="#convex-functions">Convex functions</a></li>
  <li><a href="#conjugate-sets" id="toc-conjugate-sets" class="nav-link" data-scroll-target="#conjugate-sets">Conjugate sets</a></li>
  <li><a href="#conjugate-functions" id="toc-conjugate-functions" class="nav-link" data-scroll-target="#conjugate-functions">Conjugate functions</a></li>
  <li><a href="#subgradient-and-subdifferential" id="toc-subgradient-and-subdifferential" class="nav-link" data-scroll-target="#subgradient-and-subdifferential">Subgradient and subdifferential</a></li>
  <li><a href="#kkt-and-duality" id="toc-kkt-and-duality" class="nav-link" data-scroll-target="#kkt-and-duality">KKT and duality</a></li>
  <li><a href="#linear-programming" id="toc-linear-programming" class="nav-link" data-scroll-target="#linear-programming">Linear programming</a></li>
  <li><a href="#sequence-convergence" id="toc-sequence-convergence" class="nav-link" data-scroll-target="#sequence-convergence">Sequence convergence</a></li>
  <li><a href="#line-search" id="toc-line-search" class="nav-link" data-scroll-target="#line-search">Line search</a></li>
  <li><a href="#zero-order-methods" id="toc-zero-order-methods" class="nav-link" data-scroll-target="#zero-order-methods">Zero-order methods</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient Descent</a></li>
  <li><a href="#subgradient-descent" id="toc-subgradient-descent" class="nav-link" data-scroll-target="#subgradient-descent">Subgradient Descent</a></li>
  <li><a href="#accelerated-methods" id="toc-accelerated-methods" class="nav-link" data-scroll-target="#accelerated-methods">Accelerated methods</a></li>
  <li><a href="#gradient-methods-for-conditional-problems" id="toc-gradient-methods-for-conditional-problems" class="nav-link" data-scroll-target="#gradient-methods-for-conditional-problems">Gradient methods for conditional problems</a></li>
  <li><a href="#conjugate-gradients" id="toc-conjugate-gradients" class="nav-link" data-scroll-target="#conjugate-gradients">Conjugate gradients</a></li>
  <li><a href="#newton-and-quasinewton-methods" id="toc-newton-and-quasinewton-methods" class="nav-link" data-scroll-target="#newton-and-quasinewton-methods">Newton and quasinewton methods</a></li>
  <li><a href="#proximal-gradient-method" id="toc-proximal-gradient-method" class="nav-link" data-scroll-target="#proximal-gradient-method">Proximal Gradient Method</a></li>
  <li><a href="#stochastic-gradient-methods" id="toc-stochastic-gradient-methods" class="nav-link" data-scroll-target="#stochastic-gradient-methods">Stochastic gradient methods</a></li>
  <li><a href="#big-models" id="toc-big-models" class="nav-link" data-scroll-target="#big-models">Big models</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/MerkulovDaniil/mipt23/edit/main/homework.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header">
</header>


<section id="matrix-calculus" class="level3">
<h3 class="anchored" data-anchor-id="matrix-calculus">Matrix calculus</h3>
<ol type="1">
<li><p>Given a matrix <span class="math inline">A</span> of size <span class="math inline">m \times n</span> and a vector <span class="math inline">x</span> of size <span class="math inline">n \times 1</span>, compute the gradient of the function <span class="math inline">f(x) = \text{tr}(A^T A x x^T)</span> with respect to <span class="math inline">x</span>.</p></li>
<li><p>Find the gradient <span class="math inline">\nabla f(x)</span> and hessian <span class="math inline">f''(x)</span>, if <span class="math inline">f(x) = \dfrac{1}{2} \Vert Ax - b\Vert^2_2</span>.</p></li>
<li><p>Find the gradient <span class="math inline">\nabla f(x)</span> and hessian <span class="math inline">f''(x)</span>, if <span class="math display">
f(x) = \frac1m \sum\limits_{i=1}^m \log \left( 1 + \exp(a_i^{T}x) \right) + \frac{\mu}{2}\Vert x\Vert _2^2, \; a_i, x \in \mathbb R^n, \; \mu&gt;0
</span></p></li>
<li><p>Compute the gradient <span class="math inline">\nabla_A f(A)</span> of the trace of the matrix exponential function <span class="math inline">f(A) = \text{tr}(e^A)</span> with respect to <span class="math inline">A</span>. Hint: hint: Use the definition of the matrix exponential. Use the definition of the differential <span class="math inline">df = f(A + dA) - f(A) + o(\Vert dA \Vert)</span> with the limit <span class="math inline">\Vert dA \Vert \to 0</span>.</p></li>
<li><p>Find the gradient <span class="math inline">\nabla f(x)</span> and hessian <span class="math inline">f''(x)</span>, if <span class="math inline">f(x) = \frac{1}{2}\Vert A - xx^T\Vert^2_F, A \in \mathbb{S}^n</span></p></li>
<li><p>Calculate the first and the second derivative of the following function <span class="math inline">f : S \to \mathbb{R}</span></p>
<p><span class="math display">
f(t) = \text{det}(A ‚àí tI_n),
</span></p>
<p>where <span class="math inline">A \in \mathbb{R}^{n \times n}, S := \{t \in \mathbb{R} : \text{det}(A ‚àí tI_n) \neq 0\}</span>.</p></li>
<li><p>Find the gradient <span class="math inline">\nabla f(X)</span>, if <span class="math inline">f(X) = \text{tr}\left( AX^2BX^{-\top} \right)</span>.</p></li>
</ol>
</section>
<section id="automatic-differentiation-and-jax" class="level3">
<h3 class="anchored" data-anchor-id="automatic-differentiation-and-jax">Automatic differentiation and jax</h3>
<p>You can use any automatic differentiation framework in this section (Jax, PyTorch, Autograd etc.)</p>
<ol type="1">
<li><p>You will work with the following function for this exercise, <span class="math display">
f(x,y)=e^{‚àí\left(sin(x)‚àícos(y)\right)^2}
</span><br>
Draw the computational graph for the function. Note, that it should contain only primitive operations - you need to do it automatically - <a href="https://bnikolic.co.uk/blog/python/jax/2022/02/22/jax-outputgraph-rev.html">jax example</a>, <a href="https://github.com/waleedka/hiddenlayer">PyTorch example</a> - you can google/find your way to visualize it.</p></li>
<li><p>Compare analytic and autograd (with any framework) approach for the calculation of the gradient of:<br>
<span class="math display">
f(A) = \text{tr}(e^A)
</span></p></li>
<li><p>We can use automatic differentiation not only to calculate necessary gradients but also for tuning hyperparameters of the algorithm like learning rate in gradient descent (with gradient descent ü§Ø). Suppose, we have the following function <span class="math inline">f(x) = \frac{1}{2}\Vert x\Vert^2</span>, select a random point <span class="math inline">x_0 \in \mathbb{B}^{1000} = \{0 \leq x_i \leq 1 \mid \forall i\}</span>. Consider <span class="math inline">10</span> steps of the gradient descent starting from the point <span class="math inline">x_0</span>: <span class="math display">
x_{k+1} = x_k - \alpha_k \nabla f(x_k)
</span> Your goal in this problem is to write the function, that takes <span class="math inline">10</span> scalar values <span class="math inline">\alpha_i</span> and return the result of the gradient descent on function <span class="math inline">L = f(x_{10})</span>. And optimize this function using gradient descent on <span class="math inline">\alpha \in \mathbb{R}^{10}</span>. Suppose that each of <span class="math inline">10</span> components of <span class="math inline">\alpha</span> is uniformly distributed on <span class="math inline">[0; 0.1]</span>. <span class="math display">
\alpha_{k+1} = \alpha_k - \beta \frac{\partial L}{\partial \alpha}
</span> Choose any constant <span class="math inline">\beta</span> and the number of steps you need. Describe the obtained results. How would you understand, that the obtained schedule (<span class="math inline">\alpha \in \mathbb{R}^{10}</span>) becomes better than it was at the start? How do you check numerically local optimality in this problem?</p></li>
<li><p>Compare analytic and autograd (with any framework) approach for the gradient of:<br>
<span class="math display">
f(X) = - \log \det X
</span></p></li>
<li><p>Compare analytic and autograd (with any framework) approach for the gradient and hessian of:<br>
<span class="math display">
f(x) = x^\top x x^\top x
</span></p></li>
</ol>
<hr>
</section>
<section id="convex-sets" class="level3">
<h3 class="anchored" data-anchor-id="convex-sets">Convex sets</h3>
<ol type="1">
<li>Show, that if <span class="math inline">S \subseteq \mathbb{R}^n</span> is convex set, then its interior <span class="math inline">\mathbf{int } S</span> and closure <span class="math inline">\bar{S}</span> are also convex sets.</li>
<li>Show, that <span class="math inline">\mathbf{conv}\{xx^\top: x \in \mathbb{R}^n, \Vert x\Vert  = 1\} = \{A \in \mathbb{S}^n_+: \text{tr}(A) = 1\}</span>.</li>
<li>Let <span class="math inline">K \subseteq \mathbb{R}^n_+</span> is a cone. Prove that it is convex if and only if a set of <span class="math inline">\{x \in K \mid \sum\limits_{i=1}^n x_i = 1 \}</span> is convex.</li>
<li>Prove that the set of <span class="math inline">\{x \in \mathbb{R}^2 \mid e^{x_1}\le x_2\}</span> is convex.</li>
<li>Show that the set of directions of the non-strict local descending of the differentiable function in a point is a convex cone. (Previously, the question contained a typo ‚Äústrict local descending‚Äù)</li>
<li>Is the following set convex <span class="math display">
S = \left\{ a \in \mathbb{R}^k \mid p(0) = 1, \vert p(t) \vert\leq 1 \text{ for } \alpha\leq t \leq \beta\right\},
</span> where <span class="math display">
p(t) = a_1 + a_2 t + \ldots + a_k t^{k-1} \;?
</span></li>
</ol>
<hr>
</section>
<section id="convex-functions" class="level3">
<h3 class="anchored" data-anchor-id="convex-functions">Convex functions</h3>
<ol type="1">
<li><p>Consider the function <span class="math inline">f(x) = x^d</span>, where <span class="math inline">x \in \mathbb{R}_{+}</span>. Fill the following table with ‚úÖ or ‚ùé. Explain your answers</p>
<div class="table-responsive">
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">d</span></th>
<th style="text-align: center;">Convex</th>
<th style="text-align: center;">Concave</th>
<th style="text-align: center;">Strictly Convex</th>
<th style="text-align: center;"><span class="math inline">\mu</span>-strongly convex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">-2, x \in \mathbb{R}_{++}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">-1, x \in \mathbb{R}_{++}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">0</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">0.5</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">1</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\in (1; 2)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">2</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">&gt; 2</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div></li>
<li><p>Prove that the entropy function, defined as</p>
<p><span class="math display">
f(x) = -\sum_{i=1}^n x_i \log(x_i),
</span></p>
<p>with <span class="math inline">\text{dom}(f) = \{x \in \R^n_{++} : \sum_{i=1}^n x_i = 1\}</span>, is strictly concave.</p></li>
<li><p>Show, that the function <span class="math inline">f: \mathbb{R}^n_{++} \to \mathbb{R}</span> is convex if <span class="math inline">f(x) = - \prod\limits_{i=1}^n x_i^{\alpha_i}</span> if <span class="math inline">\mathbf{1}^T \alpha = 1, \alpha \succeq 0</span>.</p></li>
<li><p>Show that the maximum of a convex function <span class="math inline">f</span> over the polyhedron <span class="math inline">P = \text{conv}\{v_1, \ldots, v_k\}</span> is achieved at one of its vertices, i.e.,</p>
<p><span class="math display">
\sup_{x \in P} f(x) = \max_{i=1, \ldots, k} f(v_i).
</span></p>
<p>A stronger statement is: the maximum of a convex function over a closed bounded convex set is achieved at an extreme point, i.e., a point in the set that is not a convex combination of any other points in the set. (you do not have to prove it). <em>Hint:</em> Assume the statement is false, and use Jensen‚Äôs inequality.</p></li>
<li><p>Show, that the two definitions of <span class="math inline">\mu</span>-strongly convex functions are equivalent:</p>
<ol type="1">
<li><p><span class="math inline">f(x)</span> is <span class="math inline">\mu</span>-strongly convex <span class="math inline">\iff</span> for any <span class="math inline">x_1, x_2 \in S</span> and <span class="math inline">0 \le \lambda \le 1</span> for some <span class="math inline">\mu &gt; 0</span>:</p>
<p><span class="math display">
f(\lambda x_1 + (1 - \lambda)x_2) \le \lambda f(x_1) + (1 - \lambda)f(x_2) - \frac{\mu}{2} \lambda (1 - \lambda)\|x_1 - x_2\|^2
</span></p></li>
<li><p><span class="math inline">f(x)</span> is <span class="math inline">\mu</span>-strongly convex <span class="math inline">\iff</span> if there exists <span class="math inline">\mu&gt;0</span> such that the function <span class="math inline">f(x) - \dfrac{\mu}{2}\Vert x\Vert^2</span> is convex.</p></li>
</ol></li>
</ol>
</section>
<section id="conjugate-sets" class="level3">
<h3 class="anchored" data-anchor-id="conjugate-sets">Conjugate sets</h3>
<ol type="1">
<li><p>Let <span class="math inline">\mathbb{A}_n</span> be the set of all <span class="math inline">n</span> dimensional antisymmetric matrices (s.t. <span class="math inline">X^T = - X</span>). Show that <span class="math inline">\left( \mathbb{A}_n\right)^* = \mathbb{S}_n</span>.</p></li>
<li><p>Find the sets <span class="math inline">S^{*}, S^{**}, S^{***}</span>, if</p>
<p><span class="math display">
S = \{ x \in \mathbb{R}^2 \mid x_1 + x_2 \ge 0, \;\; -\dfrac12x_1 + x_2 \ge 0, \;\; 2x_1 + x_2 \ge -1 \;\; -2x_1 + x_2 \ge -3\}
</span></p></li>
<li><p>Prove, that <span class="math inline">B_p</span> and <span class="math inline">B_{p_*}</span> are inter-conjugate, i.e.&nbsp;<span class="math inline">(B_p)^* = B_{p_*}, (B_{p_*})^* = B_p</span>, where <span class="math inline">B_p</span> is the unit ball (w.r.t. <span class="math inline">p</span> - norm) and <span class="math inline">p, p_*</span> are conjugated, i.e.&nbsp;<span class="math inline">p^{-1} + p^{-1}_* = 1</span>. You can assume, that <span class="math inline">p_* = \infty</span> if <span class="math inline">p = 1</span> and vice versa.</p></li>
</ol>
<hr>
</section>
<section id="conjugate-functions" class="level3">
<h3 class="anchored" data-anchor-id="conjugate-functions">Conjugate functions</h3>
<ol type="1">
<li><p>Find <span class="math inline">f^*(y)</span>, if <span class="math inline">f(x) = \vert 2x \vert</span></p></li>
<li><p>Prove, that if <span class="math inline">f(x) = \inf\limits_{u+v = x} (g(u) + h(v))</span>, then <span class="math inline">f^*(y) = g^*(y) + h^*(y)</span>.</p></li>
<li><p>Find <span class="math inline">f^*(y)</span>, if <span class="math inline">f(x) = \log \left( \sum\limits_{i=1}^n e^{x_i} \right)</span></p></li>
<li><p>Prove, that if <span class="math inline">f(x) = g(Ax)</span>, then <span class="math inline">f^*(y) = g^*(A^{-\top}y)</span></p></li>
<li><p>Find <span class="math inline">f^*(Y)</span>, if <span class="math inline">f(X) = - \ln \det X, X \in \mathbb{S}^n_{++}</span></p></li>
<li><p>The scalar Huber function is defined as</p>
<p><span class="math display">
f_{\text{hub}}(x) =
\begin{cases}
\frac{1}{2} x^2 &amp; \text{if } |x| \leq 1 \\
|x| - \frac{1}{2} &amp; \text{if } |x| &gt; 1
\end{cases}
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./huber_function.svg" class="img-fluid figure-img"></p>
<figcaption>Scalar case</figcaption>
</figure>
</div>
<p>This convex function arises in various applications, notably in robust estimation. This problem explores the generalizations of the Huber function to <span class="math inline">\mathbb{R}^n</span>. A straightforward extension to <span class="math inline">\mathbb{R}^n</span> is expressed as <span class="math inline">f_{\text{hub}}(x_1) + \ldots + f_{\text{hub}}(x_n)</span>, yet this formulation is not circularly symmetric, that is, it‚Äôs not invariant under the transformation of <span class="math inline">x</span> by an orthogonal matrix. A circularly symmetric extension to <span class="math inline">\mathbb{R}^n</span> is given by</p>
<p><span class="math display">
f_{\text{cshub}}(x) = f_{\text{hub}}(\Vert x\Vert )=
\begin{cases}
\frac{1}{2} \Vert x\Vert_2 ^2 &amp; \text{if } \Vert x\Vert_2 \leq 1 \\
\Vert x\Vert_2 - \frac{1}{2} &amp; \text{if } \Vert x\Vert_2 &gt; 1
\end{cases}
</span></p>
<p>where the subscript denotes ‚Äúcircularly symmetric Huber function‚Äù. Show, that <span class="math inline">f_{\text{cshub}}</span> is convex. Find the conjugate function <span class="math inline">f^*(y)</span>.</p></li>
</ol>
<hr>
</section>
<section id="subgradient-and-subdifferential" class="level3">
<h3 class="anchored" data-anchor-id="subgradient-and-subdifferential">Subgradient and subdifferential</h3>
<ol type="1">
<li>Find <span class="math inline">\partial f(x)</span>, if <span class="math display">
f(x) = \text{Parametric ReLU}(x) = \begin{cases}
     x &amp; \text{if } x &gt; 0, \\
     ax &amp; \text{otherwise}.
\end{cases}
</span></li>
<li>Prove, that <span class="math inline">x_0</span> - is the minimum point of a function <span class="math inline">f(x)</span> if and only if <span class="math inline">0 \in \partial f(x_0)</span>.</li>
<li>Find <span class="math inline">\partial f(x)</span>, if <span class="math inline">f(x) = \Vert Ax - b\Vert _1</span>.</li>
<li>Find <span class="math inline">\partial f(x)</span>, if <span class="math inline">f(x) = e^{\Vert x\Vert}</span>.</li>
<li>Find <span class="math inline">\partial f(x)</span>, if <span class="math inline">f(x) = \frac12 \Vert Ax - b\Vert _2^2 + \lambda \Vert x\Vert_1, \quad \lambda &gt; 0</span>.</li>
<li>Let <span class="math inline">S \subseteq \mathbb{R}^n</span> be a convex set. We will call a <em>normal cone</em> of the set <span class="math inline">S</span> at a point <span class="math inline">x</span> the following set: <span class="math display">
N_S(x) = \left\{c \in \mathbb{R}^n : \langle c, y-x\rangle \leq 0 \quad \forall y \in S\right\}
</span>
<ol type="i">
<li><p>Draw a normal cone for a set at the points <span class="math inline">A, B, C, D, E, F</span> on the figure below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./normal_cone.svg" class="img-fluid figure-img"></p>
<figcaption>Draw a normal cone for the set <span class="math inline">S</span> in these points</figcaption>
</figure>
</div></li>
<li><p>Show, that <span class="math inline">N_S(x) = \{0\} \quad \forall x \in \mathbf{ri }(S)</span>.</p></li>
<li><p>Show, that the subdifferential <span class="math inline">\partial I_S(x) = N_S(x)</span> if <span class="math inline">I_S(x)</span> is the indicator function, i.e.&nbsp; <span class="math display">
I_S(x) = \begin{cases}0,\text{if } x \in S\\ \infty, \text{otherwise}\end{cases}
</span></p></li>
</ol></li>
</ol>
<hr>
</section>
<section id="kkt-and-duality" class="level3">
<h3 class="anchored" data-anchor-id="kkt-and-duality">KKT and duality</h3>
<p>In this section, you can consider either the arbitrary norm or the Euclidian norm if nothing else is specified.</p>
<ol type="1">
<li><p><strong>Toy example</strong> <span class="math display">
\begin{split}
&amp; x^2 + 1 \to \min\limits_{x \in \mathbb{R} }\\
\text{s.t. } &amp; (x-2)(x-4) \leq 0
\end{split}
</span></p>
<ol type="1">
<li>Give the feasible set, the optimal value, and the optimal solution.</li>
<li>Plot the objective <span class="math inline">x^2 +1</span> versus <span class="math inline">x</span>. On the same plot, show the feasible set, optimal point, and value, and plot the Lagrangian <span class="math inline">L(x,\mu)</span> versus <span class="math inline">x</span> for a few positive values of <span class="math inline">\mu</span>. Verify the lower bound property (<span class="math inline">p^* \geq \inf_x L(x, \mu)</span>for <span class="math inline">\mu \geq 0</span>). Derive and sketch the Lagrange dual function <span class="math inline">g</span>.</li>
<li>State the dual problem, and verify that it is a concave maximization problem. Find the dual optimal value and dual optimal solution <span class="math inline">\mu^*</span>. Does strong duality hold?</li>
<li>Let <span class="math inline">p^*(u)</span> denote the optimal value of the problem</li>
</ol>
<p><span class="math display">
\begin{split}
&amp; x^2 + 1 \to \min\limits_{x \in \mathbb{R} }\\
\text{s.t. } &amp; (x-2)(x-4) \leq u
\end{split}
</span></p>
<p>as a function of the parameter <span class="math inline">u</span>. Plot <span class="math inline">p^*(u)</span>. Verify that <span class="math inline">\dfrac{dp^*(0)}{du} = -\mu^*</span></p></li>
<li><p>Derive the dual problem for the Ridge regression problem with <span class="math inline">A \in \mathbb{R}^{m \times n}, b \in \mathbb{R}^m, \lambda &gt; 0</span>:</p>
<p><span class="math display">
\begin{split}
\dfrac{1}{2}\|y-b\|^2 + \dfrac{\lambda}{2}\|x\|^2 &amp;\to \min\limits_{x \in \mathbb{R}^n, y \in \mathbb{R}^m }\\
\text{s.t. } &amp; y = Ax
\end{split}
</span></p></li>
<li><p>Derive the dual problem for the support vector machine problem with <span class="math inline">A \in \mathbb{R}^{m \times n}, \mathbf{1} \in \mathbb{R}^m \in \mathbb{R}^m, \lambda &gt; 0</span>:</p>
<p><span class="math display">
\begin{split}
\langle \mathbf{1}, t\rangle + \dfrac{\lambda}{2}\|x\|^2 &amp;\to \min\limits_{x \in \mathbb{R}^n, t \in \mathbb{R}^m }\\
\text{s.t. } &amp; Ax \succeq \mathbf{1} - t \\
&amp; t \succeq 0
\end{split}
</span></p></li>
<li><p>Give an explicit solution to the following LP.</p>
<p><span class="math display">
\begin{split}
&amp; c^\top x \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; 1^\top x = 1, \\
&amp; x \succeq 0
\end{split}
</span></p>
<p>This problem can be considered the simplest portfolio optimization problem.</p></li>
<li><p>Show, that the following problem has a unique solution and find it:</p>
<p><span class="math display">
\begin{split}
&amp; \langle C^{-1}, X\rangle - \log \det X \to \min\limits_{x \in \mathbb{R}^{n \times n} }\\
\text{s.t. } &amp; \langle Xa, a\rangle \leq 1,
\end{split}
</span></p>
<p>where <span class="math inline">C \in \mathbb{S}^n_{++}, a \in \mathbb{R}^n \neq 0</span>. The answer should not involve inversion of the matrix <span class="math inline">C</span>.</p></li>
<li><p>Give an explicit solution to the following QP.</p>
<p><span class="math display">
\begin{split}
&amp; c^\top x \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; (x - x_c)^\top A (x - x_c) \leq 1,
\end{split}
</span></p>
<p>where <span class="math inline">A \in \mathbb{S}^n_{++}, c \neq 0, x_c \in \mathbb{R}^n</span>.</p></li>
<li><p>Consider the equality-constrained least-squares problem</p>
<p><span class="math display">
\begin{split}
&amp; \|Ax - b\|_2^2 \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; Cx = d,
\end{split}
</span></p>
<p>where <span class="math inline">A \in \mathbb{R}^{m \times n}</span> with <span class="math inline">\mathbf{rank }A = n</span>, and <span class="math inline">C \in \mathbb{R}^{k \times n}</span> with <span class="math inline">\mathbf{rank }C = k</span>. Give the KKT conditions, and derive expressions for the primal solution <span class="math inline">x^*</span> and the dual solution <span class="math inline">\lambda^*</span>.</p></li>
<li><p>Derive the KKT conditions for the problem</p>
<p><span class="math display">
\begin{split}
&amp; \mathbf{tr \;}X - \log\text{det }X \to \min\limits_{X \in \mathbb{S}^n_{++} }\\
\text{s.t. } &amp; Xs = y,
\end{split}
</span></p>
<p>where <span class="math inline">y \in \mathbb{R}^n</span> and <span class="math inline">s \in \mathbb{R}^n</span> are given with <span class="math inline">y^\top s = 1</span>. Verify that the optimal solution is given by</p>
<p><span class="math display">
X^* = I + yy^\top - \dfrac{1}{s^\top s}ss^\top
</span></p></li>
<li><p><strong>Supporting hyperplane interpretation of KKT conditions</strong>. Consider a <strong>convex</strong> problem with no equality constraints</p>
<p><span class="math display">
\begin{split}
&amp; f_0(x) \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; f_i(x) \leq 0, \quad i = [1,m]
\end{split}
</span></p>
<p>Assume, that <span class="math inline">\exists x^* \in \mathbb{R}^n, \mu^* \in \mathbb{R}^m</span> satisfy the KKT conditions</p>
<p><span class="math display">
\begin{split}
&amp; \nabla_x L (x^*, \mu^*) = \nabla f_0(x^*) + \sum\limits_{i=1}^m\mu_i^*\nabla f_i(x^*) = 0 \\
&amp; \mu^*_i \geq 0, \quad i = [1,m] \\
&amp; \mu^*_i f_i(x^*) = 0, \quad i = [1,m]\\
&amp; f_i(x^*) \leq 0, \quad i = [1,m]
\end{split}
</span></p>
<p>Show that</p>
<p><span class="math display">
\nabla f_0(x^*)^\top (x - x^*) \geq 0
</span></p>
<p>for all feasible <span class="math inline">x</span>. In other words, the KKT conditions imply the simple optimality criterion or <span class="math inline">\nabla f_0(x^*)</span> defines a supporting hyperplane to the feasible set at <span class="math inline">x^*</span>.</p></li>
<li><p><strong>Fenchel + Lagrange = ‚ô•.</strong> Express the dual problem of</p>
<p><span class="math display">
\begin{split}
&amp; c^\top x\to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; f(x) \leq 0
\end{split}
</span></p>
<p>with <span class="math inline">c \neq 0</span>, in terms of the conjugate function <span class="math inline">f^*</span>. Explain why the problem you give is convex. We do not assume <span class="math inline">f</span> is convex.</p></li>
<li><p><strong>A penalty method for equality constraints.</strong> We consider the problem of minimization</p>
<p><span class="math display">
\begin{split}
&amp; f_0(x) \to \min\limits_{x \in \mathbb{R}^{n} }\\
\text{s.t. } &amp; Ax = b,
\end{split}
</span></p>
<p>where $f_0(x): ^n $ is convex and differentiable, and <span class="math inline">A \in \mathbb{R}^{m \times n}</span> with <span class="math inline">\mathbf{rank }A = m</span>. In a quadratic penalty method, we form an auxiliary function</p>
<p><span class="math display">
\phi(x) = f_0(x) + \alpha \|Ax - b\|_2^2,
</span></p>
<p>where <span class="math inline">\alpha &gt; 0</span> is a parameter. This auxiliary function consists of the objective plus the penalty term <span class="math inline">\alpha \Vert Ax - b\Vert_2^2</span>. The idea is that a minimizer of the auxiliary function, <span class="math inline">\tilde{x}</span>, should be an approximate solution to the original problem. Intuition suggests that the larger the penalty weight <span class="math inline">\alpha</span>, the better the approximation <span class="math inline">\tilde{x}</span> to a solution of the original problem. Suppose <span class="math inline">\tilde{x}</span> is a minimizer of <span class="math inline">\phi(x)</span>. Show how to find, from <span class="math inline">\tilde{x}</span>, a dual feasible point for the original problem. Find the corresponding lower bound on the optimal value of the original problem.</p></li>
<li><p><strong>Analytic centering.</strong> Derive a dual problem for</p>
<p><span class="math display">
-\sum_{i=1}^m \log (b_i - a_i^\top x) \to \min\limits_{x \in \mathbb{R}^{n} }
</span></p>
<p>with domain <span class="math inline">\{x \mid a^\top_i x &lt; b_i , i = [1,m]\}</span>.</p>
<p>First introduce new variables <span class="math inline">y_i</span> and equality constraints <span class="math inline">y_i = b_i ‚àí a^\top_i x</span>. (The solution to this problem is called the analytic center of the linear inequalities <span class="math inline">a^\top_i x \leq b_i ,i = [1,m]</span>. Analytic centers have geometric applications, and play an important role in barrier methods.)</p></li>
</ol>
<hr>
</section>
<section id="linear-programming" class="level3">
<h3 class="anchored" data-anchor-id="linear-programming">Linear programming</h3>
<ol type="1">
<li><p><strong>üì±üéßüíª Covers manufacturing.</strong> Lyzard Corp is producing covers for the following products:</p>
<ul>
<li>üì± phones</li>
<li>üéß headphones</li>
<li>üíª laptops</li>
</ul>
<p>The company‚Äôs production facilities are such that if we devote the entire production to headphone covers, we can produce 5000 of them in one day. If we devote the entire production to phone covers or laptop covers, we can produce 4000 or 2000 of them in one day.</p>
<p>The production schedule is one week (6 working days), and the week‚Äôs production must be stored before distribution. Storing 1000 headphone covers (packaging included) takes up 30 cubic feet of space. Storing 1000 phone covers (packaging included) takes up 50 cubic feet of space, and storing 1000 laptop covers (packaging included) takes up 220 cubic feet of space. The total storage space available is 1500 cubic feet.</p>
<p>Due to commercial agreements with Lyzard Corp has to deliver at least 4500 headphone covers and 4000 laptop covers per week to strengthen the product‚Äôs diffusion.</p>
<p>The marketing department estimates that the weekly demand for headphones covers, phone, and laptop covers does not exceed 10000 14000, and 7000 units, therefore the company does not want to produce more than these amounts for headphones, phone, and laptop covers.</p>
<p>Finally, the net profit per headphone cover, phone cover, and laptop cover are $5, $7, and $12, respectively.</p>
<p>The aim is to determine a weekly production schedule that maximizes the total net profit.</p>
<ol type="1">
<li><p>Write a Linear Programming formulation for the problem. Use the following variables:</p>
<ul>
<li><span class="math inline">y_1</span> = number of headphones covers produced over the week,<br>
</li>
<li><span class="math inline">y_2</span> = number of phone covers produced over the week,<br>
</li>
<li><span class="math inline">y_3</span> = number of laptop covers produced over the week.</li>
</ul></li>
<li><p>Find the solution to the problem using <a href="http://www.pyomo.org">PyOMO</a></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install pyomo</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> sudo apt<span class="op">-</span>get install glpk<span class="op">-</span>utils <span class="op">--</span>quiet  <span class="co"># GLPK</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> sudo apt<span class="op">-</span>get install coinor<span class="op">-</span>cbc <span class="op">--</span>quiet  <span class="co"># CoinOR</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Perform the sensitivity analysis. Which constraint could be relaxed to increase the profit the most? Prove it numerically.</p></li>
</ol></li>
<li><p>Prove the optimality of the solution</p>
<p><span class="math display">
x = \left(\frac{7}{3} , 0, \frac{1}{3}\right)^T
</span></p>
<p>to the following linear programming problem:</p>
<p><span class="math display">
\begin{split}
&amp; 9x_1 + 3x_2 + 7x_3 \to \max\limits_{x \in \mathbb{R}^3 }\\
\text{s.t. } &amp; 2x_1 + x_2 + 3x_3 \leq 6 \\
&amp; 5x_1 + 4x_2 + x_3 \leq 12 \\
&amp; 3x_3 \leq 1,\\
&amp; x_1, x_2, x_3 \geq 0
\end{split}
</span></p>
<p>but you cannot use any numerical algorithm here.</p></li>
<li><p>Transform the following linear program into an equivalent linear program in the standard form <span class="math inline">\left(c^\top x \to \min\limits_{x\in \mathbb{R}^n} : Ax = b,x ‚â• 0\right)</span>:</p>
<p><span class="math display">
\begin{split}
&amp; x_1‚àíx_2 \to \min\limits_{x \in \mathbb{R}^2 }\\
\text{s.t. } &amp; 2x_1 + x_2 \geq 3 \\
&amp; 3x_1 ‚àí x_2 \leq 7 \\
&amp; x_1 \geq 0
\end{split}
</span></p></li>
<li><p>Consider:</p>
<p><span class="math display">
\begin{split}
&amp; 4x_1 + 5x_2 + 2x_3 \to \max\limits_{x \in \mathbb{R}^3 }\\
\text{s.t. } &amp; 2x_1 - x_2 + 2x_3 \leq 9 \\
&amp; 3x_1 + 5x_2 + 4x_3 \leq 8 \\
&amp; x_1 + x_2 + 2x_3 \leq 2 \\
&amp; x_1, x_2, x_3 \geq 0,
\end{split}
</span></p>
<ol type="1">
<li>Find an optimal solution to the Linear Programming problem using the simplex method.</li>
<li>Write the dual linear program. Find an optimal dual solution. Do we have strong duality here?</li>
</ol></li>
</ol>
<hr>
</section>
<section id="sequence-convergence" class="level3">
<h3 class="anchored" data-anchor-id="sequence-convergence">Sequence convergence</h3>
<ol type="1">
<li><p>Determine the convergence or divergence of a given sequences</p>
<ul>
<li><span class="math inline">r_{k} = \frac{1}{\sqrt{k}}</span>.</li>
<li><span class="math inline">r_{k} = 0.606^k</span>.</li>
<li><span class="math inline">r_{k} = 0.606^{2^k}</span>.</li>
</ul></li>
<li><p>Determine the convergence or divergence of a given sequence <span class="math inline">r_k =\begin{cases} \frac{1}{k}, &amp; \text{if } k\text{ is even} \\ e^{-k}, &amp; \text{if } k\text{ is odd} \end{cases}</span>.</p></li>
<li><p>Determine the following sequence <span class="math inline">\{r_k\}</span> by convergence rate (linear, sublinear, superlinear). In the case of superlinear convergence, additionally, find out whether there is quadratic convergence.</p>
<p><span class="math display">
r_k = \dfrac{1}{k!}
</span></p></li>
<li><p>Determine the following sequence <span class="math inline">\{r_k\}</span> by convergence rate (linear, sublinear, superlinear). In the case of superlinear convergence, additionally find out whether there is quadratic convergence.</p>
<p><span class="math display">
r_k = \dfrac{1}{k^k}
</span></p></li>
<li><p>Let <span class="math inline">\{r_k\}</span> be a sequence of non-negative numbers given as <span class="math inline">r_{k+1} = Mr_k^2</span>, where <span class="math inline">M &gt; 0</span>, <span class="math inline">r_0 \geq 0</span>. Establish a necessary and sufficient condition on <span class="math inline">M</span> and <span class="math inline">r_0</span> under which the sequence <span class="math inline">r_k</span> will converge to zero. What is the rate of convergence?</p></li>
</ol>
</section>
<section id="line-search" class="level3">
<h3 class="anchored" data-anchor-id="line-search">Line search</h3>
<ol type="1">
<li><p>Show that for a one-dimensional quadratic function decreasing at zero, its minimum satisfies Armijo‚Äôs condition for any <span class="math inline">c_1: 0 \leq c_1 \leq \dfrac12</span>:</p>
<p><span class="math display">
f(x_k - \alpha \nabla f (x_k)) \leq f(x_k) - c_1 \cdot \alpha\|\nabla f(x_k)\|_2^2
</span></p></li>
<li><p><strong>Implementing and Testing Line Search Conditions in Gradient Descent</strong></p>
<p><span class="math display">
x_{k+1} = x_k - \alpha \nabla f(x_k)
</span></p>
<p>In this assignment, you will modify an existing Python code for gradient descent to include various line search conditions. You will test these modifications on two functions: a quadratic function and the Rosenbrock function. The main objectives are to understand how different line search strategies influence the convergence of the gradient descent algorithm and to compare their efficiencies based on the number of function evaluations.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize_scalar</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">214</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the quadratic function and its gradient</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quadratic_function(x, A, b):</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> np.dot(x.T, np.dot(A, x)) <span class="op">-</span> np.dot(b.T, x)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad_quadratic(x, A, b):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(A, x) <span class="op">-</span> b</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a 2D quadratic problem with a specified condition number</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_quadratic_problem(cond_number):</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Random symmetric matrix</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> np.random.randn(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> np.dot(M, M.T)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ensure the matrix has the desired condition number</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    U, s, V <span class="op">=</span> np.linalg.svd(M)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> np.linspace(cond_number, <span class="dv">1</span>, <span class="bu">len</span>(s))  <span class="co"># Spread the singular values</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.dot(U, np.dot(np.diag(s), V))</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Random b</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> np.random.randn(<span class="dv">2</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A, b</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient descent function</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(start_point, A, b, stepsize_func, max_iter<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> start_point.copy()</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    trajectory <span class="op">=</span> [x.copy()]</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> grad_quadratic(x, A, b)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        step_size <span class="op">=</span> stepsize_func(x, grad)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">-=</span> step_size <span class="op">*</span> grad</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        trajectory.append(x.copy())</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(trajectory)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Backtracking line search strategy using scipy</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backtracking_line_search(x, grad, A, b, alpha<span class="op">=</span><span class="fl">0.3</span>, beta<span class="op">=</span><span class="fl">0.8</span>):</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> objective(t):</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> quadratic_function(x <span class="op">-</span> t <span class="op">*</span> grad, A, b)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> minimize_scalar(objective, method<span class="op">=</span><span class="st">'golden'</span>)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res.x</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ill-posed problem</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>cond_number <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>A, b <span class="op">=</span> generate_quadratic_problem(cond_number)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Starting point</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>start_point <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="fl">1.8</span>])</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform gradient descent with both strategies</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>trajectory_fixed <span class="op">=</span> gradient_descent(start_point, A, b, <span class="kw">lambda</span> x, g: <span class="fl">5e-2</span>)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>trajectory_backtracking <span class="op">=</span> gradient_descent(start_point, A, b, <span class="kw">lambda</span> x, g: backtracking_line_search(x, g, A, b))</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the trajectories on a contour plot</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>x1, x2 <span class="op">=</span> np.meshgrid(np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">400</span>), np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">400</span>))</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.array([quadratic_function(np.array([x, y]), A, b) <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(x1.flatten(), x2.flatten())]).reshape(x1.shape)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>plt.contour(x1, x2, Z, levels<span class="op">=</span><span class="dv">50</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>plt.plot(trajectory_fixed[:, <span class="dv">0</span>], trajectory_fixed[:, <span class="dv">1</span>], <span class="st">'o-'</span>, label<span class="op">=</span><span class="st">'Fixed Step Size'</span>)</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>plt.plot(trajectory_backtracking[:, <span class="dv">0</span>], trajectory_backtracking[:, <span class="dv">1</span>], <span class="st">'o-'</span>, label<span class="op">=</span><span class="st">'Backtracking Line Search'</span>)</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a><span class="co"># Add markers for start and optimal points</span></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>plt.plot(start_point[<span class="dv">0</span>], start_point[<span class="dv">1</span>], <span class="st">'ro'</span>, label<span class="op">=</span><span class="st">'Start Point'</span>)</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>optimal_point <span class="op">=</span> np.linalg.solve(A, b)</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>plt.plot(optimal_point[<span class="dv">0</span>], optimal_point[<span class="dv">1</span>], <span class="st">'y*'</span>, markersize<span class="op">=</span><span class="dv">15</span>, label<span class="op">=</span><span class="st">'Optimal Point'</span>)</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Gradient Descent Trajectories on Quadratic Function'</span>)</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x1'</span>)</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'x2'</span>)</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">"linesearch.svg"</span>)</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="linesearch.svg" class="img-fluid figure-img"></p>
<figcaption>The code above plots this</figcaption>
</figure>
</div>
<p>Start by reviewing the provided Python code. This code implements gradient descent with a fixed step size and a backtracking line search on a quadratic function. Familiarize yourself with how the gradient descent function and the step size strategies are implemented.</p>
<ol type="1">
<li><p>Modify the gradient descent function to include the following line search conditions:</p>
<ol type="a">
<li>Sufficient Decrease Condition</li>
<li>Curvature Condition</li>
<li>Goldstein Condition</li>
<li>Wolfe Condition</li>
<li>Dichotomy</li>
</ol>
<p>Test your modified gradient descent algorithm with the implemented line search conditions on the provided quadratic function. Plot the trajectories over iterations for each condition. Choose and specify hyperparameters for inexact line search condition. Choose and specify the termination criterion. Start from the point <span class="math inline">x_0 = (-1, 2)^T</span>.</p></li>
<li><p>Compare these 7 methods from the budget perspective. Plot the graph of function value from the number of function evaluations for each method on the same graph.</p></li>
<li><p>Plot trajectory for another function with the same set of methods</p>
<p><span class="math display">
f(x_1, x_2) =  10(x_2 ‚àí x_1^2)^2 + (x_1 ‚àí 1)^2
</span></p>
<p>with <span class="math inline">x_0 = (-1, 2)^T</span>. You might need to adjust hyperparameters.</p></li>
<li><p>Plot the same function value from the number of function calls for this experiment.</p></li>
</ol></li>
</ol>
</section>
<section id="zero-order-methods" class="level3">
<h3 class="anchored" data-anchor-id="zero-order-methods">Zero-order methods</h3>
<ol type="1">
<li><p>Solve approximately the Travelling Salesman Problem with any zero-order optimization method.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="salesman_problem.svg" class="img-fluid figure-img"></p>
<figcaption>Illustration of TSP</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial <span class="im">import</span> distance_matrix</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_random_symmetric_tsp(num_cities, seed<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    np.random.seed(seed)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> np.random.rand(num_cities, <span class="dv">2</span>)  <span class="co"># Generate random coordinates</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    dist_matrix <span class="op">=</span> distance_matrix(points, points)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    dist_matrix <span class="op">=</span> (dist_matrix <span class="op">+</span> dist_matrix.T) <span class="op">/</span> <span class="dv">2</span>  <span class="co"># Ensure symmetry</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.array(dist_matrix)  <span class="co"># Convert to JAX array for further processing</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>num_cities <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>dist_matrix <span class="op">=</span> generate_random_symmetric_tsp(num_cities)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dist_matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>You can use the <a href="https://fmin.xyz/docs/applications/salesman_problem.html">genetic</a> algorithm with any significant modification of mutation(5/10)</li>
<li>You can use another algorithm (10/10)</li>
</ul></li>
<li><p>In this assignment, we aim to explore and compare the efficacy of traditional and zero-order optimization methods in training a simple neural network. The conventional approach, Stochastic Gradient Descent (SGD), has been widely used due to its efficiency in handling large datasets and its capability to work in a huge-scale setting. This method relies on gradients of the loss function with respect to the network‚Äôs parameters. In contrast, zero-order optimization methods, also known as derivative-free methods, optimize without explicit gradient information. These methods are particularly useful for non-differentiable, noisy, or highly complex loss landscapes. The assignment‚Äôs objective is to explore such algorithms as Genetic Algorithms, Simulated Annealing, Gradient-free methods, or the Nelder-Mead method for real-world problems.</p>
<p>Note, that a variety of feed-forward neural networks could be represented as a series of linear transformations, followed by some nonlinear function (say, <span class="math inline">\text{ReLU }(x)</span>):</p>
<p><span class="math display">
\mathcal{NN}(x) = f_L \circ w_L \circ \ldots \circ f_1 \circ w_1 \circ x,
</span></p>
<p>where <span class="math inline">L</span> is the number of layers, <span class="math inline">f_i</span> - non-linear activation function, <span class="math inline">w_i = W_i x + b_i</span> - linear layer. We can denote the training data by <span class="math inline">X</span> and the labels by <span class="math inline">y</span>. The overall optimization problem here is to train the neural network to approximate the mapping <span class="math inline">X \to y</span>, i.e.&nbsp;<span class="math inline">\mathcal{NN}(X)</span> should be as close to <span class="math inline">y</span> as possible for all data points. We can ensure this by minimizing the loss function, which depends on the neural network parameters:</p>
<p><span class="math display">
\mathcal{L}(\mathcal{NN}(X, W, b), y) \to \min_{W, b}
</span></p>
<p>Typically, we use a cross-entropy loss function for the classification task. Do not worry if you are not familiar with neural networks or machine learning. Try to focus on the optimization algorithm here. You are provided with an example of this approach in the <a href="https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/NN_ES.ipynb">Colab notebook</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ES_vs_SGD-8k.svg" class="img-fluid figure-img"></p>
<figcaption>Comparison of SGD vs Evolutionary strategy for neural network without hidden layer.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ES_vs_SGD-42k.svg" class="img-fluid figure-img"></p>
<figcaption>Comparison of SGD vs Evolutionary strategy for neural network with several hidden layers.</figcaption>
</figure>
</div>
<p>The assignment requires you to implement a chosen zero-order optimization algorithm and compare its performance against SGD in training a predefined simple neural network (you can vary the structure of the network as you want for this problem). The comparison should focus on aspects such as convergence speed, final accuracy, and computational efficiency. Students should provide a name of the chosen zero-order algorithm and implement it in Python. You can use any method you want except the Evolutionary strategy, which is already in the example above.</p></li>
</ol>
</section>
<section id="gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent">Gradient Descent</h3>
<ol type="1">
<li><p><strong>Convergence of Gradient Descent in non-convex smooth case</strong> (10 points)</p>
<p>We will assume nothing about the convexity of <span class="math inline">f</span>. We will show that gradient descent reaches an <span class="math inline">\varepsilon</span>-substationary point <span class="math inline">x</span>, such that <span class="math inline">\|\nabla f(x)\|_2 \leq \varepsilon</span>, in <span class="math inline">O(1/\varepsilon^2)</span> iterations. Important note: you may use here Lipschitz parabolic upper bound:</p>
<p><span id="eq-quad_ub"><span class="math display">
f(y) \leq f(x) + \nabla f(x)^T (y-x) + \frac{L}{2} \|y-x\|_2^2, \;\;\;
\text{for all $x,y$}.  
  \tag{1}</span></span></p>
<ul>
<li><p>Plug in <span class="math inline">y = x^{k+1} = x^{k} - \alpha \nabla f(x^k), x = x^k</span> to (<a href="#eq-quad_ub" class="quarto-xref">Equation&nbsp;1</a>) to show that</p>
<p><span class="math display">
  f(x^{k+1}) \leq f(x^k) - \Big (1-\frac{L\alpha}{2} \Big) \alpha \|\nabla f(x^k)\|_2^2.
  </span></p></li>
<li><p>Use <span class="math inline">\alpha \leq 1/L</span>, and rearrange the previous result, to get</p>
<p><span class="math display">
  \|\nabla f(x^k)\|_2^2 \leq \frac{2}{\alpha} \left( f(x^k) - f(x^{k+1}) \right).
  </span></p></li>
<li><p>Sum the previous result over all iterations from <span class="math inline">1,\ldots,k+1</span> to establish</p>
<p><span class="math display">
  \sum_{i=0}^k \|\nabla f(x^{i})\|_2^2 \leq
  \frac{2}{\alpha} ( f(x^{0}) - f^*).
  </span></p></li>
<li><p>Lower bound the sum in the previous result to get</p>
<p><span class="math display">
  \min_{i=0,\ldots,k} \|\nabla f(x^{i}) \|_2
  \leq \sqrt{\frac{2}{\alpha(k+1)} (f(x^{0}) - f^*)},
  </span> which establishes the desired <span class="math inline">O(1/\varepsilon^2)</span> rate for achieving <span class="math inline">\varepsilon</span>-substationarity.</p></li>
</ul></li>
<li><p><strong>How gradient descent convergence depends on the condition number and dimensionality.</strong> (20 points)</p>
<p>Investigate how the number of iterations required for gradient descent to converge depends on the following two parameters: the condition number <span class="math inline">\kappa \geq 1</span> of the function being optimized, and the dimensionality <span class="math inline">n</span> of the space of variables being optimized.</p>
<p>To do this, for given parameters <span class="math inline">n</span> and <span class="math inline">\kappa</span>, randomly generate a quadratic problem of size <span class="math inline">n</span> with condition number <span class="math inline">\kappa</span> and run gradient descent on it with some fixed required precision. Measure the number of iterations <span class="math inline">T(n, \kappa)</span> that the method required for convergence (successful termination based on the stopping criterion).</p>
<p>Recommendation: The simplest way to generate a random quadratic problem of size <span class="math inline">n</span> with a given condition number <span class="math inline">\kappa</span> is as follows. It is convenient to take a diagonal matrix <span class="math inline">A \in S_{n}^{++}</span> as simply the diagonal matrix <span class="math inline">A = \text{Diag}(a)</span>, whose diagonal elements are randomly generated within <span class="math inline">[1, \kappa]</span>, and where <span class="math inline">\min(a) = 1</span>, <span class="math inline">\max(a) = \kappa</span>. As the vector <span class="math inline">b \in \mathbb{R}^n</span>, you can take a vector with random elements. Diagonal matrices are convenient to consider since they can be efficiently processed with even for large values of <span class="math inline">n</span>.</p>
<p>Fix a certain value of the dimensionality <span class="math inline">n</span>. Iterate over different condition numbers <span class="math inline">\kappa</span> on a grid and plot the dependence of <span class="math inline">T(n,\kappa)</span> against <span class="math inline">\kappa</span>. Since the quadratic problem is generated randomly each time, repeat this experiment several times. As a result, for a fixed value of <span class="math inline">n</span>, you should obtain a whole family of curves showing the dependence of <span class="math inline">T(n, \kappa)</span> on <span class="math inline">\kappa</span>. Draw all these curves in the same color for clarity (for example, red).</p>
<p>Now increase the value of <span class="math inline">n</span> and repeat the experiment. You should obtain a new family of curves <span class="math inline">T(n',\kappa)</span> against <span class="math inline">\kappa</span>. Draw all these curves in the same color but different from the previous one (for example, blue).</p>
<p>Repeat this procedure several times for other values of <span class="math inline">n</span>. Eventually, you should have several different families of curves - some red (corresponding to one value of <span class="math inline">n</span>), some blue (corresponding to another value of <span class="math inline">n</span>), some green, etc.</p>
<p>Note that it makes sense to iterate over the values of the dimensionality <span class="math inline">n</span> on a logarithmic grid (for example, <span class="math inline">n = 10, n = 100, n = 1000</span>, etc.). Use the following stopping criterion: <span class="math inline">\|\nabla f(x_k)\|_2^2 \leq \varepsilon \|\nabla f(x_0)\|_2^2</span> with <span class="math inline">\varepsilon = 10^{-5}</span>. Select the starting point <span class="math inline">x_0 = (1, \ldots, 1)^T</span></p>
<p>What conclusions can be drawn from the resulting picture?</p></li>
</ol>
</section>
<section id="subgradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="subgradient-descent">Subgradient Descent</h3>
<ol type="1">
<li><p><strong>Subgradient descent convergence with several stepsize strategies.</strong> (10 points)</p>
<p>In this problem you will have to prove the convergence of subgradient descent (<span class="math inline">x^{k+1} = x^k - \alpha_k g_k</span>) for several stepsize strategies. First prove, that</p>
<p><span class="math display">
\|x^{k+1} - x^*\|_2^2 \leq \|x^{k} - x^*\|_2^2 - 2\alpha_k \left(f(x^k) - f^* \right) + \alpha^2_k \|g_k\|_2^2
</span></p>
<p>Then, using <span class="math inline">\|g\|_2 \leq G, \|x^0 - x^*\| \leq R</span> prove, that</p>
<p><span class="math display">
\|x^{k+1} - x^*\|_2^2 \leq R^2 - 2\sum\limits_{i=1}^k\alpha_i \left(f(x^i) - f^* \right) + G^2\sum\limits_{i=1}^k\alpha^2_i
</span></p>
<p>Then, using <span class="math inline">f_k^{\text{best}} = \min\limits_{i=1,\ldots,k} f(x^i)</span> prove, that</p>
<p><span class="math display">
f_k^{\text{best}} - f^* \leq \frac{R^2 + G^2\sum\limits_{i=1}^k\alpha^2_i}{2\sum\limits_{i=1}^k\alpha_i}
</span></p>
<p>After that, finalize the bound for the following stepsize choosing strategies</p>
<ul>
<li><p>constant step size <span class="math inline">\alpha_k = \alpha</span></p></li>
<li><p>constant step length <span class="math inline">\alpha_k = \frac{\gamma}{\|g_k\|_2}</span> (so <span class="math inline">\|x^{k+1} - x^k\|_2 = \gamma</span>)</p></li>
<li><p>Inverse square root <span class="math inline">\frac{R}{G\sqrt{k}}</span></p></li>
<li><p>Inverse <span class="math inline">\frac1k</span></p></li>
<li><p>Polyak‚Äôs step size:</p>
<p><span class="math display">
  \alpha_k = \frac{f(x^k) - f^*}{\|g_k\|_2^2}
  </span></p></li>
</ul></li>
<li><p><strong>Subgradient methods for Lasso.</strong> (10 points)</p>
<p>Consider the optimization problem</p>
<p><span class="math display">
\min_{x \in \mathbb{R}^n} f(x) := \frac12 \|Ax - b\|^2 + \lambda \|x\|_1,
</span></p>
<p>with variables <span class="math inline">x \in \mathbb{R}^n</span> and problem data <span class="math inline">A \in \mathbb{R}^{m \times n}</span>, <span class="math inline">b \in \mathbb{R}^m</span> and <span class="math inline">\lambda &gt; 0</span>. This model is known as Lasso, or Least Squares with <span class="math inline">l_1</span> regularization, which encourages sparsity in the solution via the non-smooth penalty <span class="math inline">\|x\|_1 := \sum_{j=1}^n |x_j|</span>. In this problem, we will explore various subgradient methods for fitting this model.</p>
<ul>
<li><p>Derive the subdifferential <span class="math inline">\partial f(x)</span> of the objective.</p></li>
<li><p>Find the update rule of the subgradient method and state the computational complexity of applying one update using big O notation in terms of the dimensions.</p></li>
<li><p>Let <span class="math inline">n = 1000</span>, <span class="math inline">m = 200</span> and <span class="math inline">\lambda = 0.01</span>. Generate a random matrix <span class="math inline">A \in \mathbb{R}^{m \times n}</span> with independent Gaussian entries with mean 0 and variance <span class="math inline">1/m</span>, and a fixed vector <span class="math inline">x^* = {\underbrace{[1, \ldots, 1}_{\text{k times}}, \underbrace{0, \ldots, 0]}_{\text{n-k times}}}^T \in \mathbb{R}^n</span>. Let <span class="math inline">k = 5</span> and then set <span class="math inline">b = Ax^*</span>. Implement the subgradient method to minimize <span class="math inline">f(x)</span>, initialized at the all-zeros vector. Try different step size rules, including:</p>
<ul>
<li><p>constant step size <span class="math inline">\alpha_k = \alpha</span></p></li>
<li><p>constant step length <span class="math inline">\alpha_k = \frac{\gamma}{\|g_k\|_2}</span> (so <span class="math inline">\|x^{k+1} - x^k\|_2 = \gamma</span>)</p></li>
<li><p>Inverse square root <span class="math inline">\frac{1}{\sqrt{k}}</span></p></li>
<li><p>Inverse <span class="math inline">\frac1k</span></p></li>
<li><p>Polyak‚Äôs step length with estimated objective value:</p>
<p><span class="math display">
  \alpha_k = \frac{f(x_k) - f_k^{\text{best}} + \gamma_k}{\|g_k\|_2^2}, \quad \text{ with} \sum_{k=1}^\infty \gamma_k = \infty, \quad \sum_{k=1}^\infty \gamma_k^2 &lt; \infty
  </span></p>
<p>For example, one can use <span class="math inline">\gamma_k = \frac{10}{10 + k}</span>. Here <span class="math inline">f_k^{\text{best}} - \gamma_k</span> serves as estimate of <span class="math inline">f^*</span>. It is better to take <span class="math inline">\gamma_k</span> in the same scale as the objective value. One can show, that <span class="math inline">f_k^{\text{best}} \to f^*</span>.</p></li>
</ul>
<p>Plot objective value versus iteration curves of different step size rules on the same figure.</p></li>
<li><p>Repeat previous part using a heavy ball term, <span class="math inline">\beta_k(x^k - x^{k-1})</span>, added to the subgradient. Try different step size rules as in the previous part and tune the heavy ball parameter <span class="math inline">\beta_k = \beta</span> for faster convergence.</p></li>
</ul></li>
<li><p><strong>Finding a point in the intersection of convex sets.</strong> (20 points)</p>
<p>Let <span class="math inline">A \in \mathbb{R}^{n \times n}</span> be a positive definite matrix and let <span class="math inline">\Sigma</span> be an <span class="math inline">n \times n</span> diagonal matrix with diagonal entries <span class="math inline">\sigma_1,...,\sigma_n &gt; 0</span>, and <span class="math inline">y</span> a given vector in <span class="math inline">\mathbb{R}^n</span>. Consider the compact convex sets <span class="math inline">U = \{x \in \mathbb{R}^n \mid \|A^{1/2}(x-y)\|_2 \leq 1\}</span> and <span class="math inline">V = \{x \in \mathbb{R}^n \mid \|\Sigma x\|_\infty \leq 1\}</span>.</p>
<ul>
<li><p>Minimize maximum distance from the current point to the convex sets.</p>
<p><span class="math display">
  \min_{x\in\mathbb{R}^n} f(x) =  \min_{x\in\mathbb{R}^n} \max\{\mathbf{dist}(x, U), \mathbf{dist}(x, V)\}
  </span></p>
<p>propose an algorithm to find a point <span class="math inline">x \in U \cap V</span>. You can assume that <span class="math inline">U \cap V</span> is not empty. Your algorithm must be provably converging (although you do not need to prove it and you can simply refer to the lecture slides).</p></li>
<li><p>Implement your algorithm with the following data: <span class="math inline">n = 2</span>, <span class="math inline">y = (3, 2)</span>, <span class="math inline">\sigma_1 = 0.5</span>, <span class="math inline">\sigma_2 = 1</span>,</p>
<p><span class="math display">
  A = \begin{bmatrix}
  1 &amp; 0 \\
  -1 &amp; 1
  \end{bmatrix},
  </span></p>
<p>and <span class="math inline">x = (2, 1)</span>. Plot the objective value of your optimization problem versus the number of iterations.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="convex_intersection.png" class="img-fluid figure-img"></p>
<figcaption>Illustration of the problem</figcaption>
</figure>
</div></li>
</ol>
</section>
<section id="accelerated-methods" class="level3">
<h3 class="anchored" data-anchor-id="accelerated-methods">Accelerated methods</h3>
<ol type="1">
<li><p><strong>Local Convergence of Heavy Ball Method.</strong> (10 points)</p>
<p>We will work with the heavy ball method in this problem</p>
<p><span class="math display">
\tag{HB}
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1})
</span></p>
<p>It is known, that for the quadratics the best choice of hyperparameters is <span class="math inline">\alpha^* = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}, \beta^* = \dfrac{(\sqrt{L} - \sqrt{\mu})^2}{(\sqrt{L} + \sqrt{\mu})^2}</span>, which ensures accelerated linear convergence for a strongly convex quadratic function.</p>
<p>Consider the following continuously differentiable, strongly convex with parameter <span class="math inline">\mu</span>, and smooth function with parameter <span class="math inline">L</span>:</p>
<p><span class="math display">
f(x) =
\begin{cases}
\frac{25}{2}x^2, &amp; \text{if } x &lt; 1 \\
\frac12x^2 + 24x - 12, &amp; \text{if } 1 \leq x &lt; 2 \\
\frac{25}{2}x^2 - 24x + 36, &amp; \text{if } x \geq 2
\end{cases}
\quad
\nabla f(x) =
\begin{cases}
25x, &amp; \text{if } x &lt; 1 \\
x + 24, &amp; \text{if } 1 \leq x &lt; 2 \\
25x - 24, &amp; \text{if } x \geq 2
\end{cases}
</span></p>
<ol type="1">
<li><p>How to prove, that the given function is convex? Strongly convex? Smooth?</p></li>
<li><p>Find the constants <span class="math inline">\mu</span> and <span class="math inline">L</span> for a given function.</p></li>
<li><p>Plot the function value for <span class="math inline">x \in [-4, 4]</span>.</p></li>
<li><p>Run the Heavy Ball method for the function with optimal hyperparameters <span class="math inline">\alpha^* = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}, \beta^* = \dfrac{(\sqrt{L} - \sqrt{\mu})^2}{(\sqrt{L} + \sqrt{\mu})^2}</span> for quadratic function, starting from <span class="math inline">x_0 = 3.5</span>. If you have done everything above correctly, you should receive something like</p>
<div class="quarto-video"><video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="heavy_ball_conv.mp4"></video></div>
<p>You can use the following code for plotting:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.animation <span class="im">as</span> animation</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient of the function</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad_f(x):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Heavy Ball method implementation</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> heavy_ball_method(alpha, beta, x0, num_iterations):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.zeros(num_iterations <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    x_prev <span class="op">=</span> x0</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    x_curr <span class="op">=</span> x0  <span class="co"># Initialize x[1] same as x[0] to start the algorithm</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        x[i] <span class="op">=</span> x_curr</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        x_new <span class="op">=</span> x_curr <span class="op">-</span> alpha <span class="op">*</span> grad_f(x_curr) <span class="op">+</span> beta <span class="op">*</span> (x_curr <span class="op">-</span> x_prev)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        x_prev <span class="op">=</span> x_curr</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        x_curr <span class="op">=</span> x_new</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    x[num_iterations] <span class="op">=</span> x_curr</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> ...</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> ...</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>alpha_star <span class="op">=</span> ...</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>beta_star <span class="op">=</span> ...</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> ...</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>num_iterations <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the trajectory of the method</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>trajectory <span class="op">=</span> heavy_ball_method(alpha_star, beta_star, x0, num_iterations)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup the figure and axes for the animation</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="fl">3.5</span>))</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">"Heavy ball method with optimal hyperparameters Œ±* Œ≤*"</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Function for updating the animation</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update(i):</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    ax1.clear()</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    ax2.clear()</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot f(x) and trajectory</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>    x_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">100</span>)</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    f_vals <span class="op">=</span> np.piecewise(x_vals, [x_vals <span class="op">&lt;</span> <span class="dv">1</span>, (x_vals <span class="op">&gt;=</span> <span class="dv">1</span>) <span class="op">&amp;</span> (x_vals <span class="op">&lt;</span> <span class="dv">2</span>), x_vals <span class="op">&gt;=</span> <span class="dv">2</span>],</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>                        [<span class="kw">lambda</span> x: <span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span>, <span class="kw">lambda</span> x: <span class="fl">.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">-</span> <span class="dv">12</span>, <span class="kw">lambda</span> x: <span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">+</span> <span class="dv">36</span>])</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>    ax1.plot(x_vals, f_vals, <span class="st">'b-'</span>)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>    ax1.plot(trajectory[:i], [<span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="cf">if</span> x <span class="op">&lt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="fl">.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">-</span> <span class="dv">12</span> <span class="cf">if</span> x <span class="op">&lt;</span> <span class="dv">2</span> <span class="cf">else</span> <span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">+</span> <span class="dv">36</span> <span class="cf">for</span> x <span class="kw">in</span> trajectory[:i]], <span class="st">'ro-'</span>)</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add vertical dashed lines at x=1 and x=2 on the left subplot</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>    ax1.axvline(x<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'navy'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    ax1.axvline(x<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'navy'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot function value from iteration</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>    f_trajectory <span class="op">=</span> [<span class="va">None</span> <span class="cf">for</span> x <span class="kw">in</span> trajectory]</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    f_trajectory[:i] <span class="op">=</span> [<span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="cf">if</span> x <span class="op">&lt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="fl">.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">-</span> <span class="dv">12</span> <span class="cf">if</span> x <span class="op">&lt;</span> <span class="dv">2</span> <span class="cf">else</span> <span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">+</span> <span class="dv">36</span> <span class="cf">for</span> x <span class="kw">in</span> trajectory[:i]]</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>    ax2.plot(<span class="bu">range</span>(<span class="bu">len</span>(trajectory)), f_trajectory, <span class="st">'ro-'</span>)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlim(<span class="dv">0</span>, <span class="bu">len</span>(trajectory))</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylim(<span class="bu">min</span>(f_vals), <span class="bu">max</span>(f_vals))</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add horizontal dashed lines at f(1) and f(2) on the right subplot</span></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>    f_1 <span class="op">=</span> <span class="fl">12.5</span> <span class="op">*</span> <span class="fl">1.0</span><span class="op">**</span><span class="dv">2</span></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>    f_2 <span class="op">=</span> <span class="fl">.5</span> <span class="op">*</span> <span class="fl">2.</span><span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">24</span> <span class="op">*</span> <span class="fl">2.</span> <span class="op">-</span> <span class="dv">12</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    ax2.axhline(y<span class="op">=</span>f_1, color<span class="op">=</span><span class="st">'navy'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>    ax2.axhline(y<span class="op">=</span>f_2, color<span class="op">=</span><span class="st">'navy'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ax1.set_title("Function f(x) and Trajectory")</span></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>    ax1.set_xlabel(<span class="st">"x"</span>)</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylabel(<span class="st">"f(x)"</span>)</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>    ax1.grid(linestyle<span class="op">=</span><span class="st">":"</span>)</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ax2.set_title("Function Value from Iteration")</span></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlabel(<span class="st">"Iteration"</span>)</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylabel(<span class="st">"f(x)"</span>)</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>    ax2.grid(linestyle<span class="op">=</span><span class="st">":"</span>)</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the animation</span></span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>ani <span class="op">=</span> animation.FuncAnimation(fig, update, frames<span class="op">=</span>num_iterations, repeat<span class="op">=</span><span class="va">False</span>, interval<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>HTML(ani.to_jshtml())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Change the starting point to <span class="math inline">x_0 = 3.4</span>. What do you see? How could you name such a behavior of the method?</p></li>
<li><p>Change the hyperparameter <span class="math inline">\alpha^{\text{Global}} = \frac2L, \beta^{\text{Global}} = \frac{\mu}{L}</span> and run the method again from <span class="math inline">x_0 = 3.4</span>. Check whether you have accelerated convergence here.</p></li>
</ol>
<p>Context: this counterexample was provided in the <a href="https://arxiv.org/pdf/1408.3595.pdf">paper</a>, while the global convergence of the heavy ball method for general smooth strongly convex function was introduced in another <a href="https://arxiv.org/pdf/1412.7457.pdf">paper</a>. Recently, it was <a href="https://arxiv.org/pdf/2307.11291.pdf">suggested</a>, that the heavy-ball (HB) method provably does not reach an accelerated convergence rate on smooth strongly convex problems.</p></li>
<li><p>(20 points) In this problem we will work with accelerated methods applied to the logistic regression problem. A good visual introduction to the topic is available <a href="https://mlu-explain.github.io/logistic-regression/">here</a>.</p>
<p>Logistic regression is a standard model in classification tasks. For simplicity, consider only the case of binary classification. Informally, the problem is formulated as follows: There is a training sample <span class="math inline">\{(a_i, b_i)\}_{i=1}^m</span>, consisting of <span class="math inline">m</span> vectors <span class="math inline">a_i \in \mathbb{R}^n</span> (referred to as features) and corresponding numbers <span class="math inline">b_i \in \{-1, 1\}</span> (referred to as classes or labels). The goal is to construct an algorithm <span class="math inline">b(\cdot)</span>, which for any new feature vector <span class="math inline">a</span> automatically determines its class <span class="math inline">b(a) \in \{-1, 1\}</span>.</p>
<p>In the logistic regression model, the class determination is performed based on the sign of the linear combination of the components of the vector <span class="math inline">a</span> with some fixed coefficients <span class="math inline">x \in \mathbb{R}^n</span>:</p>
<p><span class="math display">
b(a) := \text{sign}(\langle a, x \rangle).
</span></p>
<p>The coefficients <span class="math inline">x</span> are the parameters of the model and are adjusted by solving the following optimization problem:</p>
<p><span class="math display">
\tag{LogReg}
\min_{x \in \mathbb{R}^n} \left( \frac{1}{m} \sum_{i=1}^m \ln(1 + \exp(-b_i \langle a_i, x \rangle)) + \frac{\lambda}{2} \|x\|^2 \right),
</span></p>
<p>where <span class="math inline">\lambda \geq 0</span> is the regularization coefficient (a model parameter).</p>
<ol type="1">
<li><p>Will the LogReg problem be convex for <span class="math inline">\lambda = 0</span>? What is the gradient of the objective function? Will it be strongly convex? What if you will add regularization with <span class="math inline">\lambda &gt; 0</span>?</p></li>
<li><p>We will work with the real-world data for <span class="math inline">A</span> and <span class="math inline">b</span>: take the mushroom dataset. Be careful, you will need to predict if the mushroom is poisonous or edible. A poor model can cause death in this exercise.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_svmlight_file</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># URL of the file to download</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">'https://mipt23.fmin.xyz/files/mushrooms.txt'</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the file and save it locally</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> requests.get(url)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> <span class="st">'mushrooms.txt'</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure the request was successful</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> response.status_code <span class="op">==</span> <span class="dv">200</span>:</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(dataset, <span class="st">'wb'</span>) <span class="im">as</span> f:</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        f.write(response.content)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load the dataset from the downloaded file</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> load_svmlight_file(dataset)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    A, b <span class="op">=</span> data[<span class="dv">0</span>].toarray(), data[<span class="dv">1</span>]</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    n, d <span class="op">=</span> A.shape</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Data loaded successfully."</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Number of samples: </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, Number of features: </span><span class="sc">{</span>d<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Failed to download the file. Status code: </span><span class="sc">{</span>response<span class="sc">.</span>status_code<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Divide the data into two parts: training and test. We will train the model on the <span class="math inline">A_{train}</span>, <span class="math inline">b_{train}</span> and measure the accuracy of the model on the <span class="math inline">A_{test}</span>, <span class="math inline">b_{test}</span>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and test sets</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>A_train, A_test, b_train, b_test <span class="op">=</span> train_test_split(A, b, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">214</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>For the training part <span class="math inline">A_{train}</span>, <span class="math inline">b_{train}</span>, estimate the constants <span class="math inline">\mu, L</span> of the training/optimization problem. Use the same small value <span class="math inline">\lambda</span> for all experiments</p></li>
<li><p>Using gradient descent with the step <span class="math inline">\frac{1}{L}</span>, train a model. Plot: accuracy versus iteration number.</p>
<p><span class="math display">
\tag{HB}
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1})
</span></p>
<p>Fix a step <span class="math inline">\alpha = \frac{1}{L}</span> and search for different values of the momentum <span class="math inline">\beta</span> from <span class="math inline">-1</span> to <span class="math inline">1</span>. Choose your own convergence criterion and plot convergence for several values of momentum on the same graph. Is the convergence always monotonic?</p></li>
<li><p>For the best value of momentum <span class="math inline">\beta</span>, plot the dependence of the model accuracy on the test sample on the running time of the method. Add to the same graph the convergence of gradient descent with step <span class="math inline">\frac{1}{L}</span>. Draw a conclusion. Ensure, that you use the same value of <span class="math inline">\lambda</span> for both methods.</p></li>
<li><p>Solve the logistic regression problem using the Nesterov method.</p>
<p><span class="math display">
\tag{NAG}
x_{k+1} = x_k - \alpha \nabla f(x_k + \beta (x_k - x_{k-1})) + \beta (x_k - x_{k-1})  
</span></p>
<p>Fix a step <span class="math inline">\frac{1}{L}</span> and search for different values of momentum <span class="math inline">\beta</span> from <span class="math inline">-1</span> to <span class="math inline">1</span>. Check also the momentum values equal to <span class="math inline">\frac{k}{k+3}</span>, <span class="math inline">\frac{k}{k+2}</span>, <span class="math inline">\frac{k}{k+1}</span> (<span class="math inline">k</span> is the number of iterations), and if you are solving a strongly convex problem, also <span class="math inline">\frac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}</span>. Plot the convergence of the method as a function of the number of iterations (choose the convergence criterion yourself) for different values of the momentum. Is the convergence always monotonic?</p></li>
<li><p>For the best value of momentum <span class="math inline">\beta</span>, plot the dependence of the model accuracy on the test sample on the running time of the method. Add this graph to the graphs for the heavy ball and gradient descent from the previous steps. Make a conclusion.</p></li>
<li><p>Now we drop the estimated value of <span class="math inline">L</span> and will try to do it adaptively. Let us make the selection of the constant <span class="math inline">L</span> adaptive.</p>
<p><span class="math display">
f(y) \leq f(x^k) + \langle \nabla f(x^k), y - x^k \rangle + \frac{L}{2}\|x^k - y\|_2^2
</span></p>
<p>In particular, the procedure might work:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backtracking_L(f, grad, x, h, L0, rho, maxiter<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> L0</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    fx <span class="op">=</span> f(x)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    gradx <span class="op">=</span> grad(x)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">iter</span> <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">iter</span> <span class="op">&lt;</span> maxiter :</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> x <span class="op">-</span> <span class="dv">1</span> <span class="op">/</span> L <span class="op">*</span> h</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> f(y) <span class="op">&lt;=</span> fx <span class="op">-</span> <span class="dv">1</span> <span class="op">/</span> L gradx.dot(h) <span class="op">+</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> L) h.dot(h):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>            L <span class="op">=</span> L <span class="op">*</span> rho</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">iter</span> <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> L</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>What should <span class="math inline">h</span> be taken as? Should <span class="math inline">\rho</span> be greater or less than <span class="math inline">1</span>? Should <span class="math inline">L_0</span> be taken as large or small? Draw a similar figure as it was in the previous step for L computed adaptively (6 lines - GD, HB, NAG, GD adaptive L, HB adaptive L, NAG adaptive L)</p></li>
</ol></li>
</ol>
</section>
<section id="gradient-methods-for-conditional-problems" class="level3">
<h3 class="anchored" data-anchor-id="gradient-methods-for-conditional-problems">Gradient methods for conditional problems</h3>
<ol type="1">
<li><p><strong><a href="https://courses.cs.ut.ee/MTAT.03.227/2015_spring/uploads/Main/home-exercises-5.pdf">üíç Hobbit village</a></strong> (Gradient descent + Newton method + Gradient descent in conditional optimization) (20 points)</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Below one can find function plotting the village</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_village(coordinates, l<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Checking, that all the coordinates are less than l</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> (coordinates <span class="op">&lt;=</span> l).<span class="bu">all</span>(), <span class="st">'All the houses should be in a village'</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Draw horizontal line</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    plt.hlines(<span class="dv">0</span>, <span class="dv">0</span>, l)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    plt.xlim(<span class="dv">0</span>, l)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    plt.ylim(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Draw house points</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.zeros(np.shape(coordinates))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'The Hobbit Village'</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    plt.plot(coordinates,y,<span class="st">'o'</span>,ms <span class="op">=</span> <span class="dv">10</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Coordinates'</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.gcf()</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    fig.set_size_inches(<span class="dv">15</span>, <span class="dv">1</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.rand(N)<span class="op">*</span>l</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>plot_village(x, l)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The inhabitants of a one-dimensional village want to connect to the Internet, so they need a central service station from which a cable will stretch to all the houses in the village. Let the price of the cable to be pulled from the station to each house independently be determined by some function p(d). Then it is clear that the village will have to pay the following amount for access to the World Wide Web: <span class="math display">
P(w, x) = \sum\limits_{i=1}^N p(d_i) = \sum\limits_{i=1}^N p(|w - x_i|)
</span> <span class="math inline">w</span> - station location, <span class="math inline">x_i</span> - location of <span class="math inline">i</span> house.</p>
<ol type="1">
<li><p>Write analytical solution <span class="math inline">w^*</span> for minimization <span class="math inline">P(w,x)</span>, if <span class="math inline">p(d) = d^2</span></p></li>
<li><p>Write loss function <span class="math inline">P(w,x)</span></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> P(w, x):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ((x<span class="op">-</span>w)<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>()</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pass</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Plot loss function on the range <span class="math inline">(0, l)</span>:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">### YOUR CODE</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>ws <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>plt.plot(ws, [P(w, x) <span class="cf">for</span> w <span class="kw">in</span> ws])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Write gradient of loss function</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dP(w, x):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="dv">2</span><span class="op">*</span>(w<span class="op">-</span>x)).<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Plot gradient of loss function on the range <span class="math inline">(0,l)</span>. Which point on the graph is of particular interest? Why?</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">### YOUR CODE</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>ws <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>plt.plot(ws, [dP(w, x) <span class="cf">for</span> w <span class="kw">in</span> ws])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Write function <code>gradient_descent</code>, which returns <span class="math inline">w_k</span> after a fixed number of steps.</p>
<p><span class="math display">
w_{k+1} = w_k - \mu \nabla P(w_k)
</span></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>w0 <span class="op">=</span> np.random.rand()<span class="op">*</span><span class="dv">10</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(x, dP, w0, mu, Nsteps):</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Modify <code>gradient_descent</code> to return the whole optimization trajectory. Plot loss function trajectory for the following learning rates <span class="math inline">\mu = 0.0001, 0.001, 0.01, 0.1</span>.<br>
Draw conclusions.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(x, dP, w0, mu, Nsteps):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> trajectory</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>The village decided to lay cable using new technology. That‚Äôs why the price of the cable changed to:</p>
<p><span class="math display">
p(d) = |d|
</span></p>
<p>Write new function <code>P</code>, <code>dP</code>. Plot graphs for various <span class="math inline">x</span> and <span class="math inline">w</span>.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> P(w, x):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dP(w, x):</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Write new analytical solytion <span class="math inline">w^*</span></p></li>
<li><p>Plot loss trajectory for converging gradient descent for the new <span class="math inline">p(d)</span>.</p></li>
<li><p>After several years, the government propose to destroy the first station but choose locations for two new stations. In this conditions cost of connecting all house calculated by new formula:</p>
<p><span class="math display">
P(w_1, w_2, x) = \sum\limits_{i=1}^N p(d_i) = \sum\limits_{i=1}^N p\left(\min\left(\left|w_1 - x_i\right|, \left|w_2 - x_i\right|\right)\right)
</span></p>
<p>Write new <code>P</code>, <code>dP</code>.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> P(w1, w2, x):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dP(w1, w2, x):</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Plot <span class="math inline">P(w_1, w_2), \nabla P(w_1, w_2)</span> for different number of houses <span class="math inline">N</span>. Comment on what happens as you increase <span class="math inline">N</span>.</p></li>
<li><p>Write new <code>gradient_descent</code>, which returns the entire optimization trajectory <span class="math inline">(w_k)</span> after a fixed number of steps and draws the process on the graphs <span class="math inline">P</span> and <span class="math inline">\nabla P</span> that were above. To ease visualization try to use <code>ax.quiver</code></p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(x, dP, w0, mu, Nsteps):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Construction is almost underway, but new safety regulations do not allow stations to be on the distance more than 1/2:</p>
<p><span class="math display">
\left|w_1 - w_2\right| \leq \dfrac{l}{2}
</span></p>
<p>Plot our feasible set. Is it convex?</p></li>
<li><p>Write <code>projected_SGD</code>, which returns the entire optimization trajectory <span class="math inline">(w_k)</span> after a fixed number of steps and draws the process on the graphs <span class="math inline">P</span> and <span class="math inline">\nabla P</span> that were above.</p>
<p>The projected gradient descent method consists in making a gradient step and then checking if the obtained point belongs to the feasible set. If it belongs to the target set, the algorithm continues, otherwise a projection to the feasible set is made.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> projected_SGD(x, dP_sigma, w0, mu, Nsteps, p<span class="op">=</span><span class="fl">0.4</span>):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> projection(w):</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>We have same loss function</p>
<p><span class="math display">
P(w, x) = \sum\limits_{i=1}^N p(d_i) = \sum\limits_{i=1}^N p(|w - x_i|),
</span></p>
<p>where <span class="math inline">p(d) = d^2</span></p>
<p>Write functions <code>P, dP, ddP</code>. <code>ddP</code> has to return hessian of loss function</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> P(w, x):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">### YOUR CODE</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dP(w, x):</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">### YOUR CODE</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> </span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ddP(w, x):</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">### YOUR CODE</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Plot <code>ddP</code> on the range (0,l)</p></li>
<li><p>Write function <code>newton_descent</code>, which return all optimization trajectory. Update rule:</p>
<p><span class="math display">
w_{i+1} = w_{i} - \nabla^{2} P\left(w_{i}\right)^{-1} \nabla P\left(w_{i}\right)
</span></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> newton_descent(x, dP, ddP, w0, Nsteps):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trajectory</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Write function <code>multi_newton</code>, which solve 2D task:</p>
<p><span class="math display">
P(w_1, w_2, x) = \sum\limits_{i=1}^N p(d_i) = \sum\limits_{i=1}^N p\left(\min\left(\left|w_1 - x_i\right|, \left|w_2 - x_i\right|\right)\right)
</span></p>
<p>with <span class="math inline">p(d) = d^3</span> using Newton method and return optimization trajectory. Compare results with gradient descent.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> P(w1, w2, x):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">### YOUR CODE</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> </span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dP(w1, w2, x):</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">### YOUR CODE</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> </span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ddP(w1, w2, x):</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">### YOUR CODE</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> </span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> multi_newton(x, dP, ddP, w0, Nsteps):</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">### YOUR CODE</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trajectory</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol></li>
<li><p><strong>üê∫ Frank - Wolfe vs Projected gradient descent</strong> (10 points)</p>
<p>Consider the following simple quadratic optimization problem</p>
<p><span class="math display">
f(w) = \frac12 \langle Ax, x \rangle - \langle b, x \rangle \to \min\limits_{x \in \mathbb{R}^n; \; 1^\top x = 1, \; x \succeq 0}
</span></p>
<ol type="1">
<li><p>Generate and solve this problem numerically with <em>CVXPY</em>. Calculate optimal solution <code>x_optimal</code></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> numpy <span class="im">as</span> jnp</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> random</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> grad</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_problem(n, mu<span class="op">=</span><span class="dv">0</span>, L <span class="op">=</span> <span class="dv">10</span>):</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    RNG <span class="op">=</span> random.PRNGKey(<span class="dv">0</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    U <span class="op">=</span> random.normal(RNG, (n, n))</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    Q, _ <span class="op">=</span> jnp.linalg.qr(U)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    Lambda <span class="op">=</span> jnp.diag(jnp.linspace(mu, L, n, endpoint<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> Q <span class="op">@</span> Lambda <span class="op">@</span> Q.T</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    RNG, _ <span class="op">=</span> random.split(RNG)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> random.normal(RNG, (n, <span class="dv">1</span>))</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> f(x, A<span class="op">=</span>A, b<span class="op">=</span>b):</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span> x.T <span class="op">@</span> A <span class="op">@</span> x <span class="op">-</span> b.T <span class="op">@</span> x</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>    grad_f <span class="op">=</span> grad(f)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>    RNG, _ <span class="op">=</span> random.split(RNG)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>    x_0 <span class="op">=</span> jnp.zeros(n)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> random.randint(RNG, (<span class="dv">1</span>,), <span class="dv">0</span>, n)</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    x_0 <span class="op">=</span> x_0.at[idx].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> f, grad_f, A, b, x_0</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute optimal(A, b):</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">### ======</span></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>    <span class="co">### YOUR CODE HERE</span></span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>    <span class="co">### ======</span></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_optimal</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>In this problem you will consider 2 algorithms for solving this problem (Frank - Wolfe and Projected Gradient Descent). Let‚Äôs start with PGD. Write down the function, that calculates Euclidian projection on the simplex:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> projection(y):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">### ======</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">### YOUR CODE HERE</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">### ======</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Then, write the PGD method, that returns a <code>trajectory</code> list of <code>iterations+1</code> points <span class="math inline">x_0, x_1, \ldots x_k</span> and <code>time_trajectory</code> list for a cumulative time spent after each iteration :</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> PGD(A, b, x_0, iterations):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trajectory, time_trajectory</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Write down the FW method, that returns a <code>trajectory</code> list of <code>iterations+1</code> points <span class="math inline">x_0, x_1, \ldots x_k</span> and <code>time_trajectory</code> list for a cumulative time spent after each iteration :</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> FW(A, b, x_0, iterations):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trajectory, time_trajectory</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Generate a convex problem (<span class="math inline">\mu=0, L=10</span>) and compare the methods starting from the same <span class="math inline">x_0</span>. For this reason plot 2 graphs - <span class="math inline">f(x_k) - f^*</span> from iteration counter and time spent for it.</p></li>
<li><p>Generate a strongly convex problem (<span class="math inline">\mu=1, L=10</span>) and compare the methods starting from the same <span class="math inline">x_0</span>. For this reason plot 2 graphs - <span class="math inline">f(x_k) - f^*</span> from iteration counter and time spent for it.</p></li>
</ol></li>
</ol>
</section>
<section id="conjugate-gradients" class="level3">
<h3 class="anchored" data-anchor-id="conjugate-gradients">Conjugate gradients</h3>
<ol type="1">
<li><p><strong><a href="https://web.stanford.edu/class/ee364b/364b_exercises.pdf">Randomized Preconditioners for Conjugate Gradient Methods.</a></strong> (20 points)</p>
<p><em>Linear least squares</em></p>
<p>In this task, we explore the use of some randomization methods for solving overdetermined least-squares problems, focusing on conjugate gradient methods. Let <span class="math inline">\hat{A} \in \mathbb{R}^{m \times n}</span> be a matrix (we assume that <span class="math inline">m \gg n</span>) and <span class="math inline">b \in \mathbb{R}^m</span>, we aim to minimize</p>
<p><span class="math display">
f(x) = \frac{1}{2} \|\hat{A}x - b\|^2_2 = \frac{1}{2} \sum_{i=1}^m (\hat{a}_i^T x - b_i)^2,
</span></p>
<p>where the <span class="math inline">\hat{a}_i \in \mathbb{R}^n</span> denote the rows of <span class="math inline">\hat{A}</span>.</p>
<p><em>Preconditioners</em></p>
<p>We know, that the convergence bound of the CG applied for the problem depends on the condition number of the matrix. Note, that for the problem above we have the matrix <span class="math inline">\hat{A}^T \hat{A}</span> and the condition number is squared after this operation (<span class="math inline">\kappa (X^T X) =  \kappa^2 \left(X \right)</span>). That is the reason, why we typically need to use <em>preconditioners</em> (<a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">read 12. for more details</a>) with CG.</p>
<p>The general idea of using preconditioners implies switchwing from solving <span class="math inline">Ax = b</span> to <span class="math inline">MAx = Mb</span> with hope, that <span class="math inline">\kappa \left( MA\right) \ll \kappa \left( A\right)</span> or eigenvalues of <span class="math inline">MA</span> are better clustered than those of <span class="math inline">A</span> (note, that matrix <span class="math inline">A</span> here is for the general case, here we have <span class="math inline">\hat{A}^T\hat{A}</span> instead).</p>
<p>This idea can also be viewed as coordinate change <span class="math inline">x = T \hat{x}, \; \hat{x} = T^{-1}x</span>, which leads to the problem <span class="math inline">T^T A T \hat{x} = T^Tb</span>. Note, that the spectrum of <span class="math inline">T^TAT</span> is the same as the spectrum of <span class="math inline">MA</span>.</p>
<p>The best choice of <span class="math inline">M</span> is <span class="math inline">A^{-1}</span>, because <span class="math inline">\kappa (A^{-1} A) = \kappa (I) = 1</span>. However, if we know <span class="math inline">A^{-1}</span>, the original problem is already solved, that is why we need to find some trade-off between enhanced convergence, and extra cost of working with <span class="math inline">M</span>. The goal is to find <span class="math inline">M</span> that is cheap to multiply, and approximate inverse of <span class="math inline">A</span> (or at least has a more clustered spectrum than <span class="math inline">A</span>).</p>
<p>Note, that for the linear least squares problem the matrix of quadratic form is <span class="math inline">A = \hat{A}^T\hat{A}</span>. Below you can find Vanilla CG algorithm (on the left) and preconditioned CG algorithm (on the right):</p>
<p><span class="math display">
\begin{aligned}
&amp; \mathbf{r}_0 := \mathbf{b} - \mathbf{A x}_0 \\
&amp; \hbox{if } \mathbf{r}_{0} \text{ is sufficiently small, then return } \mathbf{x}_{0} \text{ as the result}\\
&amp; \mathbf{d}_0 := \mathbf{r}_0 \\
&amp; k := 0 \\
&amp; \text{repeat} \\
&amp; \qquad \alpha_k := \frac{\mathbf{r}_k^\mathsf{T} \mathbf{r}_k}{\mathbf{d}_k^\mathsf{T} \mathbf{A d}_k}  \\
&amp; \qquad \mathbf{x}_{k+1} := \mathbf{x}_k + \alpha_k \mathbf{d}_k \\
&amp; \qquad \mathbf{r}_{k+1} := \mathbf{r}_k - \alpha_k \mathbf{A d}_k \\
&amp; \qquad \hbox{if } \mathbf{r}_{k+1} \text{ is sufficiently small, then exit loop} \\
&amp; \qquad \beta_k := \frac{\mathbf{r}_{k+1}^\mathsf{T} \mathbf{r}_{k+1}}{\mathbf{r}_k^\mathsf{T} \mathbf{r}_k} \\
&amp; \qquad \mathbf{d}_{k+1} := \mathbf{r}_{k+1} + \beta_k \mathbf{d}_k \\
&amp; \qquad k := k + 1 \\
&amp; \text{end repeat} \\
&amp; \text{return } \mathbf{x}_{k+1} \text{ as the result}
\end{aligned} \qquad
\begin{aligned}
&amp; \mathbf{r}_0 := \mathbf{b} - \mathbf{A x}_0 \\
&amp; \text{if } \mathbf{r}_0 \text{ is sufficiently small, then return } \mathbf{x}_0 \text{ as the result} \\
&amp; \mathbf{z}_0 := \mathbf{M}^{-1} \mathbf{r}_0 \\
&amp; \mathbf{d}_0 := \mathbf{z}_0 \\
&amp; k := 0 \\
&amp; \text{repeat} \\
&amp; \qquad \alpha_k := \frac{\mathbf{r}_k^\mathsf{T} \mathbf{z}_k}{\mathbf{d}_k^\mathsf{T} \mathbf{A d}_k} \\
&amp; \qquad \mathbf{x}_{k+1} := \mathbf{x}_k + \alpha_k \mathbf{d}_k \\
&amp; \qquad \mathbf{r}_{k+1} := \mathbf{r}_k - \alpha_k \mathbf{A d}_k \\
&amp; \qquad \text{if } \mathbf{r}_{k+1} \text{ is sufficiently small, then exit loop} \\
&amp; \qquad \mathbf{z}_{k+1} := \mathbf{M}^{-1} \mathbf{r}_{k+1} \\
&amp; \qquad \beta_k := \frac{\mathbf{r}_{k+1}^\mathsf{T} \mathbf{z}_{k+1}}{\mathbf{r}_k^\mathsf{T} \mathbf{z}_k} \\
&amp; \qquad \mathbf{d}_{k+1} := \mathbf{z}_{k+1} + \beta_k \mathbf{d}_k \\
&amp; \qquad k := k + 1 \\
&amp; \text{end repeat} \\
&amp; \text{return } \mathbf{x}_{k+1} \text{ as the result}
\end{aligned}
</span></p>
<p><em>Hadamard matrix</em></p>
<p>Given <span class="math inline">m \in \{2^i, i = 1, 2, \ldots\}</span>, the (unnormalized) Hadamard matrix of order <span class="math inline">m</span> is defined recursively as</p>
<p><span class="math display">
H_2 = \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix}, \quad \text{and} \quad H_m = \begin{bmatrix} H_{m/2} &amp; H_{m/2} \\ H_{m/2} &amp; -H_{m/2} \end{bmatrix}.
</span></p>
<p>The associated normalized Hadamard matrix is given by <span class="math inline">H^{(\text{norm})}_m = \frac{1}{\sqrt{m}} H_m</span>, which evidently satisfies <span class="math inline">H^{(\text{norm})T}_m H^{(\text{norm})}_m = I_{m \times m}</span>. Moreover, via a recursive algorithm, it is possible to compute matvec <span class="math inline">H_m x</span> in time <span class="math inline">O(m \log m)</span>, which is much faster than <span class="math inline">m^2</span> for a general matrix.</p>
<p>To solve the least squares minimization problem using conjugate gradients, we must solve <span class="math inline">\hat{A}^T \hat{A} x = \hat{A}^T b</span>. Using a preconditioner <span class="math inline">M</span> such that <span class="math inline">M \approx A^{-1}</span> can give substantial speedup in computing solutions to large problems.</p>
<p>Consider the following scheme to generate a randomized preconditioner, assuming that <span class="math inline">m = 2^i</span> for some <span class="math inline">i</span>:</p>
<ol type="1">
<li><p>Let <span class="math inline">S = \text{diag}(S_{11}, \ldots, S_{mm})</span>, where <span class="math inline">S_{jj}</span> are random <span class="math inline">\{-1,+1\}</span> signs</p></li>
<li><p>Let <span class="math inline">p \in \mathbb{Z}^+</span> be a small positive integer, say <span class="math inline">20</span> for this problem.</p></li>
<li><p>Let <span class="math inline">R \in \{0, 1\}^{n+p \times m}</span> be a <em>row selection matrix</em>, meaning that each row of <span class="math inline">R</span> has only 1 non-zero entry, chosen uniformly at random. (The location of these non-zero columns is distinct.)</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> random</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_row_selection_matrix_jax(m, n, p, key):</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># m is the number of columns in the original matrix A</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># n+p is the number of rows in the row selection matrix R</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># key is a PRNGKey needed for randomness in JAX</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    inds <span class="op">=</span> random.permutation(key, m)[:n<span class="op">+</span>p]  <span class="co"># Generate a random permutation and select the first n+p indices</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> jnp.zeros((n<span class="op">+</span>p, m), dtype<span class="op">=</span>jnp.int32)  <span class="co"># Create a zero matrix of shape (n+p, m)</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> R.at[np.arange(n<span class="op">+</span>p), inds].<span class="bu">set</span>(<span class="dv">1</span>)     <span class="co"># Use JAX's indexed update to set the entries corresponding to inds to 1</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> R</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Define <span class="math inline">\Phi = R H^{(\text{norm})}_m S \in \mathbb{R}^{n+p \times m}</span></p></li>
</ol>
<p>We then define the matrix <span class="math inline">M</span> via its inverse <span class="math inline">M^{-1} = \hat{A}^T \Phi^T \Phi \hat{A} \in \mathbb{R}^{n \times n}</span>.</p>
<p><em>Questions</em></p>
<ol type="1">
<li><p><strong>(2 point)</strong> How many FLOPs (floating point operations, i.e.&nbsp;multiplication and additions) are required to compute the matrices <span class="math inline">M^{-1}</span> and <span class="math inline">M</span>, respectively, assuming that you can compute the matrix-vector product <span class="math inline">H_mv</span> in time <span class="math inline">m \log m</span> for any vector <span class="math inline">v \in \mathbb{R}^m</span>?</p></li>
<li><p><strong>(2 point)</strong> How many FLOPs are required to naively compute <span class="math inline">\hat{A}^T \hat{A}</span>, assuming <span class="math inline">\hat{A}</span> is dense (using standard matrix algorithms)?</p></li>
<li><p><strong>(2 point)</strong> How many FLOPs are required to compute <span class="math inline">\hat{A}^T \hat{A} v</span> for a vector <span class="math inline">v \in \mathbb{R}^n</span> by first computing <span class="math inline">u = \hat{A}v</span> and then computing <span class="math inline">\hat{A}^T u</span>?</p></li>
<li><p><strong>(4 poins)</strong> Suppose that conjugate gradients runs for <span class="math inline">k</span> iterations. Using the preconditioned conjugate gradient algorithm with <span class="math inline">M = (\hat{A}^T \Phi^T \Phi \hat{A})^{-1}</span>, how many total floating point operations have been performed? How many would be required to directly solve <span class="math inline">\hat{A}^T \hat{A} x = \hat{A}^T b</span>? How large must <span class="math inline">k</span> be to make the conjugate gradient method slower?</p></li>
<li><p><strong>(10 points)</strong> Implement the conjugate gradient algorithm for solving the positive definite linear system <span class="math inline">\hat{A}^T \hat{A} x = \hat{A}^T b</span> both with and without the preconditioner <span class="math inline">M</span>. To generate data for your problem, set <span class="math inline">m = 2^{12}</span> and <span class="math inline">n = 400</span>, then generate the matrix <span class="math inline">A</span> and the vector <span class="math inline">b</span>. For simplicity in implementation, you may directly pass <span class="math inline">\hat{A}^T \hat{A}</span> and <span class="math inline">\hat{A}^T b</span> into your conjugate gradient solver, as we only wish to explore how the methods work.</p></li>
</ol>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.sparse <span class="im">import</span> diags</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">2</span><span class="op">**</span><span class="dv">12</span>  <span class="co"># 4096</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">400</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a linear space of values from 0.001 to 100</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> np.linspace(<span class="fl">0.001</span>, <span class="dv">100</span>, n)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the matrix A</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.random.randn(m, n) <span class="op">*</span> diags(values).toarray()</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.random.randn(m, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Plot the norm of the residual <span class="math inline">r_k = \hat{A}^T b - \hat{A}^T \hat{A} x_k</span> (relative to <span class="math inline">\|\hat{A}^T b\|_2</span>) as a function of iteration <span class="math inline">k</span> for each of your conjugate gradient procedures. Additionally, compute and print the condition numbers <span class="math inline">\kappa(\hat{A}^T \hat{A})</span> and <span class="math inline">\kappa(M^{1/2} \hat{A}^T \hat{A} M^{1/2})</span>.</p></li>
</ol>
</section>
<section id="newton-and-quasinewton-methods" class="level3">
<h3 class="anchored" data-anchor-id="newton-and-quasinewton-methods">Newton and quasinewton methods</h3>
<ol type="1">
<li><p><strong>üò± Newton convergence issue</strong> (10 points)</p>
<p>Consider the following function:</p>
<p><span class="math display">
f(x,y) = \dfrac{x^4}{4} - x^2 + 2x + (y-1)^2
</span></p>
<p>And the starting point is <span class="math inline">x_0 = (0,2)^\top</span>. How does Newton‚Äôs method behave when started from this point? How can this be explained? How does the gradient descent with fixed step <span class="math inline">\alpha = 0.01</span> and the steepest descent method behave under the same conditions? (It is not necessary to show numerical simulations in this problem).</p></li>
<li><p><strong>Hessian-Free Newton method</strong> (20 points) In this exercise, we‚Äôll explore the optimization of a binary logistic regression problem using various methods. Don‚Äôt worry about the size of the problem description, first 5 bullets out of 7 could be done pretty quickly. In this problem you should start with this <a href="https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Hessian_free_Newton.ipynb">colab notebook</a></p>
<p>Given a dataset with <span class="math inline">n</span> observations, where each observation consists of a feature vector <span class="math inline">x_i</span> and an associated binary target variable <span class="math inline">y_i \in \{0,1\}</span>, the logistic regression model predicts the probability that <span class="math inline">y_i = 1</span> given <span class="math inline">x_i</span> using the logistic function. The loss function to be minimized is the negative log-likelihood of the observed outcomes under this model, summed across all observations. It has a high value when the model outputs differ significantly from the data <span class="math inline">y</span>.</p>
<p>The binary cross-entropy loss function for a single observation <span class="math inline">(x_i, y_i)</span> is given by: <span class="math display">
\text{Loss}(w; x_i, y_i) = -\left[ y_i \log(p(y_i=1 | x_i; w)) + (1-y_i) \log(1-p(y_i=1 | x_i; w)) \right]
</span></p>
<p>Here, <span class="math inline">p(y=1 | x;w)</span> is defined as: <span class="math display">
p(y=1 | x;w) = \frac{1}{1 + e^{-w^T x}}
</span></p>
<p>To define the total loss over the dataset, we sum up the individual losses: <span class="math display">
f(w) = -\sum_{i=1}^n \left[ y_i \log(p(y_i=1 | x_i; w)) + (1-y_i) \log(1-p(y_i=1 | x_i; w)) \right]
</span></p>
<p>Therefore, the optimization problem in logistic regression is: <span class="math display">
\min_w f(w) = \min_w -\sum_{i=1}^n \left[ y_i \log\left(p\left(y_i=1 | x_i; w\right)\right) + \left(1-y_i\right) \log\left(1-p(y_i=1 | x_i; w)\right) \right]
</span></p>
<p>This is a convex optimization problem and can be solved using gradient-based methods such as gradient descent, Newton‚Äôs method, or more sophisticated optimization algorithms often available in machine learning libraries. However, it is the problem is often together with <span class="math inline">l_2</span> regularization:</p>
<p><span class="math display">
\min_w f(w) = \min_w -\sum_{i=1}^n \left[ y_i \log\left(p\left(y_i=1 | x_i; w\right)\right) + \left(1-y_i\right) \log\left(1-p(y_i=1 | x_i; w)\right) \right] + \frac{\mu}{2} \|w\|_2^2
</span></p>
<ol type="1">
<li><p>(2 points) Firstly, we address the optimization with Gradient Descent (GD) in a strongly convex setting, with <span class="math inline">\mu = 1</span>. Use a constant learning rate <span class="math inline">\alpha</span>. Run the gradient descent algorithm. Report the highest learning rate that ensures convergence of the algorithm. Plot the convergence graph in terms of both domain (parameter values) and function value (loss). Describe the type of convergence observed.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="dv">1</span>,</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">1000</span>,</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">100</span>,</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"GD"</span>,</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">3e-2</span>,</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">550</span>,</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>results, params <span class="op">=</span> run_experiments(params)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>(2 points) Run Newton‚Äôs method under the same conditions, using the second derivatives to guide the optimization. Describe and analyze the convergence properties observed.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="dv">1</span>,</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">1000</span>,</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">100</span>,</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"GD"</span>,</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">3e-2</span>,</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">550</span>,</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"Newton"</span>,</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">20</span>,</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>results, params <span class="op">=</span> run_experiments(params)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>(2 points) In cases where Newton‚Äôs method may converge too rapidly or overshoot, a damped version can be more stable. Run the damped Newton method. Adjust the damping factor as a learning rate. Report the highest learning rate ensuring stability and convergence. Plot the convergence graph.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="dv">1</span>,</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">1000</span>,</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">100</span>,</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"GD"</span>,</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">3e-2</span>,</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">550</span>,</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"Newton"</span>,</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">20</span>,</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"Newton"</span>,</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">5e-1</span>,</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">50</span>,</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>results, params <span class="op">=</span> run_experiments(params)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>(2 points) Now turn off the regularization by setting <span class="math inline">\mu=0</span>. Try to find the largest learning rate, which ensures convergence of the Gradient Descent. Use a constant learning rate <span class="math inline">\alpha</span>. Run the gradient descent algorithm. Report the highest learning rate that ensures convergence of the algorithm. Plot the convergence graph in terms of both domain (parameter values) and function value (loss). Describe the type of convergence observed. How can you describe an idea to run this method for the problem to reach tight primal gap <span class="math inline">f(x_k) - f^* \approx 10^{-2}</span> or <span class="math inline">10^{-3}</span>, <span class="math inline">10^{-4}</span>?</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="dv">0</span>,</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">1000</span>,</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">100</span>,</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"GD"</span>,</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">3e-2</span>,</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">200</span>,</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"GD"</span>,</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">7e-2</span>,</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">200</span>,</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>results, params <span class="op">=</span> run_experiments(params)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>(2 points) What can you say about Newton‚Äôs method convergence in the same setting <span class="math inline">\mu=0</span>? Try several learning rates smaller, than <span class="math inline">1</span> for the damped Newton method. Does it work? Write your conclusions about the second-order method convergence for a binary logistic regression problem.</p></li>
<li><p>(5 points) Now switch back to the strongly convex setting <span class="math inline">\mu=1</span>. To avoid directly computing the Hessian matrix in Newton‚Äôs method, use the Conjugate Gradient (CG) method to solve the linear system in the Newton step. Develop the <code>newton_method_cg</code> function, which computes the Newton step using CG to solve the system <span class="math inline">\nabla^2 f(x_k) d_k = - \nabla f(x_k), \; x_{k+1} = x_k - \alpha d_k</span> defined by the Hessian. You have to use <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.sparse.linalg.cg.html"><code>jax.scipy.sparse.linalg.cg</code></a> function here. So, firstly compute the hessian as it was done in the code, then put it into this linear solver. Compare its performance in terms of computational efficiency and convergence rate to the standard Newton method.</p></li>
<li><p>(5 points) Finally, implement a Hessian-free version of Newton‚Äôs method (HFN) which utilizes Hessian-vector products derived via automatic differentiation. Note, that <code>jax.scipy.sparse.linalg.cg</code> function can take the matvec function, which directly produces the multiplication of any input vector <span class="math inline">x</span>. Implement the HFN method without explicitly forming or storing the Hessian matrix in function <code>newton_method_hfn</code>. Use autograd to compute Hessian-vector products as it is described <a href="https://iclr-blogposts.github.io/2024/blog/bench-hvp/">here</a>. Compare this method‚Äôs time complexity and memory requirements against previous implementations.</p></li>
</ol></li>
</ol>
</section>
<section id="proximal-gradient-method" class="level3">
<h3 class="anchored" data-anchor-id="proximal-gradient-method">Proximal Gradient Method</h3>
<ol type="1">
<li><p>[10 points] <strong>Proximal Method for Sparse Softmax Regression</strong> Softmax regression, also known as multinomial logistic regression, is a generalization of logistic regression to multiple classes. It is used to model categorical outcome variables where each category is mutually exclusive. The softmax function transforms any input vector to the probability-like vector as follows:</p>
<p><span class="math display">
P(y = j | x; W) = \frac{e^{W_j^T x}}{\sum\limits_{i=1}^{c} e^{W_i^T x}}
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Softmax.svg" class="img-fluid figure-img"></p>
<figcaption>Scheme of softmax regression</figcaption>
</figure>
</div>
<p>where <span class="math inline">x</span> is the input vector, <span class="math inline">W</span> is the weight matrix, <span class="math inline">c</span> is the number of classes, and <span class="math inline">P(y = j | x; W)</span> is the probability that the input <span class="math inline">x</span> belongs to class <span class="math inline">j</span>.</p>
<p>The optimization problem for softmax regression is to minimize the negative log-likelihood:</p>
<p><span class="math display">
\min_{W \in \mathbb{R}^{c \times d}} -\sum_{i=1}^{N} \log P(y_i | x_i; W) + \lambda \| W \|_1
</span></p>
<p>where <span class="math inline">N</span> is the number of training examples, <span class="math inline">\lambda</span> is the regularization parameter, and <span class="math inline">\| W \|_1</span> is the L1 norm of the weight matrix, which promotes sparsity in the solution. I suggest you to vectorize matrix and add <span class="math inline">1</span>-vector norm.</p>
<p>We will solve the sparse softmax regression problem using the subgradient method and the proximal gradient method, both incorporating L1 regularization. The proximal gradient method is particularly useful for optimization problems involving non-smooth regularizers like the L1 norm. We will use 3 class classification problem of <a href="https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success">Predicting Students‚Äô Dropout and Academic Success</a>. In this problem you should start with this <a href="https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Proximal_softmax_regression.ipynb">colab notebook</a></p>
<ol type="1">
<li>[2 points] Write down exact formulation of subgradient method and proximal gradient method here (you can not use any optimization problems in this formulation).</li>
<li>[3 points] Choose <span class="math inline">\lambda = 0</span>. Solve the softmax regression problem using subgradient descent and proximal gradient descent. Find the highest learning (individually), acceptable for both methods to converge. Report convergence curves and report final sparsity of both methods. Draw you conclusions.</li>
<li>[5 points] Solve non-smooth problem and fill the following table. For each value of <span class="math inline">\lambda</span> provide convergence curves.</li>
</ol>
<p>Report the number of iterations needed to reach specified primal gaps for each method. Present the results in the following markdown table:</p>
<table class="table">
<colgroup>
<col style="width: 14%">
<col style="width: 13%">
<col style="width: 12%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 16%">
<col style="width: 5%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Learning Rate (<span class="math inline">\eta</span>)</th>
<th style="text-align: right;">Tolerance (<span class="math inline">\epsilon</span>)</th>
<th style="text-align: center;">Number of Iterations</th>
<th style="text-align: center;">Comment(if any)</th>
<th style="text-align: center;">Final Sparsity of the solution</th>
<th style="text-align: left;"><span class="math inline">\lambda</span></th>
<th style="text-align: center;">Final test accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Subgradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-1}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Subgradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Subgradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-3}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Subgradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-4}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Subgradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-5}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-1}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-3}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-4}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-5}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Subgradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-3</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-3</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Subgradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-1</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-1</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Subgradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1</code></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table></li>
</ol>
</section>
<section id="stochastic-gradient-methods" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-gradient-methods">Stochastic gradient methods</h3>
<ol type="1">
<li><p><strong>Variance reduction for stochastic gradient methods for neural networks</strong>. [10 points]</p>
<p>Open <a href="https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/VR_exercise.ipynb">colab notebook</a>. Implement SAG and SVRG method. Consider Linear least squares problem with the following setup</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="dv">0</span>,</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">50</span>,</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">100</span>,</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">1000</span>,</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">50</span>,</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">1000</span>,</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SAG"</span>,</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">1000</span>,</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SVRG"</span>,</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>            <span class="st">"epoch_length"</span>: <span class="dv">2</span>,</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">1000</span>,</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_experiments(params)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Then, consider strongly convex case with:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="fl">1e-1</span>,</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">50</span>,</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">100</span>,</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">50</span>,</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SAG"</span>,</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SVRG"</span>,</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>            <span class="st">"epoch_length"</span>: <span class="dv">2</span>,</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And for the convex binary logistic regression:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="dv">0</span>,</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">100</span>,</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">200</span>,</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SAG"</span>,</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SVRG"</span>,</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>            <span class="st">"epoch_length"</span>: <span class="dv">3</span>,</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">100</span>,</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>and strongly convex case</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="fl">1e-1</span>,</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">100</span>,</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">200</span>,</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">2e-2</span>,</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">3000</span>,</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SAG"</span>,</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">2e-2</span>,</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">3000</span>,</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SVRG"</span>,</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">2e-2</span>,</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>            <span class="st">"epoch_length"</span>: <span class="dv">3</span>,</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">3000</span>,</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">2e-2</span>,</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">100</span>,</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">3000</span>,</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Describe the obtained convergence and compare methods.</p>
<p><img src="lls_VR.svg" class="img-fluid"></p>
<p><img src="logreg_VR.svg" class="img-fluid"></p></li>
</ol>
</section>
<section id="big-models" class="level2">
<h2 class="anchored" data-anchor-id="big-models">Big models</h2>
<ol type="1">
<li><p><strong>Fit the largest model you can on a single GPU.</strong> [20 points]</p>
<p>In this assignment, you will train a language model (LM) using the TinyStories dataset, focusing on optimizing model performance within the constraints of Google Colab‚Äôs hardware. For the sake of speed, we will do it on the part of the dataset.</p>
<p><code>Tiny Stories  Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun.   Beep was a healthy car because he always had good fuel....</code></p>
<p>Your objective is to maximize the size of the model without exceeding the available computational resources (~ 16GB VRAM). You could start with the Hugging Face Transformers library and experiment with various memory optimization techniques, such as (but not limited to):</p>
<pre><code> * Different batch size
 * Different optimizer
 * Gradient accumulation
 * Activation checkpointing
 * CPU offloading
 * 8bit optimizers</code></pre>
<p>You have a baseline of training <code>gpt-2</code> model prepared at the following <a href="https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/TinyStories_baseline.ipynb">colab notebook</a>. You can easily switch it to <code>opt-350m</code>, <code>opt-1.3b</code>, <code>gpt2</code> etc. You can find a great beginner-level guide on the topic <a href="https://huggingface.co/docs/transformers/v4.18.0/en/performance">here</a>.</p>
<p><code>GPT-2 generation  A long time ago in a galaxy far far away... a little girl named Lily was playing in the garden. She was so excited! She wanted to explore the garden and see what was around her.  Suddenly, she heard a loud noise. Lily looked up and saw a big, hairy creature. Lily was so excited! She ran to the creature and grabbed it by the arm. The creature was so big and hairy that Lily couldn't help but laugh.</code></p>
<p><img src="gpt2_generation.jpeg" class="img-fluid"></p>
<p>You have to fill this table with your description/observations.</p>
<table class="table">
<colgroup>
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Setup</th>
<th style="text-align: center;"># of parameters</th>
<th style="text-align: center;">GPU peak memory, MB</th>
<th style="text-align: center;">Final eval loss</th>
<th>Batch Size</th>
<th>Time to run 5 epochs, s</th>
<th>Generation example</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Baseline (OPT-125M)</td>
<td style="text-align: center;">125 M</td>
<td style="text-align: center;">9044</td>
<td style="text-align: center;">1.928</td>
<td>8</td>
<td>442.34</td>
<td><code>A long time ago in a galaxy far far away... there was a little girl named Lily. She was three years old and loved to explore. One day, she decided to go for a walk in the park. Lily was so excited to go for a walk. She asked her mom, "What do you want to do?" Her mom smiled and said, "I want to explore the galaxy." Lily was so excited to explore the galaxy.</code></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: center;">Baseline (GPT2-S)</td>
<td style="text-align: center;">124 M</td>
<td style="text-align: center;">13016</td>
<td style="text-align: center;">2.001</td>
<td>8</td>
<td>487.75</td>
<td><code>A long time ago in a galaxy far far away... a little girl named Lily was playing in the garden. She was so excited! She wanted to explore the garden and see what was around her. Suddenly, she heard a loud noise. Lily looked up and saw a big, hairy creature. Lily was so excited! She ran to the creature and grabbed it by the arm. The creature was so big and hairy that Lily couldn't help but laugh.</code></td>
<td>The generation seems more interesting, despite the fact, that eval loss is higher.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>For each unique trick for memory optimization, you will get 4 points (maximum 20 points). A combination of tricks is not counted as a unique trick, but will, probably, be necessary to train big models. The maximum grade is bounded with the size of the trained model: * If the model size you train is &lt;= 125M - you can get a maximum of 8 points. * If the model size you train is 126M &lt;= 350M - you can get a maximum of 12 points. * If the model size you train is 350M &lt;= 1B - you can get a maximum of 16 points. * If you fit 1B model or more - you can get a maximum 20 points.</p></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/mipt23\.fmin\.xyz");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/MerkulovDaniil/mipt23/edit/main/homework.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer><script>videojs(video_shortcode_videojs_video1);</script>




</body></html>