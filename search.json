[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "–ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ú–§–¢–ò 2023-2024",
    "section": "",
    "text": "–ö—É—Ä—Å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–≤–µ–¥–µ–Ω–∏–µ –≤ —Ä–∞–∑–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –Ω–∞—á–∞–ª–∞ –≤—ã–ø—É–∫–ª–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞, –∏–∑–ª–∞–≥–∞—é—Ç—Å—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –ø–æ–¥—Ö–æ–¥—ã –≤ —Ä–µ—à–µ–Ω–∏–∏ –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö –∑–∞–¥–∞—á –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ö—É—Ä—Å —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞–±–æ—Ä —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Å–Ω–æ–≤ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø–æ–Ω–∏–º–∞—Ç—å –ø–æ—á–µ–º—É –∏ –∫–∞–∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ä–∞–±–æ—Ç–∞—é—Ç. –í –Ω–∞—á–∞–ª–µ –∫—É—Ä—Å–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —É–ø–æ—Ä –¥–µ–ª–∞–µ—Ç—Å—è –Ω–∞ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–ø–ø–∞—Ä–∞—Ç, –≤ –∫–æ–Ω—Ü–µ —É–¥–µ–ª—è–µ—Ç—Å—è –±–æ–ª—å—à–µ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ö–∞–∂–¥–æ–µ –∑–∞–Ω—è—Ç–∏–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–Ω—è—Ç–∏—è. –ü–µ—Ä–≤–∞—è —á–∞—Å—Ç—å –∫—É—Ä—Å–∞ –±–æ–ª—å—à–µ —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–∞ –Ω–µ —Ç–µ–æ—Ä–∏—é, –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –í–æ –≤—Ç–æ—Ä–æ–π —á–∞—Å—Ç–∏ –¥–µ–ª–∞–µ—Ç—Å—è —É–ø–æ—Ä –Ω–∞ –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –Ω–∞—á–∏–Ω–∞—è c –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∑–∞–∫–∞–Ω—á–∏–≤–∞—è —Å–∞–º—ã–º–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏.\n                        \n                                            \n\n\nüìú –°–ø–∏—Å–æ–∫ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∏ —Ç–µ–æ—Ä–µ–º\n–û—Ü–µ–Ω–∫–∞ –∑–∞ —ç–∫–∑–∞–º–µ–Ω —Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –∏–∑ 4 —á–∞—Å—Ç–µ–π:\n\n–í–æ–ø—Ä–æ—Å—ã –ø–æ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º - 2 –±–∞–ª–ª–∞\n–°–Ω–∞—á–∞–ª–∞ –≤—ã–¥–∞—é—Ç—Å—è 4 —Å–ª—É—á–∞–π–Ω—ã—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π/—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∏–∑ —Å–ø–∏—Å–∫–∞. –ù–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–µ—Ç—Å—è 10 –º–∏–Ω—É—Ç. –ü—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –æ—Ç–≤–µ—Ç–µ —Ö–æ—Ç—è –±—ã –Ω–∞ 3 –∏–∑ 4 –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π/—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ —ç–∫–∑–∞–º–µ–Ω –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –¥–∞–ª—å—à–µ, –∏ –≤—ã –ø–æ–ª—É—á–∞–µ—Ç–µ x ‚àí 2 –±–∞–ª–ª–æ–≤, –≥–¥–µ x ‚Äì —á–∏—Å–ª–æ –≤–µ—Ä–Ω–æ –æ—Ç–≤–µ—á–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤. –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ –∑–∞ —ç–∫–∑–∞–º–µ–Ω –≤—ã—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è 0 –±–∞–ª–ª–æ–≤.\n–¢–µ–æ—Ä–µ–º–∞ —Å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º - 3 –±–∞–ª–ª–∞\n–†–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á - 3 –±–∞–ª–ª–∞\n–ü—Ä–∏ —É—Å–ø–µ—à–Ω–æ–π —Å–¥–∞—á–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –≤–∞–º –≤—ã–¥–∞–µ—Ç—Å—è –±–∏–ª–µ—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ, –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞—á–∏. –ù–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –∫ –æ—Ç–≤–µ—Ç—É –¥–∞–µ—Ç—Å—è 30 –º–∏–Ω—É—Ç. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –±—É–¥–µ—Ç –ø–æ —Ç–µ–æ—Ä–µ–º–∞–º –∏–∑ —Å–ø–∏—Å–∫–∞. –î–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ –∑–∞–¥–∞—á–∞–º —Å–æ–≤–µ—Ç—É–µ–º –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –¥–æ–º–∞—à–Ω–∏–µ –∑–∞–¥–∞–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞—á–∏ —Å —Å–µ–º–∏–Ω–∞—Ä–æ–≤. –í –ø—Ä–æ—Ü–µ—Å—Å–µ –±–µ—Å–µ–¥—ã –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–º –ø—É–Ω–∫—Ç–∞–º —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä –º–æ–∂–µ—Ç –∑–∞–¥–∞–≤–∞—Ç—å —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã.\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å - 2 –±–∞–ª–ª–∞\n–ü–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —ç—Ç–∞–ø—ã —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä –∑–∞–¥–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–¥–∞—á—É –∏–ª–∏ –≤–æ–ø—Ä–æ—Å, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å —Ç–µ–æ—Ä–∏–µ–π. –û—Ç–≤–µ—Ç –Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –≤ 2 –±–∞–ª–ª–∞.\n–í–æ –≤—Ä–µ–º—è —ç–∫–∑–∞–º–µ–Ω–∞ –Ω–µ–ª—å–∑—è –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –Ω–∏–∫–∞–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏.\n–ï—Å–ª–∏ –≤–æ –≤—Ä–µ–º—è —ç–∫–∑–∞–º–µ–Ω–∞ —á–µ–ª–æ–≤–µ–∫ –Ω–µ –º–æ–∂–µ—Ç –Ω–∞–ø–∏—Å–∞—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –ô–µ–Ω—Å–µ–Ω–∞ –¥–ª—è –≤—ã–ø—É–∫–ª–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –∏ —É—Å–ª–æ–≤–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —ç–∫—Å—Ç—Ä–µ–º—É–º–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ –±–µ–∑—É—Å–ª–æ–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∑–∞ –∫—É—Ä—Å —Å—Ç–∞–≤–∏—Ç—Å—è 0."
  },
  {
    "objectID": "index.html#—ç–∫–∑–∞–º–µ–Ω",
    "href": "index.html#—ç–∫–∑–∞–º–µ–Ω",
    "title": "–ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ú–§–¢–ò 2023-2024",
    "section": "",
    "text": "üìú –°–ø–∏—Å–æ–∫ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∏ —Ç–µ–æ—Ä–µ–º\n–û—Ü–µ–Ω–∫–∞ –∑–∞ —ç–∫–∑–∞–º–µ–Ω —Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –∏–∑ 4 —á–∞—Å—Ç–µ–π:\n\n–í–æ–ø—Ä–æ—Å—ã –ø–æ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º - 2 –±–∞–ª–ª–∞\n–°–Ω–∞—á–∞–ª–∞ –≤—ã–¥–∞—é—Ç—Å—è 4 —Å–ª—É—á–∞–π–Ω—ã—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π/—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∏–∑ —Å–ø–∏—Å–∫–∞. –ù–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–µ—Ç—Å—è 10 –º–∏–Ω—É—Ç. –ü—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –æ—Ç–≤–µ—Ç–µ —Ö–æ—Ç—è –±—ã –Ω–∞ 3 –∏–∑ 4 –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π/—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ —ç–∫–∑–∞–º–µ–Ω –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –¥–∞–ª—å—à–µ, –∏ –≤—ã –ø–æ–ª—É—á–∞–µ—Ç–µ x ‚àí 2 –±–∞–ª–ª–æ–≤, –≥–¥–µ x ‚Äì —á–∏—Å–ª–æ –≤–µ—Ä–Ω–æ –æ—Ç–≤–µ—á–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤. –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ –∑–∞ —ç–∫–∑–∞–º–µ–Ω –≤—ã—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è 0 –±–∞–ª–ª–æ–≤.\n–¢–µ–æ—Ä–µ–º–∞ —Å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º - 3 –±–∞–ª–ª–∞\n–†–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á - 3 –±–∞–ª–ª–∞\n–ü—Ä–∏ —É—Å–ø–µ—à–Ω–æ–π —Å–¥–∞—á–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –≤–∞–º –≤—ã–¥–∞–µ—Ç—Å—è –±–∏–ª–µ—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ, –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞—á–∏. –ù–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –∫ –æ—Ç–≤–µ—Ç—É –¥–∞–µ—Ç—Å—è 30 –º–∏–Ω—É—Ç. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –±—É–¥–µ—Ç –ø–æ —Ç–µ–æ—Ä–µ–º–∞–º –∏–∑ —Å–ø–∏—Å–∫–∞. –î–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ –∑–∞–¥–∞—á–∞–º —Å–æ–≤–µ—Ç—É–µ–º –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –¥–æ–º–∞—à–Ω–∏–µ –∑–∞–¥–∞–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞—á–∏ —Å —Å–µ–º–∏–Ω–∞—Ä–æ–≤. –í –ø—Ä–æ—Ü–µ—Å—Å–µ –±–µ—Å–µ–¥—ã –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–º –ø—É–Ω–∫—Ç–∞–º —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä –º–æ–∂–µ—Ç –∑–∞–¥–∞–≤–∞—Ç—å —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã.\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å - 2 –±–∞–ª–ª–∞\n–ü–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —ç—Ç–∞–ø—ã —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä –∑–∞–¥–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–¥–∞—á—É –∏–ª–∏ –≤–æ–ø—Ä–æ—Å, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å —Ç–µ–æ—Ä–∏–µ–π. –û—Ç–≤–µ—Ç –Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –≤ 2 –±–∞–ª–ª–∞.\n–í–æ –≤—Ä–µ–º—è —ç–∫–∑–∞–º–µ–Ω–∞ –Ω–µ–ª—å–∑—è –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –Ω–∏–∫–∞–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏.\n–ï—Å–ª–∏ –≤–æ –≤—Ä–µ–º—è —ç–∫–∑–∞–º–µ–Ω–∞ —á–µ–ª–æ–≤–µ–∫ –Ω–µ –º–æ–∂–µ—Ç –Ω–∞–ø–∏—Å–∞—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –ô–µ–Ω—Å–µ–Ω–∞ –¥–ª—è –≤—ã–ø—É–∫–ª–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –∏ —É—Å–ª–æ–≤–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —ç–∫—Å—Ç—Ä–µ–º—É–º–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ –±–µ–∑—É—Å–ª–æ–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∑–∞ –∫—É—Ä—Å —Å—Ç–∞–≤–∏—Ç—Å—è 0."
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "",
    "section": "",
    "text": "–ó–∞–Ω—è—Ç–∏–µ 1\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –í—Å–ø–æ–º–∏–Ω–∞–µ–º –±–∞–∑–æ–≤—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä—ã. –í–µ–∫—Ç–æ—Ä—ã, –º–∞—Ç—Ä–∏—Ü—ã, –Ω–æ—Ä–º—ã, —Å–∫–∞–ª—è—Ä–Ω—ã–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è\n\n    –ó–∞–Ω—è—Ç–∏–µ 2.1\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –°–ø–µ–∫—Ç—Ä –º–∞—Ç—Ä–∏—Ü—ã. SVD. Skeleton. –ì—Ä–∞–¥–∏–µ–Ω—Ç. –ì–µ—Å—Å–∏–∞–Ω. –ú–∞—Ç—Ä–∏—á–Ω–æ-–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ.\n\n    –ó–∞–Ω—è—Ç–∏–µ 2.2\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ. Forward\\Reverse Mode. –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ.\n\n    –ó–∞–Ω—è—Ç–∏–µ 3\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –í—ã–ø—É–∫–ª–æ—Å—Ç—å. –í—ã–ø—É–∫–ª—ã–µ, –∞—Ñ–∏–Ω–Ω—ã–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞. –í—ã–ø—É–∫–ª—ã–µ –∫–æ–Ω—É—Å—ã. –°—É–º–º–∞ –ú–∏–Ω–∫–æ–≤—Å–∫–æ–≥–æ\n\n    –ó–∞–Ω—è—Ç–∏–µ 4.1\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –í—ã–ø—É–∫–ª—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏. –ù–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –ô–µ–Ω—Å–µ–Ω–∞. –°–∏–ª—å–Ω–æ –≤—ã–ø—É–ª—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏. –£—Å–ª–æ–≤–∏–µ –ü–æ–ª—è–∫–∞ - –õ–æ—è—Å–∏–µ–≤–∏—á–∞\n\n    –ó–∞–Ω—è—Ç–∏–µ 4.2\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –°–æ–ø—Ä—è–∂–µ–Ω–Ω—ã–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞. –°–æ–ø—Ä—è–∂–µ–Ω–Ω—ã–µ –∫–æ–Ω—É—Å—ã. –ú–Ω–æ–≥–æ–≥—Ä–∞–Ω–Ω–∏–∫–∏\n\n    –ó–∞–Ω—è—Ç–∏–µ 5\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –°–æ–ø—Ä—è–∂–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –õ–µ–∂–∞–Ω–¥—Ä–∞. –°–æ–ø—Ä—è–∂–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞\n\n    –ó–∞–Ω—è—Ç–∏–µ 6\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –°—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç. –°—É–±–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª. –¢–µ–æ—Ä–µ–º—ã –ú–æ—Ä–æ-–†–æ–∫–∞—Ñ–µ–ª–ª–∞—Ä–∞, –î—É–±–æ–≤–∏—Ü–∫–æ–≥–æ-–ú–∏–ª—é—Ç–∏–Ω–∞\n\n    –ó–∞–Ω—è—Ç–∏–µ 7\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –£—Å–ª–æ–≤–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏. –§—É–Ω–∫—Ü–∏—è –õ–∞–≥—Ä–∞–Ω–∂–∞. –ú–Ω–æ–∂–∏—Ç–µ–ª–∏ –õ–∞–≥—Ä–∞–Ω–∂–∞. –¢–µ–æ—Ä–µ–º–∞ –ö–∞—Ä—É—à–∞ - –ö—É–Ω–∞ - –¢–∞–∫–∫–µ—Ä–∞\n\n    –ó–∞–Ω—è—Ç–∏–µ 8\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –î–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å. –í–≤–µ–¥–µ–Ω–∏–µ –≤ –¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å. –î–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞. Two-way partitioning problem. –ü—Ä–æ–µ–∫—Ü–∏—è —Ç–æ—á–∫–∏ –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π —Å–∏–º–ø–ª–µ–∫—Å\n\n    –ó–∞–Ω—è—Ç–∏–µ 9\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –î–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å. –ê–Ω–∞–ª–∏–∑ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –¢–µ–Ω–µ–≤—ã–µ —Ü–µ–Ω—ã. –ú–∞—Ç—Ä–∏—á–Ω—ã–µ –∏–≥—Ä—ã —Å–æ —Å–º–µ—à–∞–Ω–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏.\n\n    –ó–∞–Ω—è—Ç–∏–µ 10\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏\n    \n    –õ–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ. –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∞—è –∑–∞–¥–∞—á–∞ –∏ –¥—Ä—É–≥–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö –∑–∞–¥–∞—á –∫–∞–∫ –õ–ü. –°–∏–º–ø–ª–µ–∫—Å –º–µ—Ç–æ–¥ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –õ–ü.\n\n    –ó–∞–Ω—è—Ç–∏–µ 11\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏\n    \n    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –°–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏. –õ–∏–Ω–µ–π–Ω—ã–π –ø–æ–∏—Å–∫. –ù–µ—Ç–æ—á–Ω–∞—è –æ–¥–Ω–æ–º–µ—Ä–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è. –ü—Ä–∞–≤–∏–ª–∞ –ê—Ä–º–∏—Ö–æ  - –ì–æ–ª—å–¥—à—Ç–µ–π–Ω–∞. –£—Å–ª–æ–≤–∏–µ –í—É–ª—å—Ñ–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 12\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã\n    \n    –ú–µ—Ç–æ–¥—ã –Ω—É–ª–µ–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞. –ë–µ–∑–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ì–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º. –≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã.\n\n    –ó–∞–Ω—è—Ç–∏–µ 13\n    \n        \n    \n    –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫. –¢–µ–æ—Ä–µ–º—ã —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –≥–ª–∞–¥–∫–æ–º —Å–ª—É—á–∞–µ (–≤—ã–ø—É–∫–ª—ã–µ, —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã–µ, PL).\n\n    –ó–∞–Ω—è—Ç–∏–µ 14\n    \n        \n    \n    –°—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫. –¢–µ–æ—Ä–µ–º—ã —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –Ω–µ–≥–ª–∞–¥–∫–æ–º —Å–ª—É—á–∞–µ (–≤—ã–ø—É–∫–ª—ã–π —Å–ª—É—á–∞–π). –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –≤ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–µ–≥–ª–∞–¥–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ó–∞–¥–∞—á–∞ –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ —Å $l_1$ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π.\n\n    –ó–∞–Ω—è—Ç–∏–µ 15\n    \n        \n    \n    –£—Å–∫–æ—Ä—è–µ–º –º–µ—Ç–æ–¥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ - NAG, Momentum, –ø–æ–ª–∏–Ω–æ–º—ã.\n\n    –ó–∞–Ω—è—Ç–∏–µ 16\n    \n        \n    \n    –ú–µ—Ç–æ–¥ –ø—Ä–æ–µ–∫—Ü–∏–∏ —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–∞. –ú–µ—Ç–æ–¥ —É—Å–ª–æ–≤–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ (–º–µ—Ç–æ–¥ –§—Ä–∞–Ω–∫–∞ - –í—É–ª—å—Ñ–∞). –ò–¥–µ—è –º–µ—Ç–æ–¥–∞ –∑–µ—Ä–∫–∞–ª—å–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 17\n    \n        \n    \n    –ü—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã. –ë–∞—Ä—å–µ—Ä–Ω—ã–µ –º–µ—Ç–æ–¥—ã.\n\n    –ó–∞–Ω—è—Ç–∏–µ 18\n    \n        \n    \n    –ú–µ—Ç–æ–¥ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π. –û—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ì—Ä–∞–º–º–∞ - –®–º–∏–¥—Ç–∞. –ü–æ–Ω—è—Ç–∏–µ $A$-–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤. –ú–µ—Ç–æ–¥ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.\n\n    –ó–∞–Ω—è—Ç–∏–µ 19\n    \n        \n    \n    –ö–æ–Ω—Ü–µ–ø—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏. –ú–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞. –ö–≤–∞–∑–∏–Ω—å—é—Ç–æ–Ω–æ–≤—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã. –ú–µ—Ç–æ–¥ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞. KFAC.\n\n    –ó–∞–Ω—è—Ç–∏–µ 20\n    \n        \n    \n    –í–≤–µ–¥–µ–Ω–∏–µ –≤ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã. –ë–∞—Ç—á, —ç–ø–æ—Ö–∞. –°—Ö–æ–¥–∏–º–æ—Å—Ç—å SGD.\n\n    –ó–∞–Ω—è—Ç–∏–µ 21\n    \n        \n    \n    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã. –ú–µ—Ç–æ–¥—ã —Ä–µ–¥—É–∫—Ü–∏–∏ –¥–∏—Å–ø–µ—Ä—Å–∏–∏.\n\n    –ó–∞–Ω—è—Ç–∏–µ 22\n    \n        \n    \n    –û–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π. –ü–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è.\n\n    –ó–∞–Ω—è—Ç–∏–µ 23\n    \n        \n    \n    –§–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è.\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "",
    "section": "",
    "text": "üóì 29 –Ω–æ—è–±—Ä—è. ü¶Ñ 2 –±–∞–ª–ª–∞\n–ö —ç—Ç–æ–º—É –º–æ–º–µ–Ω—Ç—É –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ª–∏—á–Ω–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å —Å –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–µ–º —Ç–µ–º—É –ø—Ä–æ–µ–∫—Ç–∞ –∏ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞–±–æ—Ç—ã –Ω–∞–¥ –Ω–∏–º. –®–∞–±–ª–æ–Ω \\LaTeX –¥–ª—è —Ä–∞–±–æ—Ç—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∑–¥–µ—Å—å, –≤—ã–ø–æ–ª–Ω—è—Ç—å —ç—Ç–∞–ø—ã –ø—Ä–æ–µ–∫—Ç–∞ –Ω–∞–¥–æ –≤ –Ω—ë–º. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å VSCode —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º LaTeX Workshop. –¢–∞–∫ –∂–µ –¥–æ—Å—Ç—É–ø–Ω–∞ –≤–µ—Ä—Å–∏—è –≤ overleaf. –î–æ–ø—É—Å–∫–∞–µ—Ç—Å—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –≤ Typst, –µ—Å–ª–∏ –≤—ã —É–º–µ–µ—Ç–µ, –Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–∏–∂–µ –±—É–¥—É—Ç –ø—Ä–µ–¥—ä—è–≤–ª—è—Ç—å—Å—è —Ç–æ—á–Ω–æ —Ç–∞–∫–∏–µ –∂–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ). –í –¥–∞–ª—å–Ω–µ–π—à–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω, –¥–æ–ø–æ–ª–Ω—è—è –µ–≥–æ –ø–æ –º–µ—Ä–µ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –≤ –ø—Ä–æ–µ–∫—Ç–µ. –í—Å–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ —Å—Å—ã–ª–∫–∏ –ø–æ —Ç–µ–º–µ —É–¥–æ–±–Ω–æ —Å–æ–±–∏—Ä–∞—Ç—å –≤ notion, —á—Ç–æ–±—ã –ø—Ä–∏ –Ω–∞—à–µ–º –æ–±—Å—É–∂–¥–µ–Ω–∏–∏ –æ–Ω–∏ –±—ã–ª–∏ –ø–æ–¥ —Ä—É–∫–æ–π.\n\n\n–ì–ª–∞–≤–Ω–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∫ —Ç–µ–º–µ –ø—Ä–æ–µ–∫—Ç–∞ - –≤–∞–º –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø—Ä–∏–∫–æ–ª—å–Ω–æ –µ–≥–æ –¥–µ–ª–∞—Ç—å, —Ç–µ–º–∞ –¥–æ–ª–∂–Ω–∞ –≤–∞—Å –∂–∏–≤–æ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å. –í—Ç–æ—Ä–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ - –æ–Ω –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–≤—è–∑–∞–Ω —Å –º–µ—Ç–æ–¥–∞–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (—Ö–æ—Ç—è –±—ã –∫–∞–∫-—Ç–æ üôÇ). –¢–µ–º—É –ø—Ä–æ–µ–∫—Ç–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–∏–¥—É–º–∞—Ç—å/ –Ω–∞–π—Ç–∏/ –≤—ã–±—Ä–∞—Ç—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ.\n\n–í–∑—è—Ç—å –Ω–µ–¥–∞–≤–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é –Ω–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ NeurIPS, ICML, ICLR. –î–µ—Ç–∞–ª—å–Ω–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è. –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞ –¥—Ä—É–≥–∏—Ö –¥–∞–Ω–Ω—ã—Ö/–º–æ–¥–µ–ª—è—Ö/–º–µ—Ç–æ–¥–∞—Ö. –í–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å/–ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —á—Ç–æ-–Ω–∏–±—É–¥—å –Ω–æ–≤–æ–µ.\n–ò–∑—É—á–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏\n\n–°—Ç–∞—Ç—å—è\n–°—Ç–∞—Ç—å—è\n\n–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∫–∞–∫ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—é –≠–π–ª–µ—Ä–∞ –æ–±—ã–∫–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —É—Ä–∞–≤–Ω–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞. –û–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è, —É—Å–∫–æ—Ä–µ–Ω–Ω—ã–º –º–µ—Ç–æ–¥–∞–º —Ç–æ–∂–µ –º–æ–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏—Ö –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏. –í —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏–∑—É—á–∏—Ç—å –≥–¥–µ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω—ã –º–æ–≥—É—Ç –±—ã—Ç—å —Ç–∞–∫–∏–µ –∞–Ω–∞–ª–æ–≥–∏–∏, –¥–æ–∫–∞–∑–∞—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —É—Å–∫–æ—Ä–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –≤ –≤—ã–ø—É–∫–ª–æ–º —Å–ª—É—á–∞–µ, –∏–∑—É—á–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è —Å–ª—É—á–∞—è –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏.\n–†–µ–±—è—Ç–∞–º, –∫–æ—Ç–æ—Ä—ã–µ —Å–µ—Ä—å–µ–∑–Ω–æ —Ö–æ—Ç—è—Ç –ø–æ–≥—Ä—É–∑–∏—Ç—å—Å—è –≤ —Ç–µ–º—É –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –æ—Ü–µ–Ω–æ–∫, —Ç–µ–æ—Ä–µ–º —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏ –¥—Ä—É–≥–∏—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –Ω–∞–ø–∏—Å–∞—Ç—å –ª–µ–∫—Ç–æ—Ä—É (–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á –ì–∞—Å–Ω–∏–∫–æ–≤, —Ç–∞–∫ –∂–µ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –µ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç –≤ –≥—Ä—É–ø–ø–µ –∫—É—Ä—Å–∞ –≤ —Ç–µ–ª–µ–≥—Ä–∞–º) —Å –≤–æ–ø—Ä–æ—Å–æ–º, –æ–ø–∏—Å–∞–Ω–∏–µ–º —Å–≤–æ–∏—Ö –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤.\n–û–±—É—á–µ–Ω–∏–µ Reinforcement Learning –∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –±–µ–∑–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤. –°—Å—ã–ª–∫–∏ –¥–ª—è –≤–¥–æ—Ö–Ω–æ–≤–µ–Ω–∏—è [1], [2], [3]\n–ò–¥–µ–∏ –ø—Ä–æ–µ–∫—Ç–æ–≤ –¥–ª—è –∫—É—Ä—Å–∞ –ø–æ –º–µ—Ç–æ–¥–∞–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ú–ì–£ (–°–∞—Ä–æ–≤—Å–∫–∏–π —Ñ–∏–ª–∏–∞–ª), –∫–æ—Ç–æ—Ä—ã–π –º—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏ –≤ –∫–æ–Ω—Ü–µ –ø—Ä–æ—à–ª–æ–≥–æ –≥–æ–¥–∞.\n–ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∏–≥—Ä.\n–°–æ–∑–¥–∞–Ω–∏–µ –ø–∏—Ç–æ–Ω –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ - —á–µ—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞ –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –∑–∞–ø—É—Å–∫–∞–º–∏, –µ–¥–∏–Ω—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ–º –≥—Ä–∞—Ñ–∏–∫–æ–≤.\n–ü–æ–∏–≥—Ä–∞—Ç—å—Å—è —Å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏\n–°—Å—ã–ª–∫–∞.\n–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–µ–≥–æ–¥–Ω—è —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –¥–ª—è –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (—á–∞—â–µ –≤—Å–µ–≥–æ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π). –í —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞–∫—É—é-–Ω–∏–±—É–¥—å –ø—Ä–æ—Å—Ç—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –≤ —Å–ø–µ–π–∏—Ñ–∏–∫–µ –æ–±—É—á–µ–Ω–∏—è —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª–µ–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –ø—Ä–æ–¥–µ–ª–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–≤–æ–∏—Ö —á–∏—Å–ª–µ–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.\n–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω–æ–≥–æ –ø–æ—Ä—Ç—Ñ–µ–ª—è\n–í —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏–∑—É—á–∏—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—å–Ω–æ–≥–æ –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ –Ω–∏—Ö –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö –Ω–∞ —Ä—ã–Ω–∫–µ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ–≥—Ä–∞–º–º—ã –Ω–∞ —è–∑—ã–∫–µ Python. –ù—É–∂–Ω–æ –±—É–¥–µ—Ç —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É, –≥—Ä–∞–Ω–∏—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –∏ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è. –ú–æ–∂–Ω–æ –Ω–∞—á–∞—Ç—å —Å –ø–æ—Ä—Ç—Ñ–µ–ª—å–Ω–æ–π —Ç–µ–æ—Ä–∏–∏ –ú–∞—Ä–∫–æ–≤–∏—Ü–∞ –∏ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –º–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ–¥–∞–≤–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ NLP –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —ç—Ç–æ —É–ª—É—á—à–∏—Ç—å.\n–ï—â–µ –∏–¥–µ–∏ –ø—Ä–æ–µ–∫—Ç–æ–≤\n–í –∫–∞—á–µ—Å—Ç–≤–µ –≤–¥–æ—Ö–Ω–æ–≤–µ–Ω–∏—è –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –ª—É—á—à–∏–µ –ø—Ä–æ–µ–∫—Ç—ã —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –ø—Ä–æ—à–ª—ã—Ö –ª–µ—Ç:\n\n–õ—É—á—à–∏–µ –ø—Ä–æ–µ–∫—Ç—ã –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 2018\n–õ—É—á—à–∏–µ –ø—Ä–æ–µ–∫—Ç—ã –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 2019\n–õ—É—á—à–∏–µ –ø—Ä–æ–µ–∫—Ç—ã –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 2020\n\n\n\n\n\n–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ —Å–≤–æ–π notion –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –ø–æ–ª–µ pdf —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ç–µ–º—ã –∏ –∞–±—Å—Ç—Ä–∞–∫—Ç–æ–º. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –≤—ã —É–¥–∞–ª–∏–ª–∏ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—É–Ω–∫—Ç—ã –∏–∑ —à–∞–±–ª–æ–Ω–∞ –≤—ã—à–µ. –ü–¥—Ñ–∫–∞ –¥–æ–ª–∂–Ω–∞ –≤—ã–≥–ª—è–¥–µ—Ç—å –∫–∞–∫-—Ç–æ —Ç–∞–∫:\n\n\n\nExample of this stage of the project\n\n\n\n\n\n\n0 –±–∞–ª–ª–æ–≤ - —Ñ–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω –≤–æ–≤—Ä–µ–º—è/ —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤ –¥—Ä—É–≥–æ–º —Ñ–æ—Ä–º–∞—Ç–µ\n1 –±–∞–ª–ª - —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤–æ–≤—Ä–µ–º—è, —Ç–µ–º–∞ –Ω–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∞\n2 –±–∞–ª–ª–∞ - —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤–æ–≤—Ä–µ–º—è, —Ç–µ–º–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∞"
  },
  {
    "objectID": "projects.html#–≤—ã–±–æ—Ä-—Ç–µ–º—ã-–ø—Ä–æ–µ–∫—Ç–∞",
    "href": "projects.html#–≤—ã–±–æ—Ä-—Ç–µ–º—ã-–ø—Ä–æ–µ–∫—Ç–∞",
    "title": "",
    "section": "",
    "text": "üóì 29 –Ω–æ—è–±—Ä—è. ü¶Ñ 2 –±–∞–ª–ª–∞\n–ö —ç—Ç–æ–º—É –º–æ–º–µ–Ω—Ç—É –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ª–∏—á–Ω–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å —Å –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–µ–º —Ç–µ–º—É –ø—Ä–æ–µ–∫—Ç–∞ –∏ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞–±–æ—Ç—ã –Ω–∞–¥ –Ω–∏–º. –®–∞–±–ª–æ–Ω \\LaTeX –¥–ª—è —Ä–∞–±–æ—Ç—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∑–¥–µ—Å—å, –≤—ã–ø–æ–ª–Ω—è—Ç—å —ç—Ç–∞–ø—ã –ø—Ä–æ–µ–∫—Ç–∞ –Ω–∞–¥–æ –≤ –Ω—ë–º. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å VSCode —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º LaTeX Workshop. –¢–∞–∫ –∂–µ –¥–æ—Å—Ç—É–ø–Ω–∞ –≤–µ—Ä—Å–∏—è –≤ overleaf. –î–æ–ø—É—Å–∫–∞–µ—Ç—Å—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –≤ Typst, –µ—Å–ª–∏ –≤—ã —É–º–µ–µ—Ç–µ, –Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–∏–∂–µ –±—É–¥—É—Ç –ø—Ä–µ–¥—ä—è–≤–ª—è—Ç—å—Å—è —Ç–æ—á–Ω–æ —Ç–∞–∫–∏–µ –∂–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ). –í –¥–∞–ª—å–Ω–µ–π—à–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω, –¥–æ–ø–æ–ª–Ω—è—è –µ–≥–æ –ø–æ –º–µ—Ä–µ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –≤ –ø—Ä–æ–µ–∫—Ç–µ. –í—Å–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ —Å—Å—ã–ª–∫–∏ –ø–æ —Ç–µ–º–µ —É–¥–æ–±–Ω–æ —Å–æ–±–∏—Ä–∞—Ç—å –≤ notion, —á—Ç–æ–±—ã –ø—Ä–∏ –Ω–∞—à–µ–º –æ–±—Å—É–∂–¥–µ–Ω–∏–∏ –æ–Ω–∏ –±—ã–ª–∏ –ø–æ–¥ —Ä—É–∫–æ–π.\n\n\n–ì–ª–∞–≤–Ω–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∫ —Ç–µ–º–µ –ø—Ä–æ–µ–∫—Ç–∞ - –≤–∞–º –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø—Ä–∏–∫–æ–ª—å–Ω–æ –µ–≥–æ –¥–µ–ª–∞—Ç—å, —Ç–µ–º–∞ –¥–æ–ª–∂–Ω–∞ –≤–∞—Å –∂–∏–≤–æ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å. –í—Ç–æ—Ä–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ - –æ–Ω –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–≤—è–∑–∞–Ω —Å –º–µ—Ç–æ–¥–∞–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (—Ö–æ—Ç—è –±—ã –∫–∞–∫-—Ç–æ üôÇ). –¢–µ–º—É –ø—Ä–æ–µ–∫—Ç–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–∏–¥—É–º–∞—Ç—å/ –Ω–∞–π—Ç–∏/ –≤—ã–±—Ä–∞—Ç—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ.\n\n–í–∑—è—Ç—å –Ω–µ–¥–∞–≤–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç—å—é –Ω–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ NeurIPS, ICML, ICLR. –î–µ—Ç–∞–ª—å–Ω–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è. –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞ –¥—Ä—É–≥–∏—Ö –¥–∞–Ω–Ω—ã—Ö/–º–æ–¥–µ–ª—è—Ö/–º–µ—Ç–æ–¥–∞—Ö. –í–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å/–ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —á—Ç–æ-–Ω–∏–±—É–¥—å –Ω–æ–≤–æ–µ.\n–ò–∑—É—á–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏\n\n–°—Ç–∞—Ç—å—è\n–°—Ç–∞—Ç—å—è\n\n–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∫–∞–∫ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—é –≠–π–ª–µ—Ä–∞ –æ–±—ã–∫–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —É—Ä–∞–≤–Ω–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞. –û–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è, —É—Å–∫–æ—Ä–µ–Ω–Ω—ã–º –º–µ—Ç–æ–¥–∞–º —Ç–æ–∂–µ –º–æ–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏—Ö –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏. –í —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏–∑—É—á–∏—Ç—å –≥–¥–µ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω—ã –º–æ–≥—É—Ç –±—ã—Ç—å —Ç–∞–∫–∏–µ –∞–Ω–∞–ª–æ–≥–∏–∏, –¥–æ–∫–∞–∑–∞—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —É—Å–∫–æ—Ä–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –≤ –≤—ã–ø—É–∫–ª–æ–º —Å–ª—É—á–∞–µ, –∏–∑—É—á–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è —Å–ª—É—á–∞—è –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏.\n–†–µ–±—è—Ç–∞–º, –∫–æ—Ç–æ—Ä—ã–µ —Å–µ—Ä—å–µ–∑–Ω–æ —Ö–æ—Ç—è—Ç –ø–æ–≥—Ä—É–∑–∏—Ç—å—Å—è –≤ —Ç–µ–º—É –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –æ—Ü–µ–Ω–æ–∫, —Ç–µ–æ—Ä–µ–º —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏ –¥—Ä—É–≥–∏—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –Ω–∞–ø–∏—Å–∞—Ç—å –ª–µ–∫—Ç–æ—Ä—É (–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á –ì–∞—Å–Ω–∏–∫–æ–≤, —Ç–∞–∫ –∂–µ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –µ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç –≤ –≥—Ä—É–ø–ø–µ –∫—É—Ä—Å–∞ –≤ —Ç–µ–ª–µ–≥—Ä–∞–º) —Å –≤–æ–ø—Ä–æ—Å–æ–º, –æ–ø–∏—Å–∞–Ω–∏–µ–º —Å–≤–æ–∏—Ö –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤.\n–û–±—É—á–µ–Ω–∏–µ Reinforcement Learning –∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –±–µ–∑–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤. –°—Å—ã–ª–∫–∏ –¥–ª—è –≤–¥–æ—Ö–Ω–æ–≤–µ–Ω–∏—è [1], [2], [3]\n–ò–¥–µ–∏ –ø—Ä–æ–µ–∫—Ç–æ–≤ –¥–ª—è –∫—É—Ä—Å–∞ –ø–æ –º–µ—Ç–æ–¥–∞–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ú–ì–£ (–°–∞—Ä–æ–≤—Å–∫–∏–π —Ñ–∏–ª–∏–∞–ª), –∫–æ—Ç–æ—Ä—ã–π –º—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏ –≤ –∫–æ–Ω—Ü–µ –ø—Ä–æ—à–ª–æ–≥–æ –≥–æ–¥–∞.\n–ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∏–≥—Ä.\n–°–æ–∑–¥–∞–Ω–∏–µ –ø–∏—Ç–æ–Ω –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ - —á–µ—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞ –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –∑–∞–ø—É—Å–∫–∞–º–∏, –µ–¥–∏–Ω—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ–º –≥—Ä–∞—Ñ–∏–∫–æ–≤.\n–ü–æ–∏–≥—Ä–∞—Ç—å—Å—è —Å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏\n–°—Å—ã–ª–∫–∞.\n–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–µ–≥–æ–¥–Ω—è —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –¥–ª—è –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (—á–∞—â–µ –≤—Å–µ–≥–æ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π). –í —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞–∫—É—é-–Ω–∏–±—É–¥—å –ø—Ä–æ—Å—Ç—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –≤ —Å–ø–µ–π–∏—Ñ–∏–∫–µ –æ–±—É—á–µ–Ω–∏—è —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª–µ–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –ø—Ä–æ–¥–µ–ª–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–≤–æ–∏—Ö —á–∏—Å–ª–µ–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.\n–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω–æ–≥–æ –ø–æ—Ä—Ç—Ñ–µ–ª—è\n–í —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏–∑—É—á–∏—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—å–Ω–æ–≥–æ –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ –Ω–∏—Ö –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö –Ω–∞ —Ä—ã–Ω–∫–µ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ–≥—Ä–∞–º–º—ã –Ω–∞ —è–∑—ã–∫–µ Python. –ù—É–∂–Ω–æ –±—É–¥–µ—Ç —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É, –≥—Ä–∞–Ω–∏—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –∏ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è. –ú–æ–∂–Ω–æ –Ω–∞—á–∞—Ç—å —Å –ø–æ—Ä—Ç—Ñ–µ–ª—å–Ω–æ–π —Ç–µ–æ—Ä–∏–∏ –ú–∞—Ä–∫–æ–≤–∏—Ü–∞ –∏ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –º–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ–¥–∞–≤–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ NLP –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —ç—Ç–æ —É–ª—É—á—à–∏—Ç—å.\n–ï—â–µ –∏–¥–µ–∏ –ø—Ä–æ–µ–∫—Ç–æ–≤\n–í –∫–∞—á–µ—Å—Ç–≤–µ –≤–¥–æ—Ö–Ω–æ–≤–µ–Ω–∏—è –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –ª—É—á—à–∏–µ –ø—Ä–æ–µ–∫—Ç—ã —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –ø—Ä–æ—à–ª—ã—Ö –ª–µ—Ç:\n\n–õ—É—á—à–∏–µ –ø—Ä–æ–µ–∫—Ç—ã –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 2018\n–õ—É—á—à–∏–µ –ø—Ä–æ–µ–∫—Ç—ã –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 2019\n–õ—É—á—à–∏–µ –ø—Ä–æ–µ–∫—Ç—ã –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 2020\n\n\n\n\n\n–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ —Å–≤–æ–π notion –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –ø–æ–ª–µ pdf —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ç–µ–º—ã –∏ –∞–±—Å—Ç—Ä–∞–∫—Ç–æ–º. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –≤—ã —É–¥–∞–ª–∏–ª–∏ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—É–Ω–∫—Ç—ã –∏–∑ —à–∞–±–ª–æ–Ω–∞ –≤—ã—à–µ. –ü–¥—Ñ–∫–∞ –¥–æ–ª–∂–Ω–∞ –≤—ã–≥–ª—è–¥–µ—Ç—å –∫–∞–∫-—Ç–æ —Ç–∞–∫:\n\n\n\nExample of this stage of the project\n\n\n\n\n\n\n0 –±–∞–ª–ª–æ–≤ - —Ñ–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω –≤–æ–≤—Ä–µ–º—è/ —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤ –¥—Ä—É–≥–æ–º —Ñ–æ—Ä–º–∞—Ç–µ\n1 –±–∞–ª–ª - —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤–æ–≤—Ä–µ–º—è, —Ç–µ–º–∞ –Ω–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∞\n2 –±–∞–ª–ª–∞ - —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤–æ–≤—Ä–µ–º—è, —Ç–µ–º–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∞"
  },
  {
    "objectID": "projects.html#–æ–ø–∏—Å–∞–Ω–∏–µ-—Ä–µ—à–∞–µ–º–æ–π-–∑–∞–¥–∞—á–∏",
    "href": "projects.html#–æ–ø–∏—Å–∞–Ω–∏–µ-—Ä–µ—à–∞–µ–º–æ–π-–∑–∞–¥–∞—á–∏",
    "title": "",
    "section": "–û–ø–∏—Å–∞–Ω–∏–µ —Ä–µ—à–∞–µ–º–æ–π –∑–∞–¥–∞—á–∏",
    "text": "–û–ø–∏—Å–∞–Ω–∏–µ —Ä–µ—à–∞–µ–º–æ–π –∑–∞–¥–∞—á–∏\nüóì 2 –¥–µ–∫–∞–±—Ä—è. ü¶Ñ 2 –±–∞–ª–ª–∞\n–ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —á–µ—Ç–∫–æ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–µ—à–∞–µ–º—É—é –∑–∞–¥–∞—á—É. –° –±–æ–ª—å—à–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é, –≤ —ç—Ç–æ–º –ø—É–Ω–∫—Ç–µ –Ω—É–∂–Ω–æ –Ω–∞–ø–∏—Å–∞—Ç—å –∑–∞–¥–∞—á—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ –¥—Ä—É–≥—É—é —Ä–µ—à–∞–µ–º—É—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∑–∞–¥–∞—á—É.\n–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –µ—Å–ª–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–æ–µ–∫—Ç–∞ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å—Ç–∞—Ç—å–µ–π –≤—ã —Å–Ω–∞—á–∞–ª–∞ –¥–æ–ª–∂–Ω—ã –Ω–∞–ø–∏—Å–∞—Ç—å —Å–≤–æ—é –∑–∞–¥–∞—á—É –≤ —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞ - —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –≤ —á—ë–º-—Ç–æ, –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏, –ø–æ–≤—Ç–æ—Ä–∏—Ç—å —á–∏—Å–ª–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø—Ä–∏–¥—É–º–∞—Ç—å –¥—Ä—É–≥–∏–µ —á–∏—Å–ª–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ê –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Ç–∞–∫ –∂–µ –ø—Ä–∏–≤–µ—Å—Ç–∏ –≤ –Ω–∞–∏–±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–º –≤–∏–¥–µ —Ä–µ—à–∞–µ–º—É—é –∑–∞–¥–∞—á—É –∏–∑ —Å—Ç–∞—Ç—å–∏. –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –∏ –¥–∞–ª–µ–µ –≤–∞–∂–Ω–æ –Ω–µ –ø—Ä–∏—Å–≤–∞–∏–≤–∞—Ç—å —Å–µ–±–µ —á—É–∂–∏–µ –∑–∞—Å–ª—É–≥–∏. –ü–æ—Å—Ç–∞—Ä–∞–π—Ç–µ—Å—å –∏–∑–±–µ–≥–∞—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –≤–∏–¥–∞: –º—ã —Ä–µ—à–∞–µ–º –∑–∞–¥–∞—á—É (–≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –º–æ–∂–Ω–æ –Ω–∞–ø–∏—Å–∞—Ç—å –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ —Ä–µ—à–∞—é—Ç –∑–∞–¥–∞—á—É), –º—ã —Ö–æ—Ç–∏–º –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—É/ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏/ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å (–≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç/ –∏—Å—Å–ª–µ–¥—É—é—Ç –∏ —Ç.–¥.)\n\n–§–æ—Ä–º–∞—Ç —Å–¥–∞—á–∏\n–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ —Å–≤–æ–π notion –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –ø–æ–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π pdf —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–π –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π –∑–∞–¥–∞—á–∏ —Å —É—á—ë—Ç–æ–º —Ñ–∏–¥–±–µ–∫–∞ –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–º –ø—É–Ω–∫—Ç–∞–º.\n\n\n–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è\n\n0 –±–∞–ª–ª–æ–≤ - —Ñ–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω –≤–æ–≤—Ä–µ–º—è/ —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤ –¥—Ä—É–≥–æ–º —Ñ–æ—Ä–º–∞—Ç–µ\n1 –±–∞–ª–ª - —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤–æ–≤—Ä–µ–º—è, —Ä–µ—à–∞–µ–º–∞—è –∑–∞–¥–∞—á–∞ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∞ –Ω–µ —á—ë—Ç–∫–æ, –Ω–µ –ø–æ–Ω—è—Ç–Ω–æ; —Ä–µ—à–∞–µ–º–∞—è –∑–∞–¥–∞—á–∞ –Ω–µ–∞–¥–µ–∫–≤–∞—Ç–Ω–∞ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–æ–º—É –ø—Ä–æ–µ–∫—Ç—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞–º –Ω—É–∂–Ω–æ 1000 GPU –∏ 355 GPU-years –∏–ª–∏ –≤ —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞ –º—ã —Ö–æ—Ç–∏–º –ø—Ä–∏–¥—É–º–∞—Ç—å –º–µ—Ç–æ–¥ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã–ø—É–∫–ª–æ–π –≥–ª–∞–¥–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ —Å –æ—Ä–∞–∫—É–ª–æ–º 1 –ø–æ—Ä—è–¥–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —Å—Ö–æ–¥–∏—Ç—Å—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é —Ñ—É–Ω–∫—Ü–∏–∏); —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –µ—ë –ø–æ–Ω—è—Ç—å —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤—Ä–µ–º–µ–Ω–∏ —á—Ç–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—å–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –ø–æ —Ç–µ–º–µ; —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏ —Å–ª–∏—à–∫–æ–º –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–∞ –∏–ª–∏ –Ω–∞–∏–≤–Ω–∞.\n2 –±–∞–ª–ª–∞ - —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤–æ–≤—Ä–µ–º—è, —Ä–µ—à–∞–µ–º–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —è—Å–Ω–∞ –∏ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∞ —á—ë—Ç–∫–æ."
  },
  {
    "objectID": "projects.html#–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π-–æ–±–∑–æ—Ä",
    "href": "projects.html#–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π-–æ–±–∑–æ—Ä",
    "title": "",
    "section": "–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π –æ–±–∑–æ—Ä",
    "text": "–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π –æ–±–∑–æ—Ä\nüóì 9 –¥–µ–∫–∞–±—Ä—è. ü¶Ñ 8 –±–∞–ª–ª–æ–≤\n–ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–Ω—è—Ç—å –Ω–∞—É—á–Ω—ã–π –ª–∞–Ω–¥—à–∞—Ñ—Ç –≤–æ–∫—Ä—É–≥ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–¥–∞—á–∏. –î–ª—è —ç—Ç–æ–≥–æ –ø–æ—Å—Ç–∞—Ä–∞–π—Ç–µ—Å—å, —á—Ç–æ–±—ã –ø–æ—Å–ª–µ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ –æ–±–∑–æ—Ä–∞ –±—ã–ª–∏ —è—Å–Ω—ã –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã:\n\n–ö–∞–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ã–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã –≤ –ø–æ—Ö–æ–∂–∏—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞—Ö?\n–ï—Å—Ç—å –ª–∏ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å—Ö–æ–∂–µ–π —Ç–µ–º–∞—Ç–∏–∫–µ?\n–ù–∞ –∫–∞–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ–ø–∏—Ä–∞—Ç—å—Å—è?\n–ö–∞–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –æ–±—É—Å–ª–∞–≤–ª–∏–≤–∞—é—Ç –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º–æ–π –∑–∞–¥–∞—á–∏?\n–° –∫–∞–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è –¥–ª—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∑–∞–¥–∞—á–∏?\n–ï—Å—Ç—å –ª–∏ –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ –∫–æ–¥ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º–æ–π –∑–∞–¥–∞—á–∏?\n\n–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ, –ø–æ–∏—Å–∫ –ø–æ google scholar, –ø–æ–∏—Å–∫ –ø–æ perplexity.ai, –ø–æ–∏—Å–∫ –ø–æ —Å—Å—ã–ª–∫–∞–º –≤ —Å—Ç–∞—Ç—å–µ. –í –∏–¥–µ–∞–ª–µ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ —Ä–µ—Ü–µ–Ω–∑–∏—Ä—É–µ–º—ã–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏¬†–º–æ–Ω–æ–≥—Ä–∞—Ñ–∏–∏. –û–¥–Ω–∞–∫–æ, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏, –º–æ–∂–Ω–æ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ —Å—Ç–∞—Ç—å–∏ –Ω–∞ arxiv, –±–ª–æ–≥–ø–æ—Å—Ç—ã –∏ –¥—Ä—É–≥–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏, —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –¥–ª—è –∑–∞–¥–∞—á–∏. –¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–µ–ª–∞—Ç—å —Å –ø–æ–º–æ—â—å—é bibtex. –ü—Ä–∏–º–µ—Ä –æ—Ç–∫—É–¥–∞ –µ–≥–æ –±—Ä–∞—Ç—å –ø—Ä–∏–≤–µ–¥—ë–Ω –Ω–∏–∂–µ:\n\n\n\n–ó–¥–µ—Å—å –ø–æ–∫–∞–∑–∞–Ω –æ–¥–∏–Ω –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤ –Ω–∞–π—Ç–∏ bibtex –¥–ª—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n\n\n\n–§–æ—Ä–º–∞—Ç —Å–¥–∞—á–∏\n–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ —Å–≤–æ–π notion –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –ø–æ–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π pdf —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–º –æ–±–∑–æ—Ä–æ–º —Å —É—á—ë—Ç–æ–º —Ñ–∏–¥–±–µ–∫–∞ –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–º –ø—É–Ω–∫—Ç–∞–º.\n\n\n–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è\n–ë–∞–ª–ª—ã –±—É–¥—É—Ç —Å–Ω–∏–º–∞—Ç—å—Å—è –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–ª—É—á–∞—è—Ö (—Å–ø–∏—Å–æ–∫ –Ω–µ –ø–æ–ª–Ω—ã–π):\n\n–ú–µ–Ω–µ–µ 7 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ —Ç–µ–º–µ.\n–†–∞–±–æ—Ç–∞ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –±—ã–ª–∞ –ø—Ä–æ–≤–µ–¥–µ–Ω–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ, —Å—Å—ã–ª–∫–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã —Ä–∞–¥–∏ —Å—Å—ã–ª–æ–∫, –∞ –Ω–µ —Ä–∞–¥–∏ —Å—É—Ç–∏.\n–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ –æ–±–∑–æ—Ä–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –Ω–µ –ø–æ–Ω—è—Ç–Ω–æ, –∫–∞–∫–æ–µ –º–µ—Å—Ç–æ –∑–∞–Ω–∏–º–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç –Ω–∞ –Ω–∞—É—á–Ω–æ–º –ª–∞–Ω–¥—à–∞—Ñ—Ç–µ."
  },
  {
    "objectID": "projects.html#project-proposal",
    "href": "projects.html#project-proposal",
    "title": "",
    "section": "Project proposal",
    "text": "Project proposal\nüóì 17 –¥–µ–∫–∞–±—Ä—è. ü¶Ñ 8 –±–∞–ª–ª–æ–≤\n–°–æ–±–∏—Ä–∞–µ–º –≤–æ–µ–¥–∏–Ω–æ –≤—Å—ë, —á—Ç–æ –±—ã–ª–æ —Ä–∞–Ω—å—à–µ, –ø–ª–∞–Ω–∏—Ä—É–µ–º –∏ –ø—Ä–æ–≤–æ–¥–∏–º —Ñ–∞–∑—É –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è. –ù–∞ –º–æ–π –≤–∑–≥–ª—è–¥ - —ç—Ç–æ –≤–∞–∂–Ω–µ–π—à–∏–π —ç—Ç–∞–ø –ø—Ä–æ–µ–∫—Ç–∞. –¢—É—Ç –Ω—É–∂–Ω–æ –æ—á–µ–Ω—å —á–µ—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫—É–¥–∞ –∏ –∫–∞–∫ –¥–≤–∏–≥–∞—Ç—å—Å—è, –∫–∞–∫–∏–µ —Ç—Ä–æ–ø—ã —É–∂–µ –ø—Ä–æ–π–¥–µ–Ω—ã –¥—Ä—É–≥–∏–º–∏ –ª—é–¥—å–º–∏. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ –∞—Å–ø–µ–∫—Ç—ã:\n\n–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞.\nAbstract (–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –≤ –æ–¥–∏–Ω –∞–±–∑–∞—Ü).\n–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ (–æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—Å—Ç—å –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–¥–∞—á–∏ –∏ –µ—ë —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å).\nOutcomes - –æ–ø–∏—à–∏—Ç–µ, —á—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –±—É–¥–µ—Ç –≤—ã—Ö–æ–¥–æ–º –í–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ (–∫–æ–¥, —Ç–µ–æ—Ä–µ–º–∞, —á–∏—Å–ª–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, —Ç–µ–ª–µ–≥—Ä–∞–º –±–æ—Ç, –≤–µ–± —Å–∞–π—Ç, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, —Ä–∞—Å—Å–∫–∞–∑).\n–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π –æ–±–∑–æ—Ä.\n–î–µ—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω —Ä–∞–±–æ—Ç. –Ø—Å–Ω–æ, —á—Ç–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –æ–Ω –±—É–¥–µ—Ç –º–µ–Ω—è—Ç—å—Å—è, –æ–¥–Ω–∞–∫–æ –Ω–∞–ª–∏—á–∏–µ –ø–ª–∞–Ω–∞ –∑–¥–µ—Å—å –ª—É—á—à–µ –µ–≥–æ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è. –ó–¥–µ—Å—å –≤—ã –¥–æ–ª–∂–Ω—ã –Ω–∞–ø–∏—Å–∞—Ç—å —á—Ç–æ –≤—ã –±—É–¥–µ—Ç–µ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –∫ —Å—Ç–∞–¥–∏—è–º –≤ —Å–ª–µ–¥—É—é—â–µ–º —Å–µ–º–µ—Å—Ç—Ä–µ:\n\n–ù–∞—á–∞–ª—å–Ω–∞—è —Ñ–∞–∑–∞ –ø—Ä–æ–µ–∫—Ç–∞\nMid-term project review\n–ü—Ä–µ–¥–∑–∞—â–∏—Ç–∞\n–ó–∞—â–∏—Ç–∞\n\n–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –ü–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, –ø—Ä–∏–≤–µ–¥–∏—Ç–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –∏ –∏–∑–º–µ—Ä—è–µ–º—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –º–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –í–∞—à–µ —Ä–µ—à–µ–Ω–∏–µ¬†–ø—Ä–æ–µ–∫—Ç - —ç—Ç–æ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, —Å–æ—Ü. –æ–ø—Ä–æ—Å, –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –∏ —Ç.–¥. –û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ —ç—Ç–æ–≥–æ –ø—É–Ω–∫—Ç–∞ - –¥–æ–≥–æ–≤–æ—Ä–∏—Ç—å—Å—è –Ω–∞ –±–µ—Ä–µ–≥—É –æ —Ç–æ–º, –∫–∞–∫ –º—ã —Å–º–æ–∂–µ–º –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å —Ä–∞–±–æ—Ç—É, –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—É—é –≤ –ø—Ä–æ–µ–∫—Ç–µ. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–µ–∫—Ç–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å ‚Äú–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º‚Äù –≤ —Ç–æ–º —Å–º—ã—Å–ª–µ, —á—Ç–æ –º—ã —Å–æ–±—Ä–∞–ª–∏—Å—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –∫ –∫–∞–∫–æ–º—É-—Ç–æ –∫–ª–∞—Å—Å—É –∑–∞–¥–∞—á –∏ —É –Ω–∞—Å –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å. –≠—Ç–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, —Ç–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø—Ä–æ—Å—Ç–æ –æ–ø–∏—Å–∞—Ç—å —ç—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å (–º—ã –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª–∏ –∏ –Ω–µ –≤—ã—à–ª–æ, –Ω–æ –∑–∞—Ç–æ –≤–æ—Ç —Ç–∞–∫–æ–µ –≤–æ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ –Ω–∞–±–ª—é–¥–∞–ª–∏).\n–û—Ç—á—ë—Ç –æ —Ñ–∞–∑–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è. –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —à–∏—Ä–æ–∫–∏–º–∏ –º–∞–∑–∫–∞–º–∏ –ø—Ä–∏—Å—Ç—É–ø–∏—Ç—å –∫ —Ä–∞–±–æ—Ç–µ –Ω–∞–¥ –ø—Ä–æ–µ–∫—Ç–æ–º. –ï—Å–ª–∏ –µ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ - –Ω—É–∂–Ω–æ –µ–≥–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å, –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞—à–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –≤—ã —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å. –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –ø—Ä–µ–¥–ø—Ä–∏–Ω—è—Ç—å –ø–µ—Ä–≤—ã–µ —à–∞–≥–∏ –∫ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–µ–∫—Ç–∞. –°–¥–µ–ª–∞—Ç—å —á—Ç–æ-–Ω–∏–±—É–¥—å —Å –Ω–∞—Å–∫–æ–∫–∞. –°–æ–≤—Å–µ–º –∏–¥–µ–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å –∫–∞–∫–æ–π-–Ω–∏–±—É–¥—å –ø—Ä–æ—Ç–æ—Ç–∏–ø (–µ—Å–ª–∏ —ç—Ç–æ –ø—Ä–∏–º–µ–Ω–∏–º–æ –∫ –ø—Ä–æ–µ–∫—Ç—É).\n\n\n–§–æ—Ä–º–∞—Ç —Å–¥–∞—á–∏\n–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ —Å–≤–æ–π notion –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –ø–æ–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π pdf —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–º –æ–±–∑–æ—Ä–æ–º —Å —É—á—ë—Ç–æ–º —Ñ–∏–¥–±–µ–∫–∞ –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–º –ø—É–Ω–∫—Ç–∞–º.\n\n\n–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è\n–ë–∞–ª–ª—ã –±—É–¥—É—Ç —Å–Ω–∏–º–∞—Ç—å—Å—è –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–ª—É—á–∞—è—Ö (—Å–ø–∏—Å–æ–∫ –Ω–µ –ø–æ–ª–Ω—ã–π):\n\n–ù–µ —É—á—Ç–µ–Ω—ã –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏–ø–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–º –ø—É–Ω–∫—Ç–∞–º, –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–º–∏.\n–ü–ª–∞–Ω —Ä–∞–±–æ—Ç –Ω–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π, –æ—á–µ–Ω—å –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–π. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —Ç—è–∂–µ–ª–æ —É–≤–µ—Ä–µ–Ω–Ω–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Ç–≤–æ—Ä—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ (–¥–æ–∫–∞–∑–∞—Ç—å —Ç–µ–æ—Ä–µ–º—É). –ó–¥–µ—Å—å –ª—É—á—à–µ –ø–∏—Å–∞—Ç—å —á—É—Ç—å –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥–æ–∫–∞–∑–∞—Ç—å/ –æ–±–æ–±—â–∏—Ç—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –∏–∑ –¥—Ä—É–≥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞).\n–ù–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞.\n–ù–µ—Ç –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ –æ–±–∑–æ—Ä–∞.\n–ù–µ –Ω–∞–ø–∏—Å–∞–Ω —á—ë—Ç–∫–∏–π –≤—ã—Ö–æ–¥ (outcomes) –ø—Ä–æ–µ–∫—Ç–∞.\n–ù–µ—Ç –æ—Ç—á—ë—Ç–∞ –æ —Ñ–∞–∑–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è.\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ proposal –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞¬†–ø–ª–æ—Ö–æ –ø–æ–¥–ø–∏—Å–∞–Ω—ã."
  },
  {
    "objectID": "projects.html#–Ω–∞—á–∞–ª—å–Ω–∞—è-—Ñ–∞–∑–∞",
    "href": "projects.html#–Ω–∞—á–∞–ª—å–Ω–∞—è-—Ñ–∞–∑–∞",
    "title": "",
    "section": "–ù–∞—á–∞–ª—å–Ω–∞—è —Ñ–∞–∑–∞",
    "text": "–ù–∞—á–∞–ª—å–Ω–∞—è —Ñ–∞–∑–∞\n–ü–æ—Å—Ç–µ—Ä –≤ –ª–∞—Ç–µ—Ö–µ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ —Ä–∞–∑–¥–µ–ª—ã, –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º - —á—Ç–æ –≤ –∫–∞–∫–æ–º —Ä–∞–∑–¥–µ–ª–µ –≥–¥–µ –±—É–¥–µ—Ç. –î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –ø—Ä–∏–≤–µ–¥–µ–Ω üìù \\LaTeX —à–∞–±–ª–æ–Ω —Å üìú –ø—Ä–∏–º–µ—Ä–æ–º. –ù–∞ —ç—Ç–æ–º –º–æ–º–µ–Ω—Ç–µ –¥–∞–µ—Ç—Å—è —Ñ–∏–¥–±–µ–∫ –∏ —Å—Ç–∞–≤—è—Ç—Å—è –∑–∞–¥–∞—á–∏ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —ç—Ç–∞–ø—É."
  },
  {
    "objectID": "projects.html#mid-project-review",
    "href": "projects.html#mid-project-review",
    "title": "",
    "section": "Mid-project review",
    "text": "Mid-project review\n–ù–∞ —ç—Ç–æ–º –º–æ–º–µ–Ω—Ç–µ –¥–∞–µ—Ç—Å—è —Ñ–∏–¥–±–µ–∫ –∏ —Å—Ç–∞–≤—è—Ç—Å—è –∑–∞–¥–∞—á–∏ –∫ –∑–∞—â–∏—Ç–µ –ø—Ä–æ–µ–∫—Ç–∞."
  },
  {
    "objectID": "projects.html#–ø—Ä–µ–¥–∑–∞—â–∏—Ç–∞",
    "href": "projects.html#–ø—Ä–µ–¥–∑–∞—â–∏—Ç–∞",
    "title": "",
    "section": "–ü—Ä–µ–¥–∑–∞—â–∏—Ç–∞",
    "text": "–ü—Ä–µ–¥–∑–∞—â–∏—Ç–∞"
  },
  {
    "objectID": "projects.html#publishing-plan",
    "href": "projects.html#publishing-plan",
    "title": "",
    "section": "Publishing plan",
    "text": "Publishing plan\n–ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã –º—ã —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Ä–µ—à–∞–µ–º –∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω—ã–π —Ç—Ä—É–¥. –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å: * —Å—Ç–∞—Ç—å—è –≤ –∂—É—Ä–Ω–∞–ª–µ * —Å—Ç–∞—Ç—å—è –Ω–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ * —Å—Ç–∞—Ç—å—è –¥–ª—è –ª–µ—Ç–Ω–µ–π —à–∫–æ–ª—ã * –¥–æ–∫–ª–∞–¥ –Ω–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ * –ø—É–±–ª–∏–∫–∞—Ü–∏—è –Ω–∞ –≤–∞—à–µ–º —Å–∞–π—Ç–µ * —Å—Ç–∞—Ç—å—è –≤ –±–ª–æ–≥–µ * –≤–∏–¥–µ–æ –Ω–∞ YouTube–∫–∞–Ω–∞–ª–µ –∏ —Ç.–¥.\n–í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —ç—Ç–æ—Ç –ø–ª–∞–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞—Ç—ã, –≤—ã–±—Ä–∞–Ω–Ω—ã–π –∂—É—Ä–Ω–∞–ª, –∫—É–¥–∞ —ç—Ç–æ –±—É–¥–µ—Ç –ø–æ–¥–∞–≤–∞—Ç—å—Å—è."
  },
  {
    "objectID": "projects.html#–ø—É–±–ª–∏—á–Ω–∞—è-–∑–∞—â–∏—Ç–∞",
    "href": "projects.html#–ø—É–±–ª–∏—á–Ω–∞—è-–∑–∞—â–∏—Ç–∞",
    "title": "",
    "section": "–ü—É–±–ª–∏—á–Ω–∞—è –∑–∞—â–∏—Ç–∞",
    "text": "–ü—É–±–ª–∏—á–Ω–∞—è –∑–∞—â–∏—Ç–∞\n–ö –∑–∞—â–∏—Ç–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≥–æ—Ç–æ–≤ –ø–æ—Å—Ç–µ—Ä –≤ –ª–∞—Ç–µ—Ö–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–æ–µ–∫—Ç–∞. –ü–æ–¥–≤–µ–¥–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤. –ó–¥–µ—Å—å –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ —Å—Ç—É–¥–µ–Ω—Ç–∞. –û–Ω–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–Ω—è—Ç–Ω—ã–º, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º, –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º.\n–í—Å–µ –¥–µ–¥–ª–∞–π–Ω—ã –ø–æ–Ω–∏–º–∞—é—Ç—Å—è –∫–∞–∫ 23:59:59 –ø–æ –ú–æ—Å–∫–æ–≤—Å–∫–æ–º—É –≤—Ä–µ–º–µ–Ω–∏."
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "",
    "section": "",
    "text": "Matrix calculus\n\nGiven a matrix A of size m \\times n and a vector x of size n \\times 1, compute the gradient of the function f(x) = \\text{tr}(A^T A x x^T) with respect to x.\nFind the gradient \\nabla f(x) and hessian f''(x), if f(x) = \\dfrac{1}{2} \\Vert Ax - b\\Vert^2_2.\nFind the gradient \\nabla f(x) and hessian f''(x), if \nf(x) = \\frac1m \\sum\\limits_{i=1}^m \\log \\left( 1 + \\exp(a_i^{T}x) \\right) + \\frac{\\mu}{2}\\Vert x\\Vert _2^2, \\; a_i, x \\in \\mathbb R^n, \\; \\mu&gt;0\n\nCompute the gradient \\nabla_A f(A) of the trace of the matrix exponential function f(A) = \\text{tr}(e^A) with respect to A. Hint: hint: Use the definition of the matrix exponential. Use the definition of the differential df = f(A + dA) - f(A) + o(\\Vert dA \\Vert) with the limit \\Vert dA \\Vert \\to 0.\nFind the gradient \\nabla f(x) and hessian f''(x), if f(x) = \\frac{1}{2}\\Vert A - xx^T\\Vert^2_F, A \\in \\mathbb{S}^n\nCalculate the first and the second derivative of the following function f : S \\to \\mathbb{R}\n\nf(t) = \\text{det}(A ‚àí tI_n),\n\nwhere A \\in \\mathbb{R}^{n \\times n}, S := \\{t \\in \\mathbb{R} : \\text{det}(A ‚àí tI_n) \\neq 0\\}.\nFind the gradient \\nabla f(X), if f(X) = \\text{tr}\\left( AX^2BX^{-\\top} \\right).\n\n\n\nAutomatic differentiation and jax\nYou can use any automatic differentiation framework in this section (Jax, PyTorch, Autograd etc.)\n\nYou will work with the following function for this exercise, \nf(x,y)=e^{‚àí\\left(sin(x)‚àícos(y)\\right)^2}\n\nDraw the computational graph for the function. Note, that it should contain only primitive operations - you need to do it automatically - jax example, PyTorch example - you can google/find your way to visualize it.\nCompare analytic and autograd (with any framework) approach for the calculation of the gradient of:\n\nf(A) = \\text{tr}(e^A)\n\nWe can use automatic differentiation not only to calculate necessary gradients but also for tuning hyperparameters of the algorithm like learning rate in gradient descent (with gradient descent ü§Ø). Suppose, we have the following function f(x) = \\frac{1}{2}\\Vert x\\Vert^2, select a random point x_0 \\in \\mathbb{B}^{1000} = \\{0 \\leq x_i \\leq 1 \\mid \\forall i\\}. Consider 10 steps of the gradient descent starting from the point x_0: \nx_{k+1} = x_k - \\alpha_k \\nabla f(x_k)\n Your goal in this problem is to write the function, that takes 10 scalar values \\alpha_i and return the result of the gradient descent on function L = f(x_{10}). And optimize this function using gradient descent on \\alpha \\in \\mathbb{R}^{10}. Suppose that each of 10 components of \\alpha is uniformly distributed on [0; 0.1]. \n\\alpha_{k+1} = \\alpha_k - \\beta \\frac{\\partial L}{\\partial \\alpha}\n Choose any constant \\beta and the number of steps you need. Describe the obtained results. How would you understand, that the obtained schedule (\\alpha \\in \\mathbb{R}^{10}) becomes better than it was at the start? How do you check numerically local optimality in this problem?\nCompare analytic and autograd (with any framework) approach for the gradient of:\n\nf(X) = - \\log \\det X\n\nCompare analytic and autograd (with any framework) approach for the gradient and hessian of:\n\nf(x) = x^\\top x x^\\top x\n\n\n\n\n\nConvex sets\n\nShow, that if S \\subseteq \\mathbb{R}^n is convex set, then its interior \\mathbf{int } S and closure \\bar{S} are also convex sets.\nShow, that \\mathbf{conv}\\{xx^\\top: x \\in \\mathbb{R}^n, \\Vert x\\Vert = 1\\} = \\{A \\in \\mathbb{S}^n_+: \\text{tr}(A) = 1\\}.\nLet K \\subseteq \\mathbb{R}^n_+ is a cone. Prove that it is convex if and only if a set of \\{x \\in K \\mid \\sum\\limits_{i=1}^n x_i = 1 \\} is convex.\nProve that the set of \\{x \\in \\mathbb{R}^2 \\mid e^{x_1}\\le x_2\\} is convex.\nShow that the set of directions of the non-strict local descending of the differentiable function in a point is a convex cone. (Previously, the question contained a typo ‚Äústrict local descending‚Äù)\nIs the following set convex \nS = \\left\\{ a \\in \\mathbb{R}^k \\mid p(0) = 1, \\vert p(t) \\vert\\leq 1 \\text{ for } \\alpha\\leq t \\leq \\beta\\right\\},\n where \np(t) = a_1 + a_2 t + \\ldots + a_k t^{k-1} \\;?\n\n\n\n\n\nConvex functions\n\nConsider the function f(x) = x^d, where x \\in \\mathbb{R}_{+}. Fill the following table with ‚úÖ or ‚ùé. Explain your answers\n\n\n\n\nd\nConvex\nConcave\nStrictly Convex\n\\mu-strongly convex\n\n\n\n\n-2, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n-1, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0.5\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\\in (1; 2)\n\n\n\n\n\n\n2\n\n\n\n\n\n\n&gt; 2\n\n\n\n\n\n\n\n\nProve that the entropy function, defined as\n\nf(x) = -\\sum_{i=1}^n x_i \\log(x_i),\n\nwith \\text{dom}(f) = \\{x \\in \\R^n_{++} : \\sum_{i=1}^n x_i = 1\\}, is strictly concave.\nShow, that the function f: \\mathbb{R}^n_{++} \\to \\mathbb{R} is convex if f(x) = - \\prod\\limits_{i=1}^n x_i^{\\alpha_i} if \\mathbf{1}^T \\alpha = 1, \\alpha \\succeq 0.\nShow that the maximum of a convex function f over the polyhedron P = \\text{conv}\\{v_1, \\ldots, v_k\\} is achieved at one of its vertices, i.e.,\n\n\\sup_{x \\in P} f(x) = \\max_{i=1, \\ldots, k} f(v_i).\n\nA stronger statement is: the maximum of a convex function over a closed bounded convex set is achieved at an extreme point, i.e., a point in the set that is not a convex combination of any other points in the set. (you do not have to prove it). Hint: Assume the statement is false, and use Jensen‚Äôs inequality.\nShow, that the two definitions of \\mu-strongly convex functions are equivalent:\n\nf(x) is \\mu-strongly convex \\iff for any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1 for some \\mu &gt; 0:\n\nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2) - \\frac{\\mu}{2} \\lambda (1 - \\lambda)\\|x_1 - x_2\\|^2\n\nf(x) is \\mu-strongly convex \\iff if there exists \\mu&gt;0 such that the function f(x) - \\dfrac{\\mu}{2}\\Vert x\\Vert^2 is convex.\n\n\n\n\nConjugate sets\n\nLet \\mathbb{A}_n be the set of all n dimensional antisymmetric matrices (s.t. X^T = - X). Show that \\left( \\mathbb{A}_n\\right)^* = \\mathbb{S}_n.\nFind the sets S^{*}, S^{**}, S^{***}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge 0, \\;\\; -\\dfrac12x_1 + x_2 \\ge 0, \\;\\; 2x_1 + x_2 \\ge -1 \\;\\; -2x_1 + x_2 \\ge -3\\}\n\nProve, that B_p and B_{p_*} are inter-conjugate, i.e.¬†(B_p)^* = B_{p_*}, (B_{p_*})^* = B_p, where B_p is the unit ball (w.r.t. p - norm) and p, p_* are conjugated, i.e.¬†p^{-1} + p^{-1}_* = 1. You can assume, that p_* = \\infty if p = 1 and vice versa.\n\n\n\n\nConjugate functions\n\nFind f^*(y), if f(x) = \\vert 2x \\vert\nProve, that if f(x) = \\inf\\limits_{u+v = x} (g(u) + h(v)), then f^*(y) = g^*(y) + h^*(y).\nFind f^*(y), if f(x) = \\log \\left( \\sum\\limits_{i=1}^n e^{x_i} \\right)\nProve, that if f(x) = g(Ax), then f^*(y) = g^*(A^{-\\top}y)\nFind f^*(Y), if f(X) = - \\ln \\det X, X \\in \\mathbb{S}^n_{++}\nThe scalar Huber function is defined as\n\nf_{\\text{hub}}(x) =\n\\begin{cases}\n\\frac{1}{2} x^2 & \\text{if } |x| \\leq 1 \\\\\n|x| - \\frac{1}{2} & \\text{if } |x| &gt; 1\n\\end{cases}\n\n\n\n\nScalar case\n\n\nThis convex function arises in various applications, notably in robust estimation. This problem explores the generalizations of the Huber function to \\mathbb{R}^n. A straightforward extension to \\mathbb{R}^n is expressed as f_{\\text{hub}}(x_1) + \\ldots + f_{\\text{hub}}(x_n), yet this formulation is not circularly symmetric, that is, it‚Äôs not invariant under the transformation of x by an orthogonal matrix. A circularly symmetric extension to \\mathbb{R}^n is given by\n\nf_{\\text{cshub}}(x) = f_{\\text{hub}}(\\Vert x\\Vert )=\n\\begin{cases}\n\\frac{1}{2} \\Vert x\\Vert_2 ^2 & \\text{if } \\Vert x\\Vert_2 \\leq 1 \\\\\n\\Vert x\\Vert_2 - \\frac{1}{2} & \\text{if } \\Vert x\\Vert_2 &gt; 1\n\\end{cases}\n\nwhere the subscript denotes ‚Äúcircularly symmetric Huber function‚Äù. Show, that f_{\\text{cshub}} is convex. Find the conjugate function f^*(y).\n\n\n\n\nSubgradient and subdifferential\n\nFind \\partial f(x), if \nf(x) = \\text{Parametric ReLU}(x) = \\begin{cases}\n     x & \\text{if } x &gt; 0, \\\\\n     ax & \\text{otherwise}.\n\\end{cases}\n\nProve, that x_0 - is the minimum point of a function f(x) if and only if 0 \\in \\partial f(x_0).\nFind \\partial f(x), if f(x) = \\Vert Ax - b\\Vert _1.\nFind \\partial f(x), if f(x) = e^{\\Vert x\\Vert}.\nFind \\partial f(x), if f(x) = \\frac12 \\Vert Ax - b\\Vert _2^2 + \\lambda \\Vert x\\Vert_1, \\quad \\lambda &gt; 0.\nLet S \\subseteq \\mathbb{R}^n be a convex set. We will call a normal cone of the set S at a point x the following set: \nN_S(x) = \\left\\{c \\in \\mathbb{R}^n : \\langle c, y-x\\rangle \\leq 0 \\quad \\forall y \\in S\\right\\}\n\n\nDraw a normal cone for a set at the points A, B, C, D, E, F on the figure below:\n\n\n\nDraw a normal cone for the set S in these points\n\n\nShow, that N_S(x) = \\{0\\} \\quad \\forall x \\in \\mathbf{ri }(S).\nShow, that the subdifferential \\partial I_S(x) = N_S(x) if I_S(x) is the indicator function, i.e.¬† \nI_S(x) = \\begin{cases}0,\\text{if } x \\in S\\\\ \\infty, \\text{otherwise}\\end{cases}\n\n\n\n\n\n\nKKT and duality\nIn this section, you can consider either the arbitrary norm or the Euclidian norm if nothing else is specified.\n\nToy example \n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq 0\n\\end{split}\n\n\nGive the feasible set, the optimal value, and the optimal solution.\nPlot the objective x^2 +1 versus x. On the same plot, show the feasible set, optimal point, and value, and plot the Lagrangian L(x,\\mu) versus x for a few positive values of \\mu. Verify the lower bound property (p^* \\geq \\inf_x L(x, \\mu)for \\mu \\geq 0). Derive and sketch the Lagrange dual function g.\nState the dual problem, and verify that it is a concave maximization problem. Find the dual optimal value and dual optimal solution \\mu^*. Does strong duality hold?\nLet p^*(u) denote the optimal value of the problem\n\n\n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq u\n\\end{split}\n\nas a function of the parameter u. Plot p^*(u). Verify that \\dfrac{dp^*(0)}{du} = -\\mu^*\nDerive the dual problem for the Ridge regression problem with A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, \\lambda &gt; 0:\n\n\\begin{split}\n\\dfrac{1}{2}\\|y-b\\|^2 + \\dfrac{\\lambda}{2}\\|x\\|^2 &\\to \\min\\limits_{x \\in \\mathbb{R}^n, y \\in \\mathbb{R}^m }\\\\\n\\text{s.t. } & y = Ax\n\\end{split}\n\nDerive the dual problem for the support vector machine problem with A \\in \\mathbb{R}^{m \\times n}, \\mathbf{1} \\in \\mathbb{R}^m \\in \\mathbb{R}^m, \\lambda &gt; 0:\n\n\\begin{split}\n\\langle \\mathbf{1}, t\\rangle + \\dfrac{\\lambda}{2}\\|x\\|^2 &\\to \\min\\limits_{x \\in \\mathbb{R}^n, t \\in \\mathbb{R}^m }\\\\\n\\text{s.t. } & Ax \\succeq \\mathbf{1} - t \\\\\n& t \\succeq 0\n\\end{split}\n\nGive an explicit solution to the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = 1, \\\\\n& x \\succeq 0\n\\end{split}\n\nThis problem can be considered the simplest portfolio optimization problem.\nShow, that the following problem has a unique solution and find it:\n\n\\begin{split}\n& \\langle C^{-1}, X\\rangle - \\log \\det X \\to \\min\\limits_{x \\in \\mathbb{R}^{n \\times n} }\\\\\n\\text{s.t. } & \\langle Xa, a\\rangle \\leq 1,\n\\end{split}\n\nwhere C \\in \\mathbb{S}^n_{++}, a \\in \\mathbb{R}^n \\neq 0. The answer should not involve inversion of the matrix C.\nGive an explicit solution to the following QP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & (x - x_c)^\\top A (x - x_c) \\leq 1,\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n_{++}, c \\neq 0, x_c \\in \\mathbb{R}^n.\nConsider the equality-constrained least-squares problem\n\n\\begin{split}\n& \\|Ax - b\\|_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & Cx = d,\n\\end{split}\n\nwhere A \\in \\mathbb{R}^{m \\times n} with \\mathbf{rank }A = n, and C \\in \\mathbb{R}^{k \\times n} with \\mathbf{rank }C = k. Give the KKT conditions, and derive expressions for the primal solution x^* and the dual solution \\lambda^*.\nDerive the KKT conditions for the problem\n\n\\begin{split}\n& \\mathbf{tr \\;}X - \\log\\text{det }X \\to \\min\\limits_{X \\in \\mathbb{S}^n_{++} }\\\\\n\\text{s.t. } & Xs = y,\n\\end{split}\n\nwhere y \\in \\mathbb{R}^n and s \\in \\mathbb{R}^n are given with y^\\top s = 1. Verify that the optimal solution is given by\n\nX^* = I + yy^\\top - \\dfrac{1}{s^\\top s}ss^\\top\n\nSupporting hyperplane interpretation of KKT conditions. Consider a convex problem with no equality constraints\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\quad i = [1,m]\n\\end{split}\n\nAssume, that \\exists x^* \\in \\mathbb{R}^n, \\mu^* \\in \\mathbb{R}^m satisfy the KKT conditions\n\n\\begin{split}\n& \\nabla_x L (x^*, \\mu^*) = \\nabla f_0(x^*) + \\sum\\limits_{i=1}^m\\mu_i^*\\nabla f_i(x^*) = 0 \\\\\n& \\mu^*_i \\geq 0, \\quad i = [1,m] \\\\\n& \\mu^*_i f_i(x^*) = 0, \\quad i = [1,m]\\\\\n& f_i(x^*) \\leq 0, \\quad i = [1,m]\n\\end{split}\n\nShow that\n\n\\nabla f_0(x^*)^\\top (x - x^*) \\geq 0\n\nfor all feasible x. In other words, the KKT conditions imply the simple optimality criterion or \\nabla f_0(x^*) defines a supporting hyperplane to the feasible set at x^*.\nFenchel + Lagrange = ‚ô•. Express the dual problem of\n\n\\begin{split}\n& c^\\top x\\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & f(x) \\leq 0\n\\end{split}\n\nwith c \\neq 0, in terms of the conjugate function f^*. Explain why the problem you give is convex. We do not assume f is convex.\nA penalty method for equality constraints. We consider the problem of minimization\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b,\n\\end{split}\n\nwhere $f_0(x): ^n $ is convex and differentiable, and A \\in \\mathbb{R}^{m \\times n} with \\mathbf{rank }A = m. In a quadratic penalty method, we form an auxiliary function\n\n\\phi(x) = f_0(x) + \\alpha \\|Ax - b\\|_2^2,\n\nwhere \\alpha &gt; 0 is a parameter. This auxiliary function consists of the objective plus the penalty term \\alpha \\Vert Ax - b\\Vert_2^2. The idea is that a minimizer of the auxiliary function, \\tilde{x}, should be an approximate solution to the original problem. Intuition suggests that the larger the penalty weight \\alpha, the better the approximation \\tilde{x} to a solution of the original problem. Suppose \\tilde{x} is a minimizer of \\phi(x). Show how to find, from \\tilde{x}, a dual feasible point for the original problem. Find the corresponding lower bound on the optimal value of the original problem.\nAnalytic centering. Derive a dual problem for\n\n-\\sum_{i=1}^m \\log (b_i - a_i^\\top x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\n\nwith domain \\{x \\mid a^\\top_i x &lt; b_i , i = [1,m]\\}.\nFirst introduce new variables y_i and equality constraints y_i = b_i ‚àí a^\\top_i x. (The solution to this problem is called the analytic center of the linear inequalities a^\\top_i x \\leq b_i ,i = [1,m]. Analytic centers have geometric applications, and play an important role in barrier methods.)\n\n\n\n\nLinear programming\n\nüì±üéßüíª Covers manufacturing. Lyzard Corp is producing covers for the following products:\n\nüì± phones\nüéß headphones\nüíª laptops\n\nThe company‚Äôs production facilities are such that if we devote the entire production to headphone covers, we can produce 5000 of them in one day. If we devote the entire production to phone covers or laptop covers, we can produce 4000 or 2000 of them in one day.\nThe production schedule is one week (6 working days), and the week‚Äôs production must be stored before distribution. Storing 1000 headphone covers (packaging included) takes up 30 cubic feet of space. Storing 1000 phone covers (packaging included) takes up 50 cubic feet of space, and storing 1000 laptop covers (packaging included) takes up 220 cubic feet of space. The total storage space available is 1500 cubic feet.\nDue to commercial agreements with Lyzard Corp has to deliver at least 4500 headphone covers and 4000 laptop covers per week to strengthen the product‚Äôs diffusion.\nThe marketing department estimates that the weekly demand for headphones covers, phone, and laptop covers does not exceed 10000 14000, and 7000 units, therefore the company does not want to produce more than these amounts for headphones, phone, and laptop covers.\nFinally, the net profit per headphone cover, phone cover, and laptop cover are $5, $7, and $12, respectively.\nThe aim is to determine a weekly production schedule that maximizes the total net profit.\n\nWrite a Linear Programming formulation for the problem. Use the following variables:\n\ny_1 = number of headphones covers produced over the week,\n\ny_2 = number of phone covers produced over the week,\n\ny_3 = number of laptop covers produced over the week.\n\nFind the solution to the problem using PyOMO\n!pip install pyomo\n! sudo apt-get install glpk-utils --quiet  # GLPK\n! sudo apt-get install coinor-cbc --quiet  # CoinOR\nPerform the sensitivity analysis. Which constraint could be relaxed to increase the profit the most? Prove it numerically.\n\nProve the optimality of the solution\n\nx = \\left(\\frac{7}{3} , 0, \\frac{1}{3}\\right)^T\n\nto the following linear programming problem:\n\n\\begin{split}\n& 9x_1 + 3x_2 + 7x_3 \\to \\max\\limits_{x \\in \\mathbb{R}^3 }\\\\\n\\text{s.t. } & 2x_1 + x_2 + 3x_3 \\leq 6 \\\\\n& 5x_1 + 4x_2 + x_3 \\leq 12 \\\\\n& 3x_3 \\leq 1,\\\\\n& x_1, x_2, x_3 \\geq 0\n\\end{split}\n\nbut you cannot use any numerical algorithm here.\nTransform the following linear program into an equivalent linear program in the standard form \\left(c^\\top x \\to \\min\\limits_{x\\in \\mathbb{R}^n} : Ax = b,x ‚â• 0\\right):\n\n\\begin{split}\n& x_1‚àíx_2 \\to \\min\\limits_{x \\in \\mathbb{R}^2 }\\\\\n\\text{s.t. } & 2x_1 + x_2 \\geq 3 \\\\\n& 3x_1 ‚àí x_2 \\leq 7 \\\\\n& x_1 \\geq 0\n\\end{split}\n\nConsider:\n\n\\begin{split}\n& 4x_1 + 5x_2 + 2x_3 \\to \\max\\limits_{x \\in \\mathbb{R}^3 }\\\\\n\\text{s.t. } & 2x_1 - x_2 + 2x_3 \\leq 9 \\\\\n& 3x_1 + 5x_2 + 4x_3 \\leq 8 \\\\\n& x_1 + x_2 + 2x_3 \\leq 2 \\\\\n& x_1, x_2, x_3 \\geq 0,\n\\end{split}\n\n\nFind an optimal solution to the Linear Programming problem using the simplex method.\nWrite the dual linear program. Find an optimal dual solution. Do we have strong duality here?\n\n\n\n\n\nSequence convergence\n\nDetermine the convergence or divergence of a given sequences\n\nr_{k} = \\frac{1}{\\sqrt{k}}.\nr_{k} = 0.606^k.\nr_{k} = 0.606^{2^k}.\n\nDetermine the convergence or divergence of a given sequence r_k =\\begin{cases} \\frac{1}{k}, & \\text{if } k\\text{ is even} \\\\ e^{-k}, & \\text{if } k\\text{ is odd} \\end{cases}.\nDetermine the following sequence \\{r_k\\} by convergence rate (linear, sublinear, superlinear). In the case of superlinear convergence, additionally, find out whether there is quadratic convergence.\n\nr_k = \\dfrac{1}{k!}\n\nDetermine the following sequence \\{r_k\\} by convergence rate (linear, sublinear, superlinear). In the case of superlinear convergence, additionally find out whether there is quadratic convergence.\n\nr_k = \\dfrac{1}{k^k}\n\nLet \\{r_k\\} be a sequence of non-negative numbers given as r_{k+1} = Mr_k^2, where M &gt; 0, r_0 \\geq 0. Establish a necessary and sufficient condition on M and r_0 under which the sequence r_k will converge to zero. What is the rate of convergence?\n\n\n\nLine search\n\nShow that for a one-dimensional quadratic function decreasing at zero, its minimum satisfies Armijo‚Äôs condition for any c_1: 0 \\leq c_1 \\leq \\dfrac12:\n\nf(x_k - \\alpha \\nabla f (x_k)) \\leq f(x_k) - c_1 \\cdot \\alpha\\|\\nabla f(x_k)\\|_2^2\n\nImplementing and Testing Line Search Conditions in Gradient Descent\n\nx_{k+1} = x_k - \\alpha \\nabla f(x_k)\n\nIn this assignment, you will modify an existing Python code for gradient descent to include various line search conditions. You will test these modifications on two functions: a quadratic function and the Rosenbrock function. The main objectives are to understand how different line search strategies influence the convergence of the gradient descent algorithm and to compare their efficiencies based on the number of function evaluations.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize_scalar\nnp.random.seed(214)\n\n# Define the quadratic function and its gradient\ndef quadratic_function(x, A, b):\n    return 0.5 * np.dot(x.T, np.dot(A, x)) - np.dot(b.T, x)\n\ndef grad_quadratic(x, A, b):\n    return np.dot(A, x) - b\n\n# Generate a 2D quadratic problem with a specified condition number\ndef generate_quadratic_problem(cond_number):\n    # Random symmetric matrix\n    M = np.random.randn(2, 2)\n    M = np.dot(M, M.T)\n\n    # Ensure the matrix has the desired condition number\n    U, s, V = np.linalg.svd(M)\n    s = np.linspace(cond_number, 1, len(s))  # Spread the singular values\n    A = np.dot(U, np.dot(np.diag(s), V))\n\n    # Random b\n    b = np.random.randn(2)\n\n    return A, b\n\n# Gradient descent function\ndef gradient_descent(start_point, A, b, stepsize_func, max_iter=100):\n    x = start_point.copy()\n    trajectory = [x.copy()]\n\n    for i in range(max_iter):\n        grad = grad_quadratic(x, A, b)\n        step_size = stepsize_func(x, grad)\n        x -= step_size * grad\n        trajectory.append(x.copy())\n\n    return np.array(trajectory)\n\n# Backtracking line search strategy using scipy\ndef backtracking_line_search(x, grad, A, b, alpha=0.3, beta=0.8):\n    def objective(t):\n        return quadratic_function(x - t * grad, A, b)\n    res = minimize_scalar(objective, method='golden')\n    return res.x\n\n# Generate ill-posed problem\ncond_number = 30\nA, b = generate_quadratic_problem(cond_number)\n\n# Starting point\nstart_point = np.array([1.0, 1.8])\n\n# Perform gradient descent with both strategies\ntrajectory_fixed = gradient_descent(start_point, A, b, lambda x, g: 5e-2)\ntrajectory_backtracking = gradient_descent(start_point, A, b, lambda x, g: backtracking_line_search(x, g, A, b))\n\n# Plot the trajectories on a contour plot\nx1, x2 = np.meshgrid(np.linspace(-2, 2, 400), np.linspace(-2, 2, 400))\nZ = np.array([quadratic_function(np.array([x, y]), A, b) for x, y in zip(x1.flatten(), x2.flatten())]).reshape(x1.shape)\n\nplt.figure(figsize=(10, 8))\nplt.contour(x1, x2, Z, levels=50, cmap='viridis')\nplt.plot(trajectory_fixed[:, 0], trajectory_fixed[:, 1], 'o-', label='Fixed Step Size')\nplt.plot(trajectory_backtracking[:, 0], trajectory_backtracking[:, 1], 'o-', label='Backtracking Line Search')\n\n# Add markers for start and optimal points\nplt.plot(start_point[0], start_point[1], 'ro', label='Start Point')\noptimal_point = np.linalg.solve(A, b)\nplt.plot(optimal_point[0], optimal_point[1], 'y*', markersize=15, label='Optimal Point')\n\nplt.legend()\nplt.title('Gradient Descent Trajectories on Quadratic Function')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.savefig(\"linesearch.svg\")\nplt.show()\n\n\n\nThe code above plots this\n\n\nStart by reviewing the provided Python code. This code implements gradient descent with a fixed step size and a backtracking line search on a quadratic function. Familiarize yourself with how the gradient descent function and the step size strategies are implemented.\n\nModify the gradient descent function to include the following line search conditions:\n\nSufficient Decrease Condition\nCurvature Condition\nGoldstein Condition\nWolfe Condition\nDichotomy\n\nTest your modified gradient descent algorithm with the implemented line search conditions on the provided quadratic function. Plot the trajectories over iterations for each condition. Choose and specify hyperparameters for inexact line search condition. Choose and specify the termination criterion. Start from the point x_0 = (-1, 2)^T.\nCompare these 7 methods from the budget perspective. Plot the graph of function value from the number of function evaluations for each method on the same graph.\nPlot trajectory for another function with the same set of methods\n\nf(x_1, x_2) =  10(x_2 ‚àí x_1^2)^2 + (x_1 ‚àí 1)^2\n\nwith x_0 = (-1, 2)^T. You might need to adjust hyperparameters.\nPlot the same function value from the number of function calls for this experiment.\n\n\n\n\nZero-order methods\n\nSolve approximately the Travelling Salesman Problem with any zero-order optimization method.\n\n\n\nIllustration of TSP\n\n\nimport numpy as np\nfrom scipy.spatial import distance_matrix\nimport random\n\ndef generate_random_symmetric_tsp(num_cities, seed=0):\n    np.random.seed(seed)\n    points = np.random.rand(num_cities, 2)  # Generate random coordinates\n    dist_matrix = distance_matrix(points, points)\n    dist_matrix = (dist_matrix + dist_matrix.T) / 2  # Ensure symmetry\n    return jnp.array(dist_matrix)  # Convert to JAX array for further processing\n\n# Example usage\nnum_cities = 10\ndist_matrix = generate_random_symmetric_tsp(num_cities)\nprint(dist_matrix)\n\nYou can use the genetic algorithm with any significant modification of mutation(5/10)\nYou can use another algorithm (10/10)\n\nIn this assignment, we aim to explore and compare the efficacy of traditional and zero-order optimization methods in training a simple neural network. The conventional approach, Stochastic Gradient Descent (SGD), has been widely used due to its efficiency in handling large datasets and its capability to work in a huge-scale setting. This method relies on gradients of the loss function with respect to the network‚Äôs parameters. In contrast, zero-order optimization methods, also known as derivative-free methods, optimize without explicit gradient information. These methods are particularly useful for non-differentiable, noisy, or highly complex loss landscapes. The assignment‚Äôs objective is to explore such algorithms as Genetic Algorithms, Simulated Annealing, Gradient-free methods, or the Nelder-Mead method for real-world problems.\nNote, that a variety of feed-forward neural networks could be represented as a series of linear transformations, followed by some nonlinear function (say, \\text{ReLU }(x)):\n\n\\mathcal{NN}(x) = f_L \\circ w_L \\circ \\ldots \\circ f_1 \\circ w_1 \\circ x,\n\nwhere L is the number of layers, f_i - non-linear activation function, w_i = W_i x + b_i - linear layer. We can denote the training data by X and the labels by y. The overall optimization problem here is to train the neural network to approximate the mapping X \\to y, i.e.¬†\\mathcal{NN}(X) should be as close to y as possible for all data points. We can ensure this by minimizing the loss function, which depends on the neural network parameters:\n\n\\mathcal{L}(\\mathcal{NN}(X, W, b), y) \\to \\min_{W, b}\n\nTypically, we use a cross-entropy loss function for the classification task. Do not worry if you are not familiar with neural networks or machine learning. Try to focus on the optimization algorithm here. You are provided with an example of this approach in the Colab notebook.\n\n\n\nComparison of SGD vs Evolutionary strategy for neural network without hidden layer.\n\n\n\n\n\nComparison of SGD vs Evolutionary strategy for neural network with several hidden layers.\n\n\nThe assignment requires you to implement a chosen zero-order optimization algorithm and compare its performance against SGD in training a predefined simple neural network (you can vary the structure of the network as you want for this problem). The comparison should focus on aspects such as convergence speed, final accuracy, and computational efficiency. Students should provide a name of the chosen zero-order algorithm and implement it in Python. You can use any method you want except the Evolutionary strategy, which is already in the example above."
  }
]