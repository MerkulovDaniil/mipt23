[
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "",
    "section": "",
    "text": "Matrix calculus\n\nGiven a matrix A of size m \\times n and a vector x of size n \\times 1, compute the gradient of the function f(x) = \\text{tr}(A^T A x x^T) with respect to x.\nFind the gradient \\nabla f(x) and hessian f''(x), if f(x) = \\dfrac{1}{2} \\Vert Ax - b\\Vert^2_2.\nFind the gradient \\nabla f(x) and hessian f''(x), if \nf(x) = \\frac1m \\sum\\limits_{i=1}^m \\log \\left( 1 + \\exp(a_i^{\\top}x) \\right) + \\frac{\\mu}{2}\\Vert x\\Vert _2^2, \\; a_i, x \\in \\mathbb R^n, \\; \\mu&gt;0\n\nCompute the gradient \\nabla_A f(A) of the trace of the matrix exponential function f(A) = \\text{tr}(e^A) with respect to A. Hint: hint: Use the definition of the matrix exponential. Use the defintion of the differential df = f(A + dA) - f(A) + o(\\Vert dA \\Vert) with the limit \\Vert dA \\Vert \\to 0.\nFind the gradient \\nabla f(x) and hessian f''(x), if f(x) = \\frac{1}{2}\\Vert A - xx^\\top\\Vert^2_F, A \\in \\mathbb{S}^n\nCalculate the first and the second derivative of the following function f : S \\to \\mathbb{R} \nf(t) = \\text{det}(A ‚àí tI_n),\n where A \\in \\mathbb{R}^{n \\times n}, S := \\{t \\in \\mathbb{R} : \\text{det}(A ‚àí tI_n) \\neq 0\\}.\nFind the gradient \\nabla f(X), if f(X) = \\text{tr}\\left( AX^2BX^{-\\top} \\right).\n\n\n\nAutomatic differentiation and jax\nYou can use any automatic differentiation framework in this section (Jax, PyTorch, Autograd etc.) 1. You will work with the following function for this exercise, \n    f(x,y)=e^{‚àí\\left(sin(x)‚àícos(y)\\right)^2}\n    \nDraw the computational graph for the function. Note, that it should contain only primitive operations - you need to do it automatically - jax example, PyTorch example - you can google/find your own way to visualise it.\n\nCompare analytic and autograd (with any framework) approach for the calculation of the gradient of:\n\nf(A) = \\text{tr}(e^A)\n\nWe can use automatic differentiation not only to calculate necessary gradient, but also for tuning hyperparameters of the algorithm like learning rate in gradient descent (with gradient descent ü§Ø). Suppose, we have the following function f(x) = \\frac{1}{2}\\Vert x\\Vert^2, select a random point x_0 \\in \\mathbb{B}^{1000} = \\{0 \\leq x_i \\leq 1 \\mid \\forall i\\}. Consider 10 steps of the gradient descent starting from the point x_0: \nx_{k+1} = x_k - \\alpha_k \\nabla f(x_k)\n Your goal in this problem is to write the function, that takes 10 scalar values \\alpha_i and return the result of the gradient descent on function L = f(x_{10}). And optimize this function using gradient descent on \\alpha \\in \\mathbb{R}^{10}. Suppose that each of 10 components of \\alpha is uniformly distributed on [0; 0.1]. \n\\alpha_{k+1} = \\alpha_k - \\beta \\frac{\\partial L}{\\partial \\alpha}\n Choose any constant \\beta and the number of steps your need. Describe obtained results. How would you understand, that the obtained schedule (\\alpha \\in \\mathbb{R}^{10}) becomes better, than it was at the start? How do you check numerically local optimality in this problem?\nCompare analytic and autograd (with any framework) approach for the gradient of:\n\nf(X) = - \\log \\det X\n\nCompare analytic and autograd (with any framework) approach for the gradient and hessian of:\n\nf(x) = x^\\top x x^\\top x\n\n\n\n\n\nConvex sets\n\nShow, that if S \\subseteq \\mathbb{R}^n is convex set, then its interior \\mathbf{int } S and closure \\bar{S} are also convex sets.\nShow, that \\mathbf{conv}\\{xx^\\top: x \\in \\mathbb{R}^n, \\Vert x\\Vert = 1\\} = \\{A \\in \\mathbb{S}^n_+: \\text{tr}(A) = 1\\}.\nLet K \\subseteq \\mathbb{R}^n_+ is a cone. Prove that it is convex if and only if a set of \\{x \\in K \\mid \\sum\\limits_{i=1}^n x_i = 1 \\} is convex.\nProve that the set of \\{x \\in \\mathbb{R}^2 \\mid e^{x_1}\\le x_2\\} is convex.\nShow that the set of directions of the non-strict local descending of the differentiable function in a point is a convex cone. (Previously, the question contained a typo ‚Äústrict local descending‚Äù)\nIs the following set convex \nS = \\left\\{ a \\in \\mathbb{R}^k \\mid p(0) = 1, \\vert p(t) \\vert\\leq 1 \\text{ for } \\alpha\\leq t \\leq \\beta\\right\\},\n where \np(t) = a_1 + a_2 t + \\ldots + a_k t^{k-1} \\;?\n\n\n\n\n\nConvex functions\n\nConsider the function f(x) = x^d, where x \\in \\mathbb{R}_{+}. Fill the following table with ‚úÖ or ‚ùé. Explain your answers\n\n\n\n\n\nd\nConvex\nConcave\nStrictly Convex\n\\mu-strongly convex\n\n\n\n\n-2, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n-1, x \\in \\mathbb{R}_{++}\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0.5\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\\in (1; 2)\n\n\n\n\n\n\n2\n\n\n\n\n\n\n&gt; 2\n\n\n\n\n\n\n\n\nProve that the entropy function, defined as\n\nf(x) = -\\sum_{i=1}^n x_i \\log(x_i),\n\nwith \\text{dom}(f) = \\{x \\in \\R^n_{++} : \\sum_{i=1}^n x_i = 1\\}, is strictly concave.\nShow, that the function f: \\mathbb{R}^n_{++} \\to \\mathbb{R} is convex if f(x) = - \\prod\\limits_{i=1}^n x_i^{\\alpha_i} if \\mathbf{1}^T \\alpha = 1, \\alpha \\succeq 0.\nShow that the maximum of a convex function f over the polyhedron P = \\text{conv}\\{v_1, \\ldots, v_k\\} is achieved at one of its vertices, i.e.,\n\n\\sup_{x \\in P} f(x) = \\max_{i=1, \\ldots, k} f(v_i).\n\nA stronger statement is: the maximum of a convex function over a closed bounded convex set is achieved at an extreme point, i.e., a point in the set that is not a convex combination of any other points in the set. (you do not have to prove it). Hint: Assume the statement is false, and use Jensen‚Äôs inequality.\nShow, that the two definitions of \\mu-strongly convex functions are equivalent:\n\nf(x) is \\mu-strongly convex \\iff for any x_1, x_2 \\in S and 0 \\le \\lambda \\le 1 for some \\mu &gt; 0:\n\nf(\\lambda x_1 + (1 - \\lambda)x_2) \\le \\lambda f(x_1) + (1 - \\lambda)f(x_2) - \\frac{\\mu}{2} \\lambda (1 - \\lambda)\\|x_1 - x_2\\|^2\n\nf(x) is \\mu-strongly convex \\iff if there exists \\mu&gt;0 such that the function f(x) - \\dfrac{\\mu}{2}\\Vert x\\Vert^2 is convex.\n\n\n\n\nConjugate sets\n\nLet \\mathbb{A}_n be the set of all n dimensional antisymmetric matrices (s.t. X^T = - X). Show that \\left( \\mathbb{A}_n\\right)^* = \\mathbb{S}_n.\nFind the sets S^{*}, S^{**}, S^{***}, if\n\nS = \\{ x \\in \\mathbb{R}^2 \\mid x_1 + x_2 \\ge 0, \\;\\; -\\dfrac12x_1 + x_2 \\ge 0, \\;\\; 2x_1 + x_2 \\ge -1 \\;\\; -2x_1 + x_2 \\ge -3\\}\n\nProve, that B_p and B_{p_*} are inter-conjugate, i.e.¬†(B_p)^* = B_{p_*}, (B_{p_*})^* = B_p, where B_p is the unit ball (w.r.t. p - norm) and p, p_* are conjugated, i.e.¬†p^{-1} + p^{-1}_* = 1. You can assume, that p_* = \\infty if p = 1 and vice versa.\n\n\n\n\nConjugate functions\n\nFind f^*(y), if f(x) = \\vert 2x \\vert\nProve, that if f(x) = \\inf\\limits_{u+v = x} (g(u) + h(v)), then f^*(y) = g^*(y) + h^*(y).\nFind f^*(y), if f(x) = \\log \\left( \\sum\\limits_{i=1}^n e^{x_i} \\right)\nProve, that if f(x) = g(Ax), then f^*(y) = g^*(A^{-\\top}y)\nFind f^*(Y), if f(X) = - \\ln \\det X, X \\in \\mathbb{S}^n_{++}\nThe scalar Huber function is defined as\n\nf_{\\text{hub}}(x) =\n\\begin{cases}\n\\frac{1}{2} x^2 & \\text{if } |x| \\leq 1 \\\\\n|x| - \\frac{1}{2} & \\text{if } |x| &gt; 1\n\\end{cases}\n\n\n\n\nScalar case\n\n\nThis convex function arises in various applications, notably in robust estimation. This problem explores the generalizations of the Huber function to \\mathbb{R}^n. A straightforward extension to \\mathbb{R}^n is expressed as f_{\\text{hub}}(x_1) + \\ldots + f_{\\text{hub}}(x_n), yet this formulation is not circularly symmetric, that is, it‚Äôs not invariant under the transformation of x by an orthogonal matrix. A circularly symmetric extension to \\mathbb{R}^n is given by\n\nf_{\\text{cshub}}(x) = f_{\\text{hub}}(\\Vert x\\Vert )=\n\\begin{cases}\n\\frac{1}{2} \\Vert x\\Vert_2 ^2 & \\text{if } \\Vert x\\Vert_2 \\leq 1 \\\\\n\\Vert x\\Vert_2 - \\frac{1}{2} & \\text{if } \\Vert x\\Vert_2 &gt; 1\n\\end{cases}\n\nwhere the subscript denotes ‚Äúcircularly symmetric Huber function‚Äù. Show, that f_{\\text{cshub}} is convex. Find the conjugate function f^*(y).\n\n\n\n\nSubgradient and subdifferential\n\nFind \\partial f(x), if \nf(x) = \\text{Parametric ReLU}(x) = \\begin{cases}\n     x & \\text{if } x &gt; 0, \\\\\n     ax & \\text{otherwise}.\n\\end{cases}\n\nProve, that x_0 - is the minimum point of a function f(x) if and only if 0 \\in \\partial f(x_0).\nFind \\partial f(x), if f(x) = \\Vert Ax - b\\Vert _1.\nFind \\partial f(x), if f(x) = e^{\\Vert x\\Vert}.\nFind \\partial f(x), if f(x) = \\frac12 \\Vert Ax - b\\Vert _2^2 + \\lambda \\Vert x\\Vert_1, \\quad \\lambda &gt; 0.\nLet S \\subseteq \\mathbb{R}^n be a convex set. We will call a normal cone of the set S at a point x the following set: \nN_S(x) = \\left\\{c \\in \\mathbb{R}^n : \\langle c, y-x\\rangle \\leq 0 \\quad \\forall y \\in S\\right\\}\n\n\nDraw a normal cone for a set at the points A, B, C, D, E, F on the figure below:\n\n\n\nDraw a normal cone for the set S in these points\n\n\nShow, that N_S(x) = \\{0\\} \\quad \\forall x \\in \\mathbf{ri }(S).\nShow, that the subdifferential \\partial I_S(x) = N_S(x) if I_S(x) is the indicator function, i.e.¬† \nI_S(x) = \\begin{cases}0,\\text{if } x \\in S\\\\ \\infty, \\text{otherwise}\\end{cases}\n\n\n\n\n\n\nKKT and duality\n\nToy example \n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq 0\n\\end{split}\n\n\nGive the feasible set, the optimal value, and the optimal solution.\nPlot the objective x^2 +1 versus x. On the same plot, show the feasible set, optimal point and value, and plot the Lagrangian L(x,\\mu) versus x for a few positive values of \\mu. Verify the lower bound property (p^* \\geq \\inf_x L(x, \\mu)for \\mu \\geq 0). Derive and sketch the Lagrange dual function g.\nState the dual problem, and verify that it is a concave maximization problem. Find the dual optimal value and dual optimal solution \\mu^*. Does strong duality hold?\nLet p^*(u) denote the optimal value of the problem\n\n\n\\begin{split}\n& x^2 + 1 \\to \\min\\limits_{x \\in \\mathbb{R} }\\\\\n\\text{s.t. } & (x-2)(x-4) \\leq u\n\\end{split}\n\nas a function of the parameter u. Plot p^*(u). Verify that \\dfrac{dp^*(0)}{du} = -\\mu^*\nDerive the dual problem for the Ridge regression problem with A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, \\lambda &gt; 0:\n\n\\begin{split}\n\\dfrac{1}{2}\\|y-b\\|^2 + \\dfrac{\\lambda}{2}\\|x\\|^2 &\\to \\min\\limits_{x \\in \\mathbb{R}^n, y \\in \\mathbb{R}^m }\\\\\n\\text{s.t. } & y = Ax\n\\end{split}\n\nDerive the dual problem for the support vector machine problem with A \\in \\mathbb{R}^{m \\times n}, \\mathbf{1} \\in \\mathbb{R}^m \\in \\mathbb{R}^m, \\lambda &gt; 0:\n\n\\begin{split}\n\\langle \\mathbf{1}, t\\rangle + \\dfrac{\\lambda}{2}\\|x\\|^2 &\\to \\min\\limits_{x \\in \\mathbb{R}^n, t \\in \\mathbb{R}^m }\\\\\n\\text{s.t. } & Ax \\succeq \\mathbf{1} - t \\\\\n& t \\succeq 0\n\\end{split}\n\nGive an explicit solution of the following LP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & 1^\\top x = 1, \\\\\n& x \\succeq 0\n\\end{split}\n\nThis problem can be considered as a simplest portfolio optimization problem.\nShow, that the following problem has a unique solution and find it:\n\n\\begin{split}\n& \\langle C^{-1}, X\\rangle - \\log \\det X \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & \\langle Xa, a\\rangle \\leq 1,\n\\end{split}\n\nwhere C \\in \\mathbb{S}^n_{++}, a \\in \\mathbb{R}^n \\neq 0. The answer should not involve inversion of the matrix C.\nGive an explicit solution of the following QP.\n\n\\begin{split}\n& c^\\top x \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & (x - x_c)^\\top A (x - x_c) \\leq 1,\n\\end{split}\n\nwhere A \\in \\mathbb{S}^n_{++}, c \\neq 0, x_c \\in \\mathbb{R}^n.\nConsider the equality constrained least-squares problem\n\n\\begin{split}\n& \\|Ax - b\\|_2^2 \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & Cx = d,\n\\end{split}\n\nwhere A \\in \\mathbb{R}^{m \\times n} with \\mathbf{rank }A = n, and C \\in \\mathbb{R}^{k \\times n} with \\mathbf{rank }C = k. Give the KKT conditions, and derive expressions for the primal solution x^* and the dual solution \\lambda^*.\nDerive the KKT conditions for the problem\n\n\\begin{split}\n& \\mathbf{tr \\;}X - \\log\\text{det }X \\to \\min\\limits_{X \\in \\mathbb{S}^n_{++} }\\\\\n\\text{s.t. } & Xs = y,\n\\end{split}\n\nwhere y \\in \\mathbb{R}^n and s \\in \\mathbb{R}^n are given with y^\\top s = 1. Verify that the optimal solution is given by\n\nX^* = I + yy^\\top - \\dfrac{1}{s^\\top s}ss^\\top\n\nSupporting hyperplane interpretation of KKT conditions. Consider a convex problem with no equality constraints\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & f_i(x) \\leq 0, \\quad i = [1,m]\n\\end{split}\n\nAssume, that \\exists x^* \\in \\mathbb{R}^n, \\mu^* \\in \\mathbb{R}^m satisfy the KKT conditions\n\n\\begin{split}\n& \\nabla_x L (x^*, \\mu^*) = \\nabla f_0(x^*) + \\sum\\limits_{i=1}^m\\mu_i^*\\nabla f_i(x^*) = 0 \\\\\n& \\mu^*_i \\geq 0, \\quad i = [1,m] \\\\\n& \\mu^*_i f_i(x^*) = 0, \\quad i = [1,m]\\\\\n& f_i(x^*) \\leq 0, \\quad i = [1,m]\n\\end{split}\n\nShow that\n\n\\nabla f_0(x^*)^\\top (x - x^*) \\geq 0\n\nfor all feasible x. In other words the KKT conditions imply the simple optimality criterion or \\nabla f_0(x^*) defines a supporting hyperplane to the feasible set at x^*.\nFenchel + Lagrange = ‚ô•. Express the dual problem of\n\n\\begin{split}\n& c^\\top x\\to \\min\\limits_{x \\in \\mathbb{R}^n }\\\\\n\\text{s.t. } & f(x) \\leq 0\n\\end{split}\n\nwith c \\neq 0, in terms of the conjugate function f^*. Explain why the problem you give is convex. We do not assume f is convex.\nA penalty method for equality constraints. We consider the problem of minimization\n\n\\begin{split}\n& f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\\\\\n\\text{s.t. } & Ax = b,\n\\end{split}\n\nwhere $f_0(x): ^n $ is convex and differentiable, and A \\in \\mathbb{R}^{m \\times n} with \\mathbf{rank }A = m. In a quadratic penalty method, we form an auxiliary function\n\n\\phi(x) = f_0(x) + \\alpha \\|Ax - b\\|_2^2,\n\nwhere \\alpha &gt; 0 is a parameter. This auxiliary function consists of the objective plus the penalty term \\alpha \\Vert Ax - b\\Vert_2^2. The idea is that a minimizer of the auxiliary function, \\tilde{x}, should be an approximate solution of the original problem. Intuition suggests that the larger the penalty weight \\alpha, the better the approximation \\tilde{x} to a solution of the original problem. Suppose \\tilde{x} is a minimizer of \\phi(x). Show how to find, from \\tilde{x}, a dual feasible point for the original problem. Find the corresponding lower bound on the optimal value of the original problem.\nAnalytic centering. Derive a dual problem for\n\n-\\sum_{i=1}^m \\log (b_i - a_i^\\top x) \\to \\min\\limits_{x \\in \\mathbb{R}^{n} }\n\nwith domain \\{x \\mid a^\\top_i x &lt; b_i , i = [1,m]\\}.\nFirst introduce new variables y_i and equality constraints y_i = b_i ‚àí a^\\top_i x. (The solution of this problem is called the analytic center of the linear inequalities a^\\top_i x \\leq b_i ,i = [1,m]. Analytic centers have geometric applications, and play an important role in barrier methods.)"
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "",
    "section": "",
    "text": "–ó–∞–Ω—è—Ç–∏–µ 1\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –í—Å–ø–æ–º–∏–Ω–∞–µ–º –±–∞–∑–æ–≤—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä—ã. –í–µ–∫—Ç–æ—Ä—ã, –º–∞—Ç—Ä–∏—Ü—ã, –Ω–æ—Ä–º—ã, —Å–∫–∞–ª—è—Ä–Ω—ã–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è\n\n    –ó–∞–Ω—è—Ç–∏–µ 2.1\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –°–ø–µ–∫—Ç—Ä –º–∞—Ç—Ä–∏—Ü—ã. SVD. Skeleton. –ì—Ä–∞–¥–∏–µ–Ω—Ç. –ì–µ—Å—Å–∏–∞–Ω. –ú–∞—Ç—Ä–∏—á–Ω–æ-–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ.\n\n    –ó–∞–Ω—è—Ç–∏–µ 2.2\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ. Forward\\Reverse Mode. –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ.\n\n    –ó–∞–Ω—è—Ç–∏–µ 3\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏ ‚Ä¢ ‚ñ∂Ô∏è Youtube ‚Ä¢ üíø –°–∫–∞—á–∞—Ç—å\n    \n    –í—ã–ø—É–∫–ª–æ—Å—Ç—å. –í—ã–ø—É–∫–ª—ã–µ, –∞—Ñ–∏–Ω–Ω—ã–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞. –í—ã–ø—É–∫–ª—ã–µ –∫–æ–Ω—É—Å—ã. –°—É–º–º–∞ –ú–∏–Ω–∫–æ–≤—Å–∫–æ–≥–æ\n\n    –ó–∞–Ω—è—Ç–∏–µ 4.1\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏\n    \n    –í—ã–ø—É–∫–ª—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏. –ù–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –ô–µ–Ω—Å–µ–Ω–∞. –°–∏–ª—å–Ω–æ –≤—ã–ø—É–ª—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏. –£—Å–ª–æ–≤–∏–µ –ü–æ–ª—è–∫–∞ - –õ–æ—è—Å–∏–µ–≤–∏—á–∞\n\n    –ó–∞–Ω—è—Ç–∏–µ 4.2\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏\n    \n    –°–æ–ø—Ä—è–∂–µ–Ω–Ω—ã–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞. –°–æ–ø—Ä—è–∂–µ–Ω–Ω—ã–µ –∫–æ–Ω—É—Å—ã. –ú–Ω–æ–≥–æ–≥—Ä–∞–Ω–Ω–∏–∫–∏\n\n    –ó–∞–Ω—è—Ç–∏–µ 5\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏\n    \n    –°–æ–ø—Ä—è–∂–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –õ–µ–∂–∞–Ω–¥—Ä–∞. –°–æ–ø—Ä—è–∂–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞\n\n    –ó–∞–Ω—è—Ç–∏–µ 6\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏\n    \n    –°—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç. –°—É–±–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª. –¢–µ–æ—Ä–µ–º—ã –ú–æ—Ä–æ-–†–æ–∫–∞—Ñ–µ–ª–ª–∞—Ä–∞, –î—É–±–æ–≤–∏—Ü–∫–æ–≥–æ-–ú–∏–ª—é—Ç–∏–Ω–∞\n\n    –ó–∞–Ω—è—Ç–∏–µ 7\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏\n    \n    –£—Å–ª–æ–≤–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏. –§—É–Ω–∫—Ü–∏—è –õ–∞–≥—Ä–∞–Ω–∂–∞. –ú–Ω–æ–∂–∏—Ç–µ–ª–∏ –õ–∞–≥—Ä–∞–Ω–∂–∞. –¢–µ–æ—Ä–µ–º–∞ –ö–∞—Ä—É—à–∞ - –ö—É–Ω–∞ - –¢–∞–∫–∫–µ—Ä–∞\n\n    –ó–∞–Ω—è—Ç–∏–µ 8\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã ‚Ä¢ üìù –ó–∞–ø–∏—Å–∏\n    \n    –î–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å. –í–≤–µ–¥–µ–Ω–∏–µ –≤ –¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å. –î–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞. Two-way partitioning problem. –ü—Ä–æ–µ–∫—Ü–∏—è —Ç–æ—á–∫–∏ –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π —Å–∏–º–ø–ª–µ–∫—Å\n\n    –ó–∞–Ω—è—Ç–∏–µ 9\n    \n        \n    \n    –õ–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ. –ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á –≤ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–µ LP. –î–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞ –∫ LP. –°–∏–º–ø–ª–µ–∫—Å –º–µ—Ç–æ–¥ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.\n\n    –ó–∞–Ω—è—Ç–∏–µ 10\n    \n        \n    \n    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –°–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏. –õ–∏–Ω–µ–π–Ω—ã–π –ø–æ–∏—Å–∫. –ù–µ—Ç–æ—á–Ω–∞—è –æ–¥–Ω–æ–º–µ—Ä–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è. –ü—Ä–∞–≤–∏–ª–∞ –ê—Ä–º–∏—Ö–æ  - –ì–æ–ª—å–¥—à—Ç–µ–π–Ω–∞. –£—Å–ª–æ–≤–∏–µ –í—É–ª—å—Ñ–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 11\n    \n        \n    \n    –ú–µ—Ç–æ–¥—ã –Ω—É–ª–µ–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞. –ë–µ–∑–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ì–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º. –≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã.\n\n    –ó–∞–Ω—è—Ç–∏–µ 12\n    \n        üìÑ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã\n    \n    –ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã –∑–∞–¥–∞—á –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. LP, QP, SOCP, SDP, –ó–∞–¥–∞—á–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ.\n\n    –ó–∞–Ω—è—Ç–∏–µ 13\n    \n        \n    \n    –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫. –¢–µ–æ—Ä–µ–º—ã —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –≥–ª–∞–¥–∫–æ–º —Å–ª—É—á–∞–µ (–≤—ã–ø—É–∫–ª—ã–µ, —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã–µ, PL). –£—Å–ª–æ–≤–∏–µ –ü–æ–ª—è–∫–∞ - –õ–æ—è—Å–µ–≤–∏—á–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 14\n    \n        \n    \n    –°—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫. –¢–µ–æ—Ä–µ–º—ã —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –Ω–µ–≥–ª–∞–¥–∫–æ–º —Å–ª—É—á–∞–µ (–≤—ã–ø—É–∫–ª—ã–π —Å–ª—É—á–∞–π). –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –≤ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–µ–≥–ª–∞–¥–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ó–∞–¥–∞—á–∞ –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ —Å $l_1$ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π.\n\n    –ó–∞–Ω—è—Ç–∏–µ 15\n    \n        \n    \n    –£—Å–∫–æ—Ä—è–µ–º –º–µ—Ç–æ–¥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ - NAG, Momentum, –ø–æ–ª–∏–Ω–æ–º—ã.\n\n    –ó–∞–Ω—è—Ç–∏–µ 16\n    \n        \n    \n    –ú–µ—Ç–æ–¥ –ø—Ä–æ–µ–∫—Ü–∏–∏ —Å—É–±–≥—Ä–∞–¥–∏–µ–Ω—Ç–∞. –ú–µ—Ç–æ–¥ —É—Å–ª–æ–≤–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ (–º–µ—Ç–æ–¥ –§—Ä–∞–Ω–∫–∞ - –í—É–ª—å—Ñ–∞). –ò–¥–µ—è –º–µ—Ç–æ–¥–∞ –∑–µ—Ä–∫–∞–ª—å–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞.\n\n    –ó–∞–Ω—è—Ç–∏–µ 17\n    \n        \n    \n    –ü—Ä–æ–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã. –ë–∞—Ä—å–µ—Ä–Ω—ã–µ –º–µ—Ç–æ–¥—ã.\n\n    –ó–∞–Ω—è—Ç–∏–µ 18\n    \n        \n    \n    –ú–µ—Ç–æ–¥ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π. –û—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ì—Ä–∞–º–º–∞ - –®–º–∏–¥—Ç–∞. –ü–æ–Ω—è—Ç–∏–µ $A$-–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤. –ú–µ—Ç–æ–¥ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.\n\n    –ó–∞–Ω—è—Ç–∏–µ 19\n    \n        \n    \n    –ö–æ–Ω—Ü–µ–ø—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏. –ú–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞. –ö–≤–∞–∑–∏–Ω—å—é—Ç–æ–Ω–æ–≤—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã. –ú–µ—Ç–æ–¥ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞. KFAC.\n\n    –ó–∞–Ω—è—Ç–∏–µ 20\n    \n        \n    \n    –í–≤–µ–¥–µ–Ω–∏–µ –≤ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã. –ë–∞—Ç—á, —ç–ø–æ—Ö–∞. –°—Ö–æ–¥–∏–º–æ—Å—Ç—å SGD.\n\n    –ó–∞–Ω—è—Ç–∏–µ 21\n    \n        \n    \n    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã. –ú–µ—Ç–æ–¥—ã —Ä–µ–¥—É–∫—Ü–∏–∏ –¥–∏—Å–ø–µ—Ä—Å–∏–∏.\n\n    –ó–∞–Ω—è—Ç–∏–µ 22\n    \n        \n    \n    –û–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π. –ü–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è.\n\n    –ó–∞–Ω—è—Ç–∏–µ 23\n    \n        \n    \n    –§–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è.\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "–ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ú–§–¢–ò 2023-2024",
    "section": "",
    "text": "–ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ú–§–¢–ò 2023-2024\n–ö—É—Ä—Å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–≤–µ–¥–µ–Ω–∏–µ –≤ —Ä–∞–∑–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –Ω–∞—á–∞–ª–∞ –≤—ã–ø—É–∫–ª–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞, –∏–∑–ª–∞–≥–∞—é—Ç—Å—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –ø–æ–¥—Ö–æ–¥—ã –≤ —Ä–µ—à–µ–Ω–∏–∏ –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö –∑–∞–¥–∞—á –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ö—É—Ä—Å —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞–±–æ—Ä —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Å–Ω–æ–≤ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø–æ–Ω–∏–º–∞—Ç—å –ø–æ—á–µ–º—É –∏ –∫–∞–∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ä–∞–±–æ—Ç–∞—é—Ç. –í –Ω–∞—á–∞–ª–µ –∫—É—Ä—Å–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —É–ø–æ—Ä –¥–µ–ª–∞–µ—Ç—Å—è –Ω–∞ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–ø–ø–∞—Ä–∞—Ç, –≤ –∫–æ–Ω—Ü–µ —É–¥–µ–ª—è–µ—Ç—Å—è –±–æ–ª—å—à–µ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ö–∞–∂–¥–æ–µ –∑–∞–Ω—è—Ç–∏–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–Ω—è—Ç–∏—è. –ü–µ—Ä–≤–∞—è —á–∞—Å—Ç—å –∫—É—Ä—Å–∞ –±–æ–ª—å—à–µ —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–∞ –Ω–µ —Ç–µ–æ—Ä–∏—é, –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –í–æ –≤—Ç–æ—Ä–æ–π —á–∞—Å—Ç–∏ –¥–µ–ª–∞–µ—Ç—Å—è —É–ø–æ—Ä –Ω–∞ –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –Ω–∞—á–∏–Ω–∞—è c –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∑–∞–∫–∞–Ω—á–∏–≤–∞—è —Å–∞–º—ã–º–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏."
  }
]