---
title: "Conjugate gradient method"
author: Daniil Merkulov
institute: Optimization methods. MIPT
# bibliography: ../../files/biblio.bib
# csl: ../../files/diabetologia.csl
format: 
    beamer:
        pdf-engine: pdflatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: false
        incremental: true
        include-in-header: ../../files/header.tex  # Custom LaTeX commands and preamble
---

# Quadratic optimization problem

## Strongly convex quadratics

:::: {.columns}

::: {.column width="60%"}
Consider the following quadratic optimization problem:
$$
\min\limits_{x \in \mathbb{R}^d} f(x) =  \min\limits_{x \in \mathbb{R}^d} \dfrac{1}{2} x^\top  A x - b^\top  x + c, \text{ where }A \in \mathbb{S}^d_{++}.
$$
:::
::: {.column width="40%"}
Optimality conditions
$$
Ax^* = b
$$
:::
::::
![](SD_vs_CG.pdf)

## Exact line search aka steepest descent

:::: {.columns}
::: {.column width="80%"}
$$
\alpha_k = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_{k+1}) = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_k - \alpha \nabla f(x_k))
$$
More theoretical than practical approach. It also allows you to analyze the convergence, but often exact line search can be difficult if the function calculation takes too long or costs a lot.

An interesting theoretical property of this method is that each following iteration is orthogonal to the previous one:
$$
\alpha_k = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_k - \alpha \nabla f(x_k))
$$

. . .

Optimality conditions:

. . .

$$
\nabla f(x_k)^T\nabla f(x_{k+1})  = 0
$$

:::{.callout-caution}

### Optimal value for quadratics

$$
\nabla f(x_k)^\top A (x_k - \alpha \nabla f(x_k)) - \nabla f(x_k)^\top b = 0 \qquad \alpha_k = \frac{\nabla f(x_k)^T \nabla f(x_k)}{\nabla f(x_k)^T A \nabla f(x_k)}
$$
:::
:::
::: {.column width="20%"}

![Steepest Descent](GD_vs_Steepest.pdf)

[Open In Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Steepest_descent.ipynb)
:::
::::

## Conjugate directions. $A$-orthogonality.

[![](A_orthogonality.pdf){#fig-aorth}](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/CG.ipynb)

## Conjugate directions. $A$-orthogonality.

Suppose, we have two coordinate systems and some quadratic function $f(x) = \frac12 x^T I x$ looks just like on the left part of @fig-aorth, while in another coordinates it looks like $f(\hat{x}) = \frac12 \hat{x}^T A \hat{x}$, where $A \in \mathbb{S}^d_{++}$.

:::: {.columns}

::: {.column width="50%"}
$$
\frac12 x^T I x
$$
:::
::: {.column width="50%"}
$$
\frac12 \hat{x}^T A \hat{x}
$$
:::
::::
Since $A = Q \Lambda Q^T$:
$$
\uncover<+->{ \frac12 \hat{x}^T A \hat{x} }\uncover<+->{ = \frac12 \hat{x}^T Q \Lambda Q^T \hat{x} }\uncover<+->{  = \frac12 \hat{x}^T Q \Lambda^{\frac12}\Lambda^{\frac12} Q^T \hat{x} }\uncover<+->{ = \frac12 x^T I x} \uncover<+->{  \qquad \text{if } x  = \Lambda^{\frac12} Q^T \hat{x} } \uncover<+->{\text{ and }  \hat{x} = Q \Lambda^{-\frac12} x}
$$

. . .

:::{.callout-caution}

### $A$-orthogonal vectors

Vectors $x \in \mathbb{R}^d$ and $y \in \mathbb{R}^d$ are called $A$-orthogonal (or $A$-conjugate) if
$$
x^T A y = 0 \qquad \Leftrightarrow \qquad x \perp_A y 
$$
When $A = I$, $A$-orthogonality becomes orthogonality.
:::

## Gram–Schmidt process

# Method of Conjugate Directions

## Idea of the method of conjugate directions

Thus, we formulate an algorithm:

1. Let $k = 0$ and $x_k = x_0$, count $d_k = d_0 = -\nabla f(x_0)$.
2. By the procedure of line search we find the optimal length of step. Calculate $\alpha$ minimizing $f(x_k + \alpha_k d_k)$ by the formula
    $$
    \alpha_k = -\frac{d_k^\top (A x_k - b)}{d_k^\top A d_k}
    $$
3. We're doing an algorithm step:
    $$
    x_{k+1} = x_k + \alpha_k d_k
    $$
4. update the direction: $d_{k+1} = -\nabla f(x_{k+1}) + \beta_k d_k$, where $\beta_k$ is calculated by the formula:
    $$
    \beta_k = \frac{\nabla f(x_{k+1})^\top A d_k}{d_k^\top A d_k}.
    $$
5. Repeat steps 2-4 until $n$ directions are built, where $n$ is the dimension of space (dimension of $x$).

## Method of Conjugate Directions

If a set of vectors $d_1, \ldots, d_k$ - are $A$-conjugate (each pair of vectors is $A$-conjugate), these vectors are linearly independent. $A \in \mathbb{S}^n_{++}$.

**Proof**

We'll show, that if $\sum\limits_{i=1}^k\alpha_k d_k = 0$, than all coefficients should be equal to zero:

$$
\begin{split}
0 &= \sum\limits_{i=1}^n\alpha_k d_k \\
&= d_j^\top A \left( \sum\limits_{i=1}^n\alpha_k d_k \right) \\
&=  \sum\limits_{i=1}^n \alpha_k d_j^\top A d_k  \\
&=  \alpha_j d_j^\top A d_j  + 0 + \ldots + 0\\
\end{split}
$$

Thus, $\alpha_j = 0$, for all other indices one have perform the same process

## Conjugate Gradients

## Conjugate Gradients

## Conjugate Gradients

# Conjugate gradient

## Conjugate gradient method

:::: {.columns}

::: {.column width="50%"}
:::{.callout-caution appearance="simple"}
$$
\begin{aligned}
&\text{Conjugate Gradient} = \text{Conjugate Directions} \\
&+ \text{ Residuals as starting vectors for Gram–Schmidt}
\end{aligned}
$$
:::
:::
::: {.column width="50%"}
$$
\begin{aligned}
& \mathbf{r}_0 := \mathbf{b} - \mathbf{A x}_0 \\
& \hbox{if } \mathbf{r}_{0} \text{ is sufficiently small, then return } \mathbf{x}_{0} \text{ as the result}\\
& \mathbf{d}_0 := \mathbf{r}_0 \\
& k := 0 \\
& \text{repeat} \\
& \qquad \alpha_k := \frac{\mathbf{r}_k^\mathsf{T} \mathbf{r}_k}{\mathbf{d}_k^\mathsf{T} \mathbf{A d}_k}  \\
& \qquad \mathbf{x}_{k+1} := \mathbf{x}_k + \alpha_k \mathbf{d}_k \\
& \qquad \mathbf{r}_{k+1} := \mathbf{r}_k - \alpha_k \mathbf{A d}_k \\
& \qquad \hbox{if } \mathbf{r}_{k+1} \text{ is sufficiently small, then exit loop} \\
& \qquad \beta_k := \frac{\mathbf{r}_{k+1}^\mathsf{T} \mathbf{r}_{k+1}}{\mathbf{r}_k^\mathsf{T} \mathbf{r}_k} \\
& \qquad \mathbf{d}_{k+1} := \mathbf{r}_{k+1} + \beta_k \mathbf{d}_k \\
& \qquad k := k + 1 \\
& \text{end repeat} \\
& \text{return } \mathbf{x}_{k+1} \text{ as the result}
\end{aligned}
$$
:::
::::


## Convergence


**Theorem 1.** If matrix $A$ has only $r$ different eigenvalues, then the conjugate gradient method converges in $r$ iterations.

**Theorem 2.** The following convergence bound holds

$$
\| x_{k} - x^* \|_A \leq 2\left( \dfrac{\sqrt{\kappa(A)} - 1}{\sqrt{\kappa(A)} + 1} \right)^k \|x_0 - x^*\|_A,
$$

where $\|x\|^2_A = x^{\top}Ax$ and $\kappa(A) = \frac{\lambda_1(A)}{\lambda_n(A)}$ is the conditioning number of matrix $A$, $\lambda_1(A) \geq ... \geq \lambda_n(A)$ are the eigenvalues of matrix $A$

**Note:** compare the coefficient of the geometric progression with its analog in gradient descent.


## Non-linear conjugate gradient method

In case we do not have an analytic expression for a function or its gradient, we will most likely not be able to solve the one-dimensional minimization problem analytically. Therefore, step 2 of the algorithm is replaced by the usual line search procedure. But there is the following mathematical trick for the fourth point:

For two iterations, it is fair:

$$
x_{k+1} - x_k = c d_k,
$$

where $c$ is some kind of constant. Then for the quadratic case, we have:

$$ 
\nabla f(x_{k+1}) - \nabla f(x_k) = (A x_{k+1} - b) - (A x_k - b) = A(x_{k+1}-x_k) = cA d_k
$$

Expressing from this equation the work $Ad_k = \dfrac{1}{c} \left( \nabla f(x_{k+1}) - \nabla f(x_k)\right)$, we get rid of the "knowledge" of the function in step definition $\beta_k$, then point 4 will be rewritten as:

$$
\beta_k = \frac{\nabla f(x_{k+1})^\top (\nabla f(x_{k+1}) - \nabla f(x_k))}{d_k^\top (\nabla f(x_{k+1}) - \nabla f(x_k))}.
$$

This method is called the Polack - Ribier method.

## Preconditioned conjugate gradient method


