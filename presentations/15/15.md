---
title: "Gradient Descent. Non-smooth case. Linear Least squares with $l_1$-regularization."
author: Daniil Merkulov
institute: Optimization methods. MIPT
# bibliography: ../../files/biblio.bib
# csl: ../../files/diabetologia.csl
format: 
    beamer:
        pdf-engine: pdflatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: false
        incremental: true
        include-in-header: ../../files/header.tex  # Custom LaTeX commands and preamble
---

# Recap

## Previously

:::: {.columns}
::: {.column width="50%"}

* Gradient Descent. Convergence for strongly convex quadratic function. Optimal hyperparameters.

    $$
    \alpha = \dfrac{2}{\mu + L} \quad \varkappa = \dfrac{L}{\mu} \geq 1 \quad \rho = \dfrac{\varkappa -1}{\varkappa + 1}
    $$

    $$
    \|x_k - x^*\| \leq \rho^k \|x_0 - x^*\|
    $$

* Gradient Descent. Smooth convex case convergence.

    $$
    f(x_k)-f^* \leq \frac{L \|x_0-x^*\|^2}{2 k}.
    $$

* Gradient Descent. Smooth PL case convergence.

    $$
    f(x_{k})-f^* \leq \left(1-\dfrac{\mu}{L}\right)^k (f(x_0)-f^*).
    $$

:::
::: {.column width="50%"}

![PL function](pl_3d.pdf)

:::
::::

## Any $\mu$-strongly convex differentiable function is a PL-function

:::{.callout-theorem}
If a function $f(x)$ is differentiable and $\mu$-strongly convex, then it is a PL-function.
:::

**Proof**

:::: {.columns}

::: {.column width="60%"}

By first order strong convexity criterion:

$$
f(y) \geq f(x) + \nabla f(x)^T(y-x) + \dfrac{\mu}{2}\|y-x\|_2^2
$$

Putting $y = x^*$:

$$
\begin{split}
\uncover<+->{ f(x^*) &\geq f(x) + \nabla f(x)^T(x^*-x) + \dfrac{\mu}{2}\|x^*-x\|_2^2 \\ }
\uncover<+->{ f(x) - f(x^*) &\leq \nabla f(x)^T(x-x^*) - \dfrac{\mu}{2}\|x^*-x\|_2^2 = \\ }
\uncover<+->{ &= \left(\nabla f(x)^T - \dfrac{\mu}{2}(x^*-x)\right)^T (x-x^*) = \\ }
\uncover<+->{ &= \frac12 \left(\frac{2}{\sqrt{\mu}}\nabla f(x)^T - \sqrt{\mu}(x^*-x)\right)^T \sqrt{\mu}(x-x^*) = \\ }
\end{split}
$$
:::

. . .

::: {.column width="40%"}

Let $a = \frac{1}{\sqrt{\mu}}\nabla f(x)$ and $b =\sqrt{\mu}(x-x^*) -\frac{1}{\sqrt{\mu}}\nabla f(x)$ 

. . .

Then $a+b = \sqrt{\mu}(x-x^*)$ and $a-b=\frac{2}{\sqrt{\mu}}\nabla f(x)-\sqrt{\mu}(x-x^*)$
:::
::::

## Any $\mu$-strongly convex differentiable function is a PL-function

$$
\begin{split}
\uncover<+->{ f(x) - f(x^*) &\leq \frac12 \left(\frac{1}{\mu}\|\nabla f(x)\|^2_2 - \left\|\sqrt{\mu}(x-x^*) -\frac{1}{\sqrt{\mu}}\nabla f(x)\right\|_2^2\right) \\ }
\uncover<+->{ f(x) - f(x^*) &\leq \frac{1}{2\mu}\|\nabla f(x)\|^2_2, \\ }
\end{split}
$$

which is exactly PL-condition. It means, that we already have linear convergence proof for any strongly convex function.

# Subgradient Descent

## Non-smooth optimization

$$
\min_{x \in \mathbb{R}^n} f(x),
$$

A classical convex optimization problem is considered. We assume that $f(x)$ is a convex function, but now we do not require smoothness. 

![Norm cones for different $p$ - norms are non-smooth](norm_cones.pdf){width=90%}

## Non-smooth optimization

![Wolfe's example. [\faPython Open in Colab](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/subgrad.ipynb)](wolfe_3d.pdf){width=90%}

## Algorithm

A vector $g$ is called the **subgradient** of the function $f(x): S \to \mathbb{R}$ at the point $x_0$ if $\forall x \in S$:

$$
f(x) \geq f(x_0) + \langle g, x - x_0 \rangle
$$

. . .

The idea is very simple: let's replace the gradient $\nabla f(x_k)$ in the gradient descent algorithm with a subgradient $g_k$ at point $x_k$:

$$
\tag{SD}
x_{k+1} = x_k - \alpha_k g_k,
$$

where $g_k$ is an arbitrary subgradient of the function $f(x)$ at the point $x_k$, $g_k \in \partial f (x_k)$

## Convergence bound

:::: {.columns}

::: {.column width="60%"}

$$
\begin{split}
\uncover<+->{\| x_{k+1} - x^* \|^2 & = \|x_k - x^* - \alpha_k g_k\|^2 = \\ }
\uncover<+->{& = \| x_k - x^* \|^2 + \alpha_k^2 \|g_k\|^2 - 2 \alpha_k \langle g_k, x_k - x^* \rangle \\ }
\uncover<+->{2\alpha_k \langle g_k, x_k - x^* \rangle &=  \| x_k - x^* \|^2 + \alpha_k^2 \|g_k\|^2 - \| x_{k+1} - x^* \|^2 \\ }
\end{split}
$$

. . .

Let us sum the obtained equality for $k = 0, \ldots, T-1$:

. . .

$$
\begin{split}
\uncover<+->{ \sum\limits_{k = 0}^{T-1}2\alpha_k \langle g_k, x_k - x^* \rangle &=  \| x_0 - x^* \|^2 - \| x_{T} - x^* \|^2 + \sum\limits_{k=0}^{T-1}\alpha_k^2 \|g_k^2\| \\ }
\uncover<+->{ &\leq \| x_0 - x^* \|^2 + \sum\limits_{k=0}^{T-1}\alpha_k^2 \|g_k^2\| \\ }
\uncover<+->{&\leq R^2 + G^2\sum\limits_{k=0}^{T-1}\alpha_k^2}
\end{split}
$$

:::
::: {.column width="40%"}

* Let's write down how close we came to the optimum $x^* = \text{arg}\min\limits_{x \in \mathbb{R}^n} f(x) = \text{arg} f^*$ on the last iteration:
* For a subgradient: $\langle g_k, x_k - x^* \rangle \leq f(x_k) - f(x^*) = f(x_k) - f^*$.
* We additionaly assume, that $\|g_k\|^2 \leq G^2$
* We use the notation $R = \|x_0 - x^*\|_2$
:::
::::

## Convergence bound

:::: {.columns}

::: {.column width="60%"}

Assuming $\alpha_k = \alpha$ (constant stepsize), we have:

$$
\begin{split}
\sum\limits_{k = 0}^{T-1} \langle g_k, x_k - x^* \rangle \leq \dfrac{R^2}{2 \alpha} + \dfrac{\alpha}{2}G^2 T
\end{split}
$$

. . .

Minimizing the right-hand side by $\alpha$ gives $\alpha^* = \dfrac{R}{G}\sqrt{\dfrac{1}{T}}$ and $\sum\limits_{k = 0}^{T-1} \langle g_k, x_k - x^* \rangle \leq GR \sqrt{T}$.

. . .

$$
\begin{split}
\uncover<+->{ f(\overline{x}) - f^* &= f \left( \frac{1}{T}\sum\limits_{k=0}^{T-1} x_k \right) - f^* \leq \dfrac{1}{T} \left( \sum\limits_{k=0}^{T-1} (f(x_k) - f^* )\right) \\ }
\uncover<+->{ & \leq \dfrac{1}{T} \left( \sum\limits_{k=0}^{T-1}\langle g_k, x_k - x^* \rangle\right) \\ }
\uncover<+->{ & \leq G R \dfrac{1}{ \sqrt{T}} }
\end{split}
$$

:::

. . .

::: {.column width="40%"}
Important notes:

* Obtaining bounds not for $x_T$ but for the arithmetic mean over iterations $\overline{x}$ is a typical trick in obtaining estimates for methods where there is convexity but no monotonic decreasing at each iteration. There is no guarantee of success at each iteration, but there is a guarantee of success on average
* To choose the optimal step, we need to know (assume) the number of iterations in advance. Possible solution: initialize $T$ with a small value, after reaching this number of iterations double $T$ and restart the algorithm. A more intelligent way: adaptive selection of stepsize.
:::
::::

## Steepest subgradient descent convergence bound

$$
\begin{split}
\uncover<+->{\| x_{k+1} - x^* \|^2 & = \|x_k - x^* - \alpha_k g_k\|^2 = \\ }
\uncover<+->{& = \| x_k - x^* \|^2 + \alpha_k^2 \|g_k\|^2 - 2 \alpha_k \langle g_k, x_k - x^* \rangle \circeq \\ }
\uncover<+->{ \alpha_k & = \dfrac{\langle g_k, x_k - x^*\rangle}{\| g_k\|^2}\text{ (from minimizing right hand side over stepsize)}\\ }
\uncover<+->{& \circeq \| x_k - x^* \|^2 - \dfrac{\langle g_k, x_k - x^*\rangle^2}{\| g_k\|^2} \\ }
\uncover<+->{\langle g_k, x_k - x^*\rangle^2 &= \left( \| x_k - x^* \|^2 - \| x_{k+1} - x^* \|^2 \right) \| g_k\|^2 \leq \left( \| x_k - x^* \|^2 - \| x_{k+1} - x^* \|^2 \right) G^2  \\ }
\uncover<+->{ \sum\limits_{k=0}^{T-1}\langle g_k, x_k - x^*\rangle^2 &\leq \sum\limits_{k=0}^{T-1}\left( \| x_k - x^* \|^2 - \| x_{k+1} - x^* \|^2 \right) G^2 \leq \left( \| x_0 - x^* \|^2 - \| x_{T} - x^* \|^2 \right) G^2 \\ }
\uncover<+->{ \dfrac{1}{T}\left(\sum\limits_{k=0}^{T-1}\langle g_k, x_k - x^*\rangle \right)^2 &\leq \sum\limits_{k=0}^{T-1}\langle g_k, x_k - x^*\rangle^2 \leq R^2  G^2 \qquad \sum\limits_{k=0}^{T-1}\langle g_k, x_k - x^*\rangle  \leq GR \sqrt{T} }
\end{split}
$$

. . .

Which leads to exactly the same bound of $\mathcal{O}\left(\frac{1}{\sqrt{T}}\right)$ on the primal gap. In fact, for this class of functions, you can't get a better result than $\frac{1}{\sqrt{T}}$.

## Linear Least Squares with $l_1$-regularization

$$
\min_{x \in \mathbb{R}^n} \dfrac{1}{2}\|Ax - b\|_2^2 + \lambda \|x\|_1
$$

Algorithm will be written as:

$$
x_{k+1} = x_k - \alpha_k \left( A^\top(Ax_k - b) + \lambda \text{sign}(x_k)\right)
$$

where signum function is taken element-wise.

![Illustration [\faPython Open in Colab](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/SD.ipynb)](SD_gap.pdf){width=60%}

## Great illustration of $l_1$-regularization

[![\faGithub Code for the animation](l1_reg.jpeg)](https://github.com/ievron/RegularizationAnimation/raw/main/Regularization.mp4)

## Support Vector Machines

Let $D = \{ (x_i, y_i) \mid x_i \in \mathbb{R}^n, y_i \in \{\pm 1\}\}$

We need to find $\omega \in \mathbb{R}^n$ and $b \in \mathbb{R}$ such that

$$
\min_{\omega \in \mathbb{R}^n, b \in \mathbb{R}} \dfrac{1}{2}\|\omega\|_2^2 + C\sum\limits_{i=1}^m \text{max}[0, 1 - y_i(\omega^\top x_i + b)]
$$