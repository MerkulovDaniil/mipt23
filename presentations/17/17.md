---
title: "Conditional gradient methods. Projected Gradient Descent. Frank-Wolfe Method."
author: Daniil Merkulov
institute: Optimization methods. MIPT
# bibliography: ../../files/biblio.bib
# csl: ../../files/diabetologia.csl
format: 
    beamer:
        pdf-engine: pdflatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: false
        incremental: true
        include-in-header: ../../files/header.tex  # Custom LaTeX commands and preamble
---

# Conditional methods

## Constrained optimization

:::: {.columns}

::: {.column width="50%"}

### Unconstrained optimization

$$
\min_{x \in \mathbb{R}^n} f(x)
$$

* Any point $x_0 \in \mathbb{R}^n$ is feasible and could be a solution.

:::

::: {.column width="50%"}

### Constrained optimization

$$
\min_{x \in S} f(x)
$$

* Not all $x \in \mathbb{R}^n$ is feasible and could be a solution.
* The solution has to be inside the set $S$.
* Example: 
    $$
    \frac12\|Ax - b\|_2^2 \to \min_{\|x\|_2^2 \leq 1}
    $$

:::

::::

Gradient Descent is a great way to solve unconstrained problem 
$$
\tag{GD}
x_{k+1} = x_k - \alpha_k \nabla f(x_k)
$$
Is it possible to tune GD to fit constrained problem? 

. . .

**Yes**. We need to use projections to ensure feasibility on every iteration.

## Projection

The distance $d$ from point $\mathbf{y} \in \mathbb{R}^n$ to closed set $S \subset \mathbb{R}^n$:
$$
d(\mathbf{y}, S, \| \cdot \|) = \inf\{\|x - y\| \mid x \in S \}
$$
We will focus on Euclidean projection (other options are possible) of a point $\mathbf{y} \in \mathbb{R}^n$ on set $S \subseteq \mathbb{R}^n$ is a point $\text{proj}_S(\mathbf{y}) \in S$: 
$$
\text{proj}_S(\mathbf{y}) = \frac12 \underset{\mathbf{x} \in S}{\operatorname{argmin}} \|x - y\|_2^2
$$

* **Sufficient conditions of existence of a projection**. If $S \subseteq \mathbb{R}^n$ - closed set, then the projection on set $S$ exists for any point.
* **Sufficient conditions of uniqueness of a projection**. If $S \subseteq \mathbb{R}^n$ - closed convex set, then the projection on set $S$ is unique for any point.
* If a set is open, and a point is beyond this set, then its projection on this set does not exist.
* If a point is in set, then its projection is the point itself.

## Projection criterion (Bourbaki-Cheney-Goldstein inequality)

$$
\langle \mathbf{y} - \text{proj}_S(\mathbf{y}), \mathbf{x} - \text{proj}_S(\mathbf{y})\rangle \leq 0 \quad \forall x \in S.
$$ 

## Projection operator is non-expansive

* A function $f$ is called non-expansive if $f$ is $L$-Lipschitz with $L \leq 1$ ^[Non-expansive becomes contractive if $L < 1$.]. That is, for any two points $x,y \in \text{dom} f$,
    $$
    \|f(x)-f(y)\| \leq L\|x-y\|, \text{ where } L \leq 1.
    $$
    It means the distance between the mapped points is possibly smaller than that of the unmapped points.

* Projection operator is non-expansive:
    $$
    \| \text{proj}(x) - \text{proj}(y) \|_2 \leq \| x - y \|_2.
    $$

* Next: variational characterization implies non-expansiveness. i.e.,
    $$
    \langle y - \text{proj}(y), x - \text{proj}(y) \rangle \leq 0 \quad \forall x \in S \qquad \Rightarrow \qquad \| \text{proj}(x) - \text{proj}(y) \|_2 \leq \| x - y \|_2.
    $$ 

## Projection operator is non-expansive

Shorthand notation: let $\pi = \text{proj}$ and $\pi(x)$ denotes $\text{proj}(x)$.

Begins with the variational characterization / obtuse angle inequality
$$
\langle y-\pi(y) , x-\pi(y) \rangle \leq 0 \quad \forall x \in S.
$$ {#eq-proj1}


:::: {.columns}

::: {.column width="50%"}
Replace $x$ by $\pi(x)$ in @eq-proj1
$$
\langle y-\pi(y), \pi(x)-\pi(y) \rangle \leq 0.
$$ {#eq-proj2}
:::

::: {.column width="50%"}
Replace $y$ by $x$ and $x$ by $\pi(y)$ in @eq-proj1

$$
\langle x-\pi(x), \pi(y)-\pi(x) \rangle \leq 0.
$$ {#eq-proj3}
:::

::::

(@eq-proj2)+(@eq-proj3) will cancel $\pi(y) - \pi(x)$, not good. So flip the sign of (@eq-proj3) gives
$$
\langle \pi(x)-x, \pi(x)-\pi(y) \rangle \leq 0.
$$ {#eq-proj4}

:::: {.columns}

::: {.column width="60%"}
$$
\begin{split}
\langle y-\pi(y)+\pi(x)-x , \pi(x)-\pi(y) \rangle & \leq 0 \\
\langle y-x+\pi(x)-\pi(y), \pi(x)-\pi(y) \rangle & \leq 0 \\
\langle y - x, \pi(x) - \pi(y) \rangle & \leq -\langle \pi(x)-\pi(y), \pi(x)-\pi(y) \rangle \\
\langle y - x, \pi(y) - \pi(x) \rangle & \geq \lVert \pi(x) - \pi(y) \rVert^2_2 \\
\lVert (y - x)^\top (\pi(y) - \pi(x)) \rVert_2 & \geq \lVert \pi(x) - \pi(y) \rVert^2_2
\end{split}
$$
:::

::: {.column width="40%"}
By Cauchy-Schwarz inequality, the left-hand-side is upper bounded by $\lVert y - x \rVert_2 \lVert \pi(y) - \pi(x) \rVert_2$, we get $\lVert y - x \rVert_2 \lVert \pi(y) - \pi(x) \rVert_2 \geq \lVert \pi(x) - \pi(y) \rVert^2_2$. Cancels $\lVert \pi(x) - \pi(y) \rVert_2$ finishes the proof.
:::

::::

## Example: projection on the ball

Find $\pi_S (y) = \pi$, if $S = \{x \in \mathbb{R}^n \mid \|x - x_0\| \le R \}$, $y \notin S$ 

Build a hypothesis from the figure: $\pi = x_0 + R \cdot \frac{y - x_0}{\|y - x_0\|}$ 

:::: {.columns}

::: {.column width="60%"}
Check the inequality for a convex closed set: $(\pi - y)^T(x - \pi) \ge 0$ 

$$
\begin{split}
\left( x_0 - y + R \frac{y - x_0}{\|y - x_0\|} \right)^T\left( x - x_0 - R \frac{y - x_0}{\|y - x_0\|} \right) &= \\
\left( \frac{(y - x_0)(R - \|y - x_0\|)}{\|y - x_0\|} \right)^T\left( \frac{(x-x_0)\|y-x_0\|-R(y - x_0)}{\|y - x_0\|} \right) &= \\
\frac{R - \|y - x_0\|}{\|y - x_0\|^2} \left(y - x_0 \right)^T\left( \left(x-x_0\right)\|y-x_0\|-R\left(y - x_0\right) \right) &= \\
\frac{R - \|y - x_0\|}{\|y - x_0\|} \left( \left(y - x_0 \right)^T\left( x-x_0\right)-R\|y - x_0\| \right) &= \\
\left(R - \|y - x_0\| \right) \left( \frac{(y - x_0 )^T( x-x_0)}{\|y - x_0\|}-R \right) & \\
\end{split}
$$
:::

::: {.column width="40%"}
The first factor is negative for point selection $y$. The second factor is also negative, which follows from the Cauchy-Bunyakovsky inequality: 

$$
(y - x_0 )^T( x-x_0) \le \|y - x_0\|\|x-x_0\|
$$

$$
\frac{(y - x_0 )^T( x-x_0)}{\|y - x_0\|} - R \le \frac{\|y - x_0\|\|x-x_0\|}{\|y - x_0\|} - R = \|x - x_0\| - R \le 0
$$
:::
::::

## Example: projection on the halfspace

Find $\pi_S (y) = \pi$, if $S = \{x \in \mathbb{R}^n \mid c^T x = b \}$, $y \notin S$. Build a hypothesis from the figure: $\pi = y + \alpha c$. Coefficient $\alpha$ is chosen so that $\pi \in S$: $c^T \pi = b$, so:

:::: {.columns}

::: {.column width="40%"}
$$
\begin{split}
c^T (y + \alpha c) &= b \\
c^Ty + \alpha c^T c &= b \\
c^Ty &= b - \alpha c^T c \\
\end{split}
$$
:::

::: {.column width="60%"}
Check the inequality for a convex closed set: $(\pi - y)^T(x - \pi) \ge 0$ 

$$
\begin{split}
(y + \alpha c - y)^T(x - y - \alpha c) =& \\
\alpha c^T(x - y - \alpha c) =& \\
\alpha (c^Tx) - \alpha (c^T y) - \alpha^2 (c^Tc) =& \\
\alpha b - \alpha (b - \alpha c^T c) - \alpha^2 c^Tc =& \\
\alpha b - \alpha b + \alpha^2 c^T c - \alpha^2 c^Tc =& 0 \ge 0
\end{split}
$$
:::

::::
 
# Projected Gradient Descent (PGD)

## Convergence rate for smooth and convex case

# Frank-Wolfe Method

## Idea

## Convergence

## Comparison to PGD

# Projected Subradient Descent (PSD)